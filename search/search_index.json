{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Nextflow and HPC","text":"<p>This workshop is delivered over 2 3-hour sessions that cover how HPC works, to running and optimising workflows on it using Nextflow and nf-core.</p> <p>We work through a realistic experimental scenario: adapting a whole genome variant calling workflow to run on high performance computing (HPC) infrastructure. Each lesson connects foundational HPC concepts to workflow design and configuration in Nextflow.</p> <p>This workshop is delivered concurrently on Australia's Tier-1 HPCs; NCI's Gadi HPC and Pawsey's Setonix HPC.</p>"},{"location":"#developers","title":"Developers","text":"<ul> <li>Michael Geaghan, Sydney Informatics Hub, University of Sydney</li> <li>Fred Jaya, Sydney Informatics Hub, University of Sydney</li> <li>Mitchell O'Brien, Sydney Informatics Hub, University of Sydney</li> <li>Georgie Samaha, Sydney Informatics Hub, University of Sydney</li> <li>Cali Willet, Sydney Informatics Hub, University of Sydney</li> </ul>"},{"location":"#facilitators","title":"Facilitators","text":"<ul> <li>Giorgia Mori, Australian BioCommons</li> <li>Sarah Beecroft, Pawsey Supercomputing Research Centre</li> <li>Kisaru Liyanage, National Computational Infrastructure</li> <li>Wenjing Xue, National Computational Infrastructure</li> <li>Cali Willet, Sydney Informatics Hub, University of Sydney</li> <li>Kristina Gagalova, Curtin University</li> <li>Gayatri Aniruddha, University of Western Australia</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is an intermediate-advanced workshop for people developing reproducible bioinformatics workflows to run on HPC. It assumes experience with the following:</p> <ul> <li>Running and developing Nextflow workflows</li> <li>Working in a Linux environment using basic scripting (e.g. Bash)</li> <li>Working on a HPC cluster.</li> </ul> <p>Attendees would benefit from having previously completed our Nextflow for the Life Sciences workshop.</p>"},{"location":"#set-up-requirements","title":"Set up requirements","text":"<p>Please see our set up instructions to set up your laptop for this workshop.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>In order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences * Gracefully accept constructive criticism</li> <li>Focus on what is best for the community</li> <li>Show courtesy and respect towards other community members.</li> </ul>"},{"location":"#credits-and-acknowledgements","title":"Credits and acknowledgements","text":"<p>We gratefully acknowledge the contributions of Abdullah Shaikh, Ludovic Capelli, Ziad Al-Bkhetan, Melissa Burke, Magda Antczak, Matthew Downton, Marie-Emilie Gauthier, Thanh Nguyen, and Eric Urng.</p> <p>This workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney. Provision of computing and data resources were provided by the Australian BioCommons Leadership Share (ABLeS) program, the National Computational Infrastructure (NCI) and Pawsey Supercomputing Research Centre. The workshop was enabled by Australian BioCommons' BioCLI Platforms Project (NCRIS via Bioplatforms Australia).</p> <p></p>"},{"location":"setup/","title":"Set up your computer","text":"<p>In this workshop, we will be using Pawsey's Setonix HPC and NCI's Gadi HPC. </p> <p>The requirements for this workshop are a personal computer with:</p> <ul> <li>Visual Studio Code (VSCode)</li> <li>A web browser</li> </ul> <p>Below, you will find instructions on how to set up VSCode and connect to the HPC system to which you've been assigned.</p> <p>Each participant will be provided with their training account and password prior to the workshop. Before the workshop, you must have the following:</p> <ol> <li>VSCode installed</li> <li>The necessary VSCode extensions installed</li> <li>Be able to connect to your assigned HPC.</li> </ol> <p>Info</p> <p>If you require assistance with the setup, please write in the discussion board on the Google document.</p>"},{"location":"setup/#installing-visual-studio-code","title":"Installing Visual Studio Code","text":"<p>Visual Studio Code (VSCode) is a versatile code editor that we will use for the workshop. We will use VSCode to connect to the VM, navigate the directories, edit, view and download files.</p> <ol> <li>Download VSCode by following the installation instructions for your local Operating System.</li> <li>Open VSCode to confirm it was installed correctly.</li> </ol> <p></p>"},{"location":"setup/#installing-the-vscode-extensions","title":"Installing the VSCode extensions","text":"<p>Specific VSCode extensions are required to connect to the VM and make working with Nextflow files easier (i.e. syntax highlighting).</p> <ol> <li> <p>In the VSCode sidebar on the left, click on the extensions button (four blocks)</p> </li> <li> <p>In the Extensions Marketplace search bar, search for <code>remote ssh</code>. Select \"Remote - SSH\"</p> <p></p> </li> <li> <p>Click on the blue <code>Install</code> button.</p> </li> <li> <p>Once installed, you should see a blue bar in the bottom left corner of the screen. This means that the SSH extension was successfully installed.</p> </li> <li> <p>Close the Extensions tab and sidebar</p> </li> </ol>"},{"location":"setup/#connecting-to-the-hpcs","title":"Connecting to the HPCs","text":"<p>Ensure you have your training details of your assigned system.</p> <ol> <li> <p>Click the blue bar in the bottom left corner of the window. A menu will appear up the top of the window:</p> <p></p> </li> <li> <p>Click \"Connect to Host...\"</p> <p></p> </li> <li> <p>In the following text box, type the following command. Note: be sure to select the relevant tab below for your assinged platform (\"Gadi (PBS)\" or \"Setonix (Slurm)\"):</p> Gadi (PBS)Setonix (Slurm) <p><code>ssh &lt;username&gt;@gadi.nci.org.au</code></p> <p> </p> <p><code>ssh &lt;username&gt;@setonix.pawsey.org.au</code></p> <p></p> </li> <li> <p>Press the Enter key</p> </li> <li> <p>In the next menu, you are prompted to select an SSH configuration file to update with the new settings. Select the one that is in your home directory.</p> <p></p> </li> <li> <p>You will see a confirmation message that the new host was successfully added.</p> <p></p> </li> <li> <p>Repeat steps 1 and 2 again, clicking on the blue SSH bar at the bottom left and selecting \"Connect to Host...\"</p> </li> <li> <p>Click on the remote that you just added: this will be called <code>gadi.nci.org.au</code> for Gadi and <code>setonix.pawsey.org.au</code> for Setonix.</p> </li> <li> <p>A new window will open and a prompt will appear at the top of the window asking for your password. Enter it here and press Enter.</p> </li> <li> <p>A message will appear for a few moments saying \"Setting up SSH Host... Initializing VS Code Server\"</p> </li> <li> <p>Once VS Code has been set up on the remote and you are successfully logged in, you will see the text <code>SSH: &lt;hostname&gt;</code> in the blue SSH box in the bottom left corner of the window, where <code>&lt;hostname&gt;</code> is either <code>gadi.nci.org.au</code> or <code>setonix.pawsey.org.au</code>, depending on your system.</p> <p></p> </li> </ol>"},{"location":"setup/#installing-the-nextflow-extension","title":"Installing the Nextflow extension","text":"<p>Once you have connected to your assigned HPC, you should also install the Nextflow extension, which provides syntax highlighting and can help identify any potential errors in your code. Note that this needs to be done after you have connected, as you are installing the extension on the remote computer, not your local computer or laptop.</p> <ol> <li> <p>Once again, click on the extensions button in the left sidebar (the icon with four blocks)</p> </li> <li> <p>In the Extensions Marketplace search bar, search for <code>nextflow</code> and install the \"nextflow\" estension.</p> <p></p> </li> <li> <p>The Nextflow extension should now show as installed and the blue \"Install\" button has changed to \"Uninstall\". You should also now see a new icon in the left sidebar that looks like a curved X shape. This means that the Nextflow extension has been installed correctly. You can now close the extensions tab.</p> <p> </p> </li> </ol> <p>Success</p> <p>You have now configured VSCode for the workshop!</p>"},{"location":"part1/01_0_intro/","title":"1.0 Introduction","text":"<p>In the first part of this workshop, we will familarise ourselves with some foundational concepts required to effectively run bioinformatics workflows on HPC clusters. We will then apply these concepts to the configuration of a popular nf-core pipeline, Sarek. In part 2, we will further apply these concepts to a custom Nextflow workflow.</p> <p></p> <p>Note the code blocks!</p> <p>You\u2019ll notice that many code examples in this workshop are presented in tabs for the different HPC systems we're using in this workshop.</p> <p>The commands provided are largely the same, but each system has its own scheduler, queues, and job submission syntax.</p> <p>Select the tab that matches the system you\u2019re using, the content will stay synced across the page.</p> <p>If you switch to Setonix once, all other tabs on this page will automatically follow.</p> Gadi (PBS)Setonix (Slurm) <p>If you've been assigned a training account on Gadi, your commands and configurations will be specific to the PBS Pro job scheduler running on Gadi.</p> <pre><code>An example command to run on Gadi\n</code></pre> <p>If you've been assigned a training account on Setonix, your commands and configurations will be specific to the SLURM job scheduler running on Setonix.</p> <pre><code>An example command to run on Setonix\n</code></pre> <p>Let us know if you need help</p> <p>HPCs are complex, shared systems, and this can sometimes introduce unexpected issues and errors. If you ran into any issues at any time during this workshop, please use the \" No\" Zoom react and our facilitators can help you out. If necessary, we will make use of breakout rooms to diagnose significant issues and make sure that everyone is at the same point before moving on to further lessons.</p> <p>We will also provide a Google Docs document for questions you might have.</p>"},{"location":"part1/01_0_intro/#101-log-in-to-your-assigned-hpc","title":"1.0.1 Log in to your assigned HPC","text":"<p>If you haven't already done so, follow the setup instructions to set up VSCode and log in to your assigned HPC with the user account and password provided to you.</p> <p>Make sure you are logged in with your training account credentials!</p> <p>If you have previously had an account on either Gadi or Setonix, it is possible that you may have those accounts set up in you SSH configuration file (by default this is <code>~/.ssh/config</code>, where <code>~</code> is your home directory). In this case, you might have duplicate entries for your HPC hostname, causing you to accidentally log in with those credentials rather than your provided training credentials.</p> <p>When you log in to your assigned account and access the terminal in VSCode (<code>Ctrl + J</code> (Windows/Linux) / <code>Cmd + J</code> (Mac)), you should see your username in the prompt:</p> Gadi (PBS)Setonix (Slurm) <pre><code>[aaa000@gadi-login-01 ~]$\n</code></pre> <pre><code>cou001@setonix-01:~&gt;\n</code></pre> <p>If you see a different user name, close the window and try re-connecting to the HPC. If you find that you keep logging in to the wrong user account, you can also try editing your SSH config file and removing the old account details:</p> Gadi (PBS)Setonix (Slurm) Duplicate entries for gadi.nci.org.au in ~/.ssh/config<pre><code>Host gadi.nci.org.au\n    HostName gadi.nci.org.au\n    User aaa000\n\nHost gadi.nci.org.au\n    HostName gadi.nci.org.au\n    User oldusername\n</code></pre> Duplicate entries for setonix.pawsey.org.au in ~/.ssh/config<pre><code>Host setonix.pawsey.org.au\n    HostName setonix.pawsey.org.au\n    User cou001\n\nHost setonix.pawsey.org.au\n    HostName setonix.pawsey.org.au\n    User oldusername\n</code></pre> <p>If you see duplicate entires like this, delete the old entry and try logging in again.</p>"},{"location":"part1/01_0_intro/#102-setup-the-workspace","title":"1.0.2 Setup the workspace","text":"<p>Skip this if the workspace has already been set up</p> <p>This step will setup the workspace for the workshop. This includes:</p> <ul> <li>Creating the directory <code>/scratch/&lt;PROJECT&gt;/&lt;USER&gt;/nextflow-on-hpc-materials</code></li> <li>Cloning the <code>sarek</code> git repository to <code>/scratch/&lt;PROJECT&gt;/&lt;USER&gt;/nextflow-on-hpc-materials/part1/sarek</code></li> <li>Cloning the <code>config-demo-nf</code> git repository to <code>/scratch/&lt;PROJECT&gt;/&lt;USER&gt;/nextflow-on-hpc-materials/part1/config-demo-nf</code></li> <li>Creating and populating the Singularity cache directory at <code>/scratch/&lt;PROJECT&gt;/&lt;USER&gt;/nextflow-on-hpc-materials/singularity</code></li> </ul> <p>This is a one-time setup. If these folders and files already exist on your system, it means it has already been setup and you can safely skip ahead to section 1.0.3: Navigate to the <code>part1</code> working directory.</p> <p>We will let you know on the day whether these steps have already been performed for you.</p> <p>For this workshop, we will be working within the scratch storage system of the HPCs. Navigate to the scratch space for the workshop project.</p> <ol> <li> <p>In the left-hand side bar, click on the \"Explorer\" tab (an icon that looks like two sheets of paper).</p> <p></p> </li> <li> <p>Click on \"Open Folder\"</p> </li> <li> <p>In the text box that appears, enter the path of your assigned directory in the HPC scratch space:</p> Gadi (PBS)Setonix (Slurm) <p>On Gadi, we will be working entirely within the scratch space for the <code>vp91</code> project: <code>/scratch/vp91/</code>. In this directory, everyone will have their own folder, labelled with the same name as their user ID. This is the path you should enter into the text box when prompted. For example, for the username <code>usr123</code>, you would enter:</p> <pre><code>/scratch/vp91/usr123\n</code></pre> <p>On Setonix, we will be working entirely within the scratch space for the <code>courses01</code> project: <code>/scratch/courses01/</code>. In this directory, everyone will have their own folder, labelled with the same name as their user ID. This is the path you should enter into the text box when prompted. For example, for the username <code>usr123</code>, you would enter:</p> <pre><code>/scratch/courses01/usr123\n</code></pre> </li> </ol> <p>When you first log in, your directory in the scratch space will be an empty folder. The first job for the day will be to clone the workshop materials into this space. To do this, open the VSCode terminal (<code>Ctrl + J</code> (Windows/Linux) / <code>Cmd + J</code> (Mac)) and run the following commands:</p> <pre><code>git clone https://github.com/Sydney-Informatics-Hub/nextflow-on-hpc-materials.git\ncd nextflow-on-hpc-materials\nls\n</code></pre> <p>You should see a few folders and files inside here:</p> Output<pre><code>README.md        part1/           setup.gadi.sh\ndata/            part2/           setup.setonix.sh\n</code></pre> <p>There are two scripts in the main directory: <code>setup.gadi.sh</code> and <code>setup.setonix.sh</code>, one for each HPC we are using in this workshop. These scripts set up the workspace and pull a few additional git repositories that we will be using throughout the workshop. Go ahead and run the relevant script for your system:</p> Gadi (PBS)Setonix (Slurm) <pre><code>./setup.gadi.sh\n</code></pre> <pre><code>./setup.setonix.sh\n</code></pre> <p>Once completed, the script will print out \"Setup complete\" to confirm everything has successfully been set up.</p> <p>Attention Gadi users: If you get this message...</p> <p>On Gadi, you may get the following message at the end of the setup script:</p> <pre><code>IMPORTANT: YOUR DEFAULT PROJECT HAS BEEN CHANGED TO 'vp91'. PLEASE LOG OUT AND BACK IN AGAIN TO REFRESH YOUR SESSION.\n</code></pre> <p>If you see this message, you will need to close your window and re-connect to the HPC as per the setup instructions.</p>"},{"location":"part1/01_0_intro/#103-navigate-to-the-part1-working-directory","title":"1.0.3 Navigate to the <code>part1</code> working directory","text":"<p>As a final step, go to VSCode's \"File\" menu and select \"Open Folder...\". Enter the full path to the <code>part1</code> directory in the text box that appears at the top of the window:</p> Gadi (PBS)Setonix (Slurm) <p><code>/scratch/vp91/&lt;username&gt;/nextflow-on-hpc-materials/part1</code></p> <p><code>/scratch/courses01/&lt;username&gt;/nextflow-on-hpc-materials/part1</code></p> <p>Press Enter and the window will refresh. Now the window is loaded at today's working directory, since everything we will be doing in part 1 of this workshop will be within this folder.</p> <p>Did everything work correctly?</p> <p>The above setup steps are vital for the rest of the workshop. If you ran into any issues, please react with a \" No\" and we can help out before we move on.</p> <p>Otherwise, if you're ready to move on, please let us know by reacting on zoom with a \" Yes\".</p> <p>We are now ready to get started with the workshop!</p>"},{"location":"part1/01_10_outro/","title":"1.10 Summary","text":"<p>In today's session we have learned about what HPCs are, what sets them apart from your standard computer and how they require you to work in a particular way, by explicitly requesting the resources you need and submitting jobs to a scheduler to be executed whenever those resources become available. We have also learned how Nextflow uses executors to talk to the scheduler and manage submitting the individual workflow tasks to be executed on the HPC compute nodes.</p>"},{"location":"part1/01_10_outro/#hpcs-are-shared-systems","title":"HPCs are shared systems","text":"<p>Remember, you are working on a shared system with hundreds of other users. Because of this, you need to be explicit about how many resources your jobs require. The scheduler's job is to take those requests and find a suitable time and place to run the jobs. It is important to optimise your pipelines; if you under-request resources, you job will fail; but if you over-request, your job may wait in the queue for longer and cost more than it needed to.</p>"},{"location":"part1/01_10_outro/#containers-are-your-friend","title":"Containers are your friend","text":"<p>Controlling the versions of the software you use is vital for reproducible research. Out of the various different methods for running software on HPCs, containers provide the most freedom, allowing you to use any version of a tool you would like, without having to worry about dependency issues, version conflicts, or building the software yourself. Their self-contained nature and portability makes containers the recommended method for using software within your Nextflow workflows.</p>"},{"location":"part1/01_10_outro/#nextflow-is-portable","title":"Nextflow is portable","text":"<p>The goal of Nextflow is to allow for the development of portable and reproducible workflows that are written once and run anywhere. To achieve this, they separate out the actual workflow logic from the configuration. When properly written, the Nextflow code itself should never need to be altered when moving the workflow between systems, whether that be a local laptop, an institutional HPC, or the cloud. Instead, one or more configuration files can be used to handle system-specific details, such as communicating with an HPC scheduler. Remember that there are different levels of configuration that a pipeline will typically require, and each should have their own configuration files:</p> <ul> <li>Pipeline-specific configuration details should be kept in <code>nextflow.config</code> and other <code>.config</code> files bundled with the workflow code. These should never need to be altered from system to system.</li> <li>Institutional-specific configuration details, such as HPC executor configuration, should be kept in a separate file (e.g. <code>gadi.config</code> or <code>setonix.config</code>) that can be used by multiple different pipelines. Ideally, this shouldn't need to be altered between pipelines on the same system.</li> <li>Run-level configuration details, such as fine-tuning memory and CPU requirements for a particular dataset, should be kept in yet another configuration file that is specific to that instance of that pipeline.</li> </ul>"},{"location":"part1/01_10_outro/#fine-tuning-workflows-for-your-data","title":"Fine-tuning workflows for your data","text":"<p>While a well-written pipeline should be able to handle inputs of different sizes and dynamically request resources accordingly, it is impossible to forsee every possibility. As such, it is often necessary to have a custom configuration file to fine-tune a pipeline to your specific dataset. We saw today that for very small datasets, a pipeline may by default request too many resources than necessary. In other cases, for a particularly large dataset, you may need to increase the resources you require to prevent out-of-memory failures and jobs exceeding their requested walltime.</p>"},{"location":"part1/01_10_outro/#next-steps-configuring-a-custom-pipeline","title":"Next steps: configuring a custom pipeline","text":"<p>While nf-core is an amazing resource for community-built bioinformatics tools, it still has its limitations. There isn't a pipeline for every purpose you might have, and when there is, you might find it is still under development, or contains bugs that break your analyses, or simply doesn't quite do what you need in just the right way. As such, it is often necessary to write your own custom pipelines that handle your data in just the way you like. In tomorrow's session, we will apply the concepts we learned today to configuring such a custom workflow for HPC. We will be continuing along the same theme as today: WGS short variant calling. In doing so, we will explore how we can update a pipeline's logic to better take advantage of the parallel nature of HPCs and implement our own scatter-gather pattern for parallel processing of sequencing data.</p>"},{"location":"part1/01_1_hpc_for_workflows/","title":"1.1 HPC for bioinformatics workflows","text":"<p>Learning objectives</p> <ul> <li>Define high performance computing (HPC) and describe its main components</li> <li>Explain when bioinformatics workflows require HPC resources rather than local execution</li> <li>Summarise the main constraints imposed by HPC environments.</li> </ul>"},{"location":"part1/01_1_hpc_for_workflows/#111-what-is-an-hpc","title":"1.1.1 What is an HPC?","text":"<p>High Performance Computing (HPC) systems are large clusters of computers with lots of CPUs, memory (RAM) and storage space. They are specially-built to run large numbers of computational jobs efficiently and concurrently. Bioinformatics analysis often involves many steps, many tools, many samples, and large datasets, which can quickly overwhelm your average laptop or desktop computer. When this becomes the case, HPCs can be the perfect solution to scaling up your workflows and running them efficiently and quickly. However, HPCs expect work to be submitted in a particular way, following specific rules. This means our workflows often need to be designed for HPC, not just moved to HPC.</p>"},{"location":"part1/01_1_hpc_for_workflows/#112-when-does-a-workflow-need-hpc","title":"1.1.2 When does a workflow need HPC?","text":"<p>In bioinformatics, a workflow is simply a defined series of steps that take data as input and transform that data into processed data and/or analytical results. This is true whether you are doing whole genome variant calling, proteomics quantification, single-cell transcriptomics, or metagenomics assembly. Each step in the pipeline performs one job, and each job depends on some form of computation and storage.</p> <p></p>"},{"location":"part1/01_1_hpc_for_workflows/#signs-your-workflow-is-ready-for-hpc","title":"Signs your workflow is ready for HPC","text":"<p>Not every workflow needs a supercomputer. Many analyses start on a laptop and stay there, especially during method development, testing small datasets, or when turnaround is more important than throughput. HPC becomes necessary when your workflow starts to hit practical limits of time, memory, storage, reliability, or governance.</p> <p>A workflow is usually ready for HPC when scale becomes a problem. This might be scale in data size (more gigabytes than your laptop can hold), compute time (weeks of serial runs), memory usage (jobs crash due to insufficient RAM), or workflow complexity (tens of jobs become too painful to run manually).</p> Challenge Example scenario Runtime is too long A single sample takes &gt;12 hours to process Data size is too big Multiple large FASTQs need to be processed Memory requirements are too large R or Python crashes loading matrices Scaling samples manually is painful Manually running multiple scripts across multiple samples Storage is a bottleneck Local disk is constantly full due to raw and processed data size Serial execution is too slow Workflow is too slow to run one sample after another - multi-sample analysis must run faster Data governance, ethics, and security constraints Legal and/or ethical requirements mean highly-protected data must stay on institutional, secure systems <p>If any of the above scenarios sound familiar, then your workflow is likely ready to be moved to and configured for running on an HPC.</p>"},{"location":"part1/01_1_hpc_for_workflows/#113-from-your-laptop-to-hpc","title":"1.1.3 From your laptop to HPC","text":"<p>Before running a workflow, it is important to understand the system we are running it on. Running workloads on HPC is very different from running them on your laptop or a local workstation. HPCs are not just bigger, they are also:</p> <ul> <li>Shared</li> <li>Scheduled</li> <li>Resource constrained.</li> </ul> <p>This introduces an important trade-off. HPCs give you access to massive computational power but at the cost of flexibility. On your laptop or a local workstation you can run whatever you like, whenever you like so long as it fits within the resource limitations of the system. On HPC, you gain scale and speed but you must work within system policies and limits.</p> <p></p>"},{"location":"part1/01_1_hpc_for_workflows/#shared-systems","title":"Shared systems","text":"<p>HPCs are large-scale institutional computing clusters that are intended to be used by many users at once. Indeed, their size and available resources mean than dozens or even hundreds of users can be using them at the same time and still manage to run large scale workflows concurrently and in a timely manner. However, this shared nature puts a significant constraint on how they can be used.</p> <p>The primary constraint is that you don't have the freedom to install whatever software you want on the system. This means that you need other solutions to running the tools that you want. We will explore this issue in the next section.</p> <p>Another constraint is the file system. While HPCs typically have huge shared file systems, they are neither infinite in size nor speed. Running workflows that generate lots of files, or read and write to the file system too frequently, will degrade the performance of the system for all users. Therefore, we need to be conscious of what our workflow is doing and make sure we design it to use the system fairly and efficiently. We will discuss storage limitations in 1.3 HPC architecture.</p>"},{"location":"part1/01_1_hpc_for_workflows/#schedulers","title":"Schedulers","text":"<p>On your local laptop, you will be used to running things whenever you like, but on shared systems like HPCs, this is not the case. Instead, HPCs require you to submit jobs to a scheduler, which decides where and when to run your job based on its resource requirements and the requirements of all other jobs in the queue. This makes HPCs asynchronous and non-interactive: job execution doesn't happen immediately and jobs won't necessarily execute in the order that they were submitted. As such, an HPC workflow needs to be designed to handle this delayed and potentially out-of-order execution style. As we will see later today, Nextflow is perfectly suited to writing workflows that work in this way.</p>"},{"location":"part1/01_1_hpc_for_workflows/#resource-constraints","title":"Resource constraints","text":"<p>Finally, HPCs may have large amounts of computing resources, but they aren't infinite, and they also need to be shared between many users. Therefore, it is vital when running jobs on an HPC to define exactly how many resources you require, including the number of CPUs you need, the amount of memory/RAM, and how much time your jobs needs. As you will see later in this workshop, it is very important to optimise these requests as best as you can, as under- and over-requesting resources can negatively impact your jobs.</p>"},{"location":"part1/01_1_hpc_for_workflows/#114-introducing-our-workshop-scenario-wgs-short-variant-calling","title":"1.1.4 Introducing our workshop scenario: WGS short variant calling","text":"<p>Don't worry if you don't have prior knowledge of this workflow</p> <p>The focus of this workflow is on learning Nextflow; the experimental context we are using (WGS short variant calling) is just a practical example to help you understand workflow design principles for HPC and how Nextflow works. You are not expected to have prior knowledge of variant calling workflows or best practices.</p> <p>For this workshop, we will be focussing on a common bioinformatics analysis workflow used in genomics to identify genetic variants (SNPs and indels) from short-read whole genome sequencing data. This workflow involves multiple processes and tools and is computationally intensive. At a high level, the general procedure is:</p> <ol> <li>Quality control of raw sequences, e.g. filtering &amp; trimming reads</li> <li>Alignment of reads to a reference genome</li> <li>Post alignment processing: sorting, marking duplicates, indexing</li> <li>Variant calling: call SNVs and indels for each sample against reference</li> <li>Joint genotyping: combining samples from a cohort into a single callset</li> <li>Reporting</li> </ol> <p></p> <p>Running this workflow end-to-end captures many challenges that running on HPC using Nextflow can solve:</p> <ul> <li>Many independent jobs: each sample can be processed separately for many steps</li> <li>Resource diversity: tools used at each step require different amounts of CPU, memory, and walltime</li> <li>Large I/O demands: reading and writing of multi-gigabyte files benefits from parallel filesystems</li> </ul> <p>Throughout the workshop we will implement and explore different parts of this workflow in slightly different ways in order to highlight the lessons being taught.</p> <p>How does HPC help run this workflow?</p> <p>Consider the workflow described above:</p> <ol> <li>How does each stage use computational resources? What is the limiting factor of each stage?</li> <li>What would happen if we tried to run this workflow on a personal computer?</li> </ol> Answer <p>Each stage of this workflow has different computational requirements, and many are quite intense:</p> Stage Limiting factor Explanation QC Storage - I/O speed Low CPU &amp; memory requirements, but needs fast access to large files Alignment CPU CPU speed determines how quickly reads can be aligned. Memory requirements are variable: reads can be aligned independently, can read in data in small chunks. Post-alignment processing CPU + memory Both CPU and memory requirements are high as many reads need to be processed together Variant calling CPU + memory CPU usage is high as lots of calculations need to be performed to determine how likely a variant is at each genomic position. All reads within a given region must be processed together, so memory use is also high. Joint genotyping CPU + memory Need to read data from all samples into memory at once, so memory usage is high. CPU also high to make final variant calls for entire cohort. Reporting Storage - I/O speed Low CPU &amp; memory requirements as we only need to summarise the dataset. Fast access to large files creates storage bottleneck. <p>On a standard laptop, this workflow would not get very far before failing due to running out of memory during the alignment or variant calling phases. Designing this workflow for HPC lets us take advantage of large numbers of CPUs, lots of memory, and parallel execution of tasks to considerably speed up each stage.</p> <p>Short variant calling is just one example of where HPCs can be utilised to more efficiently process bioinformatics data. Many bioinformatics workflows, such as RNA sequencing and proteomics data analysis, involve similarly large datasets whose analysis is computationally expensive yet often parallelisable. If you find that your workflows are starting to struggle on your laptop, or you find that it is difficult to scale up your workflows to multiple samples and larger datasets, then this is a good sign that they need to be moved to an HPC and possibly re-designed to take advantage of parallel computation.</p>"},{"location":"part1/01_2_hpc_software/","title":"1.2 Software is different on HPC","text":"<p>Learning objectives</p> <ul> <li>Explain software constraints of HPC systems </li> <li>Demonstrate how to load and use software modules on HPC</li> <li>Describe how containerisation works and understand how containers encapsulate software dependencies </li> <li>Justify why constainerisation is essential for building portable and reproducible workflows </li> </ul> <p>As soon as we start using HPC, we\u2019re working in a shared environment that we have limited control over. That affects our ability to install and manage software.   </p> <p>No sudo for you!</p> <p>Unlike your laptop, you do not have administrative (<code>sudo</code>) privileges on HPC systems. This puts some restrictions on what software you can install and where. On a laptop, you can install software however you like. On HPC, thousands of users share the same system, so unrestricted installs would break environments, cause version conflicts, and introduce security risks. That\u2019s why HPC systems block <code>sudo</code>.</p>"},{"location":"part1/01_2_hpc_software/#121-software-installation-is-different-on-hpcs","title":"1.2.1 Software installation is different on HPCs","text":"<p>There are a few ways in which we can install and use software on HPCs. Some of these are more complex than others, and they each have their benefits and drawbacks:</p> Approach Description Pros Cons Pre-built executables Pre-built, ready-to-run executable files that you simply download and run Easy to use - just download and run Few tools provide pre-built executables on Linux; they must be built for the same operating system and CPU architecture that you are using DIY installation Compiling and installing software yourself from open-source code Full control over where the software gets installed Not beginner-friendly; build times can be long; often requires lots of configuration and specific dependencies Conda/Mamba environments User-managed Python/R environments Flexible; easy for development; well-documented; lots of tutorials available Slow installs; dependency conflicts common; not fully reproducible; can use up lots of storage space Environment modules Pre-installed software provided by HPC admins, loaded with <code>module load</code> Fast; easy to use; no setup required Limited versions; may conflict with workflow needs; not every tool is available on every system Containers (Apptainer/Singularity) Portable, isolated software environments Reproducible; portable; avoids dependency issues; huge repositories of pre-built containers available Requires container knowledge; introduces slightly more complexity into scripts and workflows <p>Of the several options described above, we highly recommend using containers for workflow development, especially when using workflow languages like Nextflow. Containers bundle up everything a tool needs, including, dependencies, libraries, OS layers, into a single portable image. This image is self-contained (hence the name!) and doesn't interact or conflict with any of the software installed on your computer. On HPC, that means:</p> <ul> <li>No dependency conflicts: every container runs in its own isolated environment, unaffected by other users or system modules</li> <li>Reproducibility: the same container image can be used across clusters, clouds, or laptops, ensuring identical software behaviour everywhere</li> <li>Reduced maintenace: you don't need to worry about installing, updating, and debugging complex software stacks.</li> </ul> <p>We source prebuilt containers from the BioContainers on the quay.io registry and Seqera containers. </p> <p>One container per tool</p> <p>Containers are an ideal way to package up a tool with all of its dependencies, but are still susceptible to version and dependency conflicts if you try to package up too many tools into the one image. As such, the general best practice is that one container is built around one tool. As a consequence, when writing workflows, we also typically want to aim for one tool and one container per process. In fact, nexflow only lets us use one container for a single process. This forces us to break up our workflow into smaller, modular chunks, which helps improve readability and maintainability of our pipelines. </p> <p>Note that there are cases where we may package up two very closely related tools into a single container and Nextflow process, either because they are part of a larger suite of software, or they are known to work well together, or it would introduce unnecessary complexity into our pipeline to separate them. However, this is the exception, not the rule.</p>"},{"location":"part1/01_2_hpc_software/#122-a-simple-command","title":"1.2.2 A simple command","text":"<p>Let's start exploring HPC software with a simple command: accessing the help page for <code>fastqc</code>.</p> <p>Exercise: Try to run fastqc</p> <p>Try running the following command:</p> <pre><code>fastqc --help\n</code></pre> Result... <p>You will see:</p> <pre><code>bash: fastqc: command not found\n</code></pre> <p>The command fails because <code>fastqc</code> is not available by default. On HPC, you\u2019ll need to either load a pre-installed module or use a container.</p>"},{"location":"part1/01_2_hpc_software/#123-using-modules","title":"1.2.3 Using modules","text":"<p>Modules are the standard way to access centrally installed software on HPC systems. There are lots of modules pre-installed on HPCs like Gadi and Setonix that allow you to use many common tools, including lots of bioinformatics tools. <code>fastqc</code> is one such tool that is pre-installed on both of these HPC systems.</p> <p>Exercise: Finding the <code>fastqc</code> module</p> <p>You can list available modules on your system with:</p> <pre><code>module avail\n</code></pre> <p>The terminal will fill up with a long list of available modules:</p> Gadi (PBS)Setonix (Slurm) Available modules<pre><code>-------------------------------------------------------------------- /opt/Modules/modulefiles ---------------------------------------------------------------------\npbs  singularity  \n\n----------------------------------------------------------------- /opt/Modules/v4.3.0/modulefiles -----------------------------------------------------------------\ndot  module-git  module-info  modules  null  use.own  \n\n-------------------------------------------------------------------- /apps/Modules/modulefiles --------------------------------------------------------------------\nabaqus/2020                gamess/2021-R2-p1              intel-dnnl/2025.2.0              lammps/3Aug2022           openquake/3.10.1                              \nabaqus/2021                gamess/2022-R2                 intel-dpct/2021.1.1              lammps/3Mar2020           openquake/3.11.2                              \n</code></pre> Available modules<pre><code>----------------------------------------------- /opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0 ------------------------------------------------\ncray-hdf5-parallel/1.14.3.1        cray-mpixlate/1.0.5        cray-parallel-netcdf/1.12.3.13 (D)\ncray-hdf5-parallel/1.14.3.3        cray-mpixlate/1.0.6        cray-parallel-netcdf/1.12.3.15\ncray-hdf5-parallel/1.14.3.5 (D)    cray-mpixlate/1.0.7 (D)    cray-parallel-netcdf/1.12.3.17\n\n---------------------------------------------- /software/setonix/2025.08/modules/zen3/gcc/14.2.0/astro-applications ----------------------------------------------\napr-util/1.6.3                  casacore/3.4.0-adios2        giant-squid/2.3.0           mwalib/1.8.7                wcstools/3.9.7\napr/1.7.5                       casacore/3.4.0-openmp        hyperbeam/0.10.2-cpu        pgplot/5.2.2                wsclean/2.9-idg\n</code></pre> <p>Note that on Setonix some modules are marked with a <code>(D)</code>: this indicates that it is the default version of that module. However, Setonix disables automatic loading of default modules; instead, you must explicitly specify the version you want when loading a module.</p> <p>This list can be navigated with the <code>j</code> and <code>k</code> keys to move down and up, respectively. To exit the list, press <code>q</code>.</p> <p>Note that the modules appear in a format of <code>&lt;TOOL NAME&gt;/&lt;VERSION&gt;</code>. Often, several versions of a tools will be pre-installed on the system for you to choose between.</p> <p>Exercise: Loading the <code>fastqc</code> module</p> <p>Find the available <code>fastqc</code> versions on your system with:</p> <pre><code>module avail fastqc\n</code></pre> <p>Load the module on your system and confirm the version: </p> Gadi (PBS)Setonix (Slurm) <pre><code>module load fastqc\nfastqc --version\n</code></pre> Output<pre><code>FastQC v0.12.1\n</code></pre> <pre><code>module load fastqc/0.11.9--hdfd78af_1\nfastqc --version\n</code></pre> Output: fastqc version<pre><code>FastQC v0.11.9\n</code></pre> <p>Once loaded, you can rerun the <code>fastqc --help</code> command and verify that it now runs successfully.</p> <pre><code>fastqc --help\n</code></pre> <pre><code>            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n        [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n\n...\n</code></pre> <p>Modules are quick and convenient, but they depend on what your HPC administrators provide. As we saw above, Gadi and Setonix provided different versions of FastQC. Relying on administrators to install modules for you can be a barrier to running your workflows. </p> <p>Running your own modules</p> <p>If you like, you can make your own software \"module loadable\" for all members of your project space. This is a nice way to share software within a project but can be tedious to maintain as the number of tools and dependencies grow. </p> <p>If you're interested in trying this our for yourself, take a look at this tutorial from NCSU on custom modules.</p> <p>When you need specific versions or multiple conflicting tools, containers are a far better option.</p>"},{"location":"part1/01_2_hpc_software/#124-using-containers","title":"1.2.4 Using containers","text":"<p>Containers are portable software environments: they package everything your tool needs to run. </p> <p>Exercise: Load the Singularity module</p> <p>Unload any existing module first:</p> <pre><code>module unload fastqc\n</code></pre> <p>Singularity, like other software, is not loaded by default. On Gadi, there is just one default Singularity module, and can be simply loaded with <code>module load</code>. On Setonix, you can find the versions of Singularity available with <code>module avail</code>:</p> Gadi (PBS)Setonix (Slurm) <pre><code>module load singularity\n</code></pre> <p><pre><code>module avail singularity\nmodule load singularity/4.1.0-slurm\n</code></pre> <pre><code>-------------------------------------------------- /software/setonix/2025.08/modules/zen3/gcc/14.2.0/utilities ---------------------------------------------------\nsingularityce/3.11.4    singularityce/4.1.0 (D)\n\n------------------------------------------------------------ /software/setonix/2025.08/pawsey/modules ------------------------------------------------------------\nsingularity/3.11.4-askap-gpu    singularity/3.11.4-mpi       singularity/3.11.4-slurm       singularity/4.1.0-mpi-gpu    singularity/4.1.0-nompi\nsingularity/3.11.4-askap        singularity/3.11.4-nohost    singularity/4.1.0-askap-gpu    singularity/4.1.0-mpi        singularity/4.1.0-slurm (D)\nsingularity/3.11.4-mpi-gpu      singularity/3.11.4-nompi     singularity/4.1.0-askap        singularity/4.1.0-nohost\n</code></pre></p> <p>To run a command or script inside a singularity container, you simply run <code>singularity exec /path/to/image.img &lt;your command&gt;</code>, where <code>/path/to/image.img</code> is the path to the container image that you wish to use. There are also numerous options and flags that you can provide to the <code>singularity exec</code> command itself. For the purposes of today's workshop, we will be using Singularity's default parameters, so we don't need to provide any options. As part of these defaults, the command will be run within your current working directory, as if you simply ran the command directly.</p> <p>Exercise: Run FastQC in a Singularity container</p> <p>You should find a pre-built container image for running the <code>fastqc</code> command at <code>../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img</code>.  </p> <p>You can execute the <code>fastqc --help</code> inside it with:</p> <pre><code>singularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img fastqc --help\n</code></pre> <p>The output should look familiar:</p> <pre><code>            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n        [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n\n...\n</code></pre> <p>How are you going?</p> <p>If you're following along so far, let us know by reacting on zoom with a \" Yes\".</p> <p>If you're running into any issues, please react with a \" No\" and we can help out before we move on to the next section.</p> <p>Later, when we set up Nextflow to run on the HPC, we will configure it to use singularity containers. Behind the scenes, this process of running a command within a container is essentially what Nextflow does for us.</p>"},{"location":"part1/01_3_hpc_architecture/","title":"1.3 HPC architecture","text":"<p>Learning objectives</p> <ul> <li>Describe the HPC system components that workflows interact with</li> <li>Describe the roles of login nodes, compute nodes, and shared storage in HPC systems</li> <li>Distinguish between login nodes and compute nodes and know where workflows execute</li> <li>Explain how shared filesystems are accessed and managed safely in workflows</li> <li>Define job resource requirements (CPU, memory, walltime) and their impact on scheduling</li> </ul>"},{"location":"part1/01_3_hpc_architecture/#131-typical-hpc-architecture","title":"1.3.1 Typical HPC architecture","text":"<p>While HPCs can look intimidating, their architecture follows a simple structure that supports large-scale computation through shared resources. From a workflow perspective, this architecture means there are a few important realities to accept: work is not run interactively, resources must be requested rather than assumed and everything is governed by shared access.</p> <p></p>"},{"location":"part1/01_3_hpc_architecture/#login-nodes","title":"Login nodes","text":"<p>When a user connects to an HPC, they first land on a login node. This is a shared access point used to prepare work, not perform computations. From here, users submit jobs to the scheduler, monitor their progress and organise their project directories. The login node exists only to coordinate access to the system, and because it is shared by many people at once, it must not be overloaded with computational tasks.</p>"},{"location":"part1/01_3_hpc_architecture/#compute-nodes","title":"Compute nodes","text":"<p>The real work happens on the compute nodes. These are powerful machines with many CPU cores, large amounts of memory and fast access to storage. When you run a workflow, the scheduler assigns the individual tasks of the workflow to available compute nodes based on the resources requested. This separation between the login node and compute nodes allows users to interact with the system while computation is queued and executed elsewhere.</p>"},{"location":"part1/01_3_hpc_architecture/#queues","title":"Queues","text":"<p>HPC systems divide compute resources into queues (also called \"partitions\" in Slurm). Each queue represents a group of compute nodes with specific hardware characteristics, limits, and intended usage.</p> <p>Queues help balance the system between different types of work, for example, short interactive tasks, long-running simulations, or large parallel jobs. Each queue enforces policies on maximum runtime (walltime), maximum cores or memory per job, job priority and eligible users or projects.</p> <p>Because queues are mapped to distinct sets of compute nodes, requesting the right queue for your job is important for both performance and fairness.</p> <p>Queue names and limits differ across infrastructure providers but some common examples include:</p> Queue type Description Typical limits Normal/work Default queue for most batch jobs Moderate walltime (e.g. 24\u201348 h), general-purpose CPUs Express/short Prioritised for rapid turnaround Small jobs, short walltime Large/high memory For jobs requiring many CPUs or nodes Long walltime, higher resource requests Copy For copying data to and from the system Low CPU and RAM availability, moderate walltime (10-48h) GPU Access to GPU-enabled nodes GPU-specific workloads only <p>For details about queue limits and scheduling policies on systems used today, see:</p> <ul> <li>Setonix (Slurm) \u2013 Running Jobs on Setonix (Pawsey)</li> <li>Gadi (PBS Pro) \u2013 Queue Limits (NCI)</li> </ul>"},{"location":"part1/01_3_hpc_architecture/#internet-access-on-compute-nodes","title":"Internet access on compute nodes","text":"<p>Often, a workflow may need access to resources on the internet, such as Singularity container images and large reference databases. This can become an issue on HPCs, as some systems restrict internet access on the compute nodes. While on Setonix, all compute nodes are given internet access and can be used for downloading such resources, on Gadi, most of the compute nodes are kept offline; instead, only the compute nodes that make up the <code>copyq</code> queue are allowed to access the internet. Furthermore, this queue has a few important constraints, primarily that it only allows single CPU jobs and a maximum walltime of 10 hours. As such, when designing your workflows, you need to make sure you are working within these constraints.</p> <p>As a general rule of thumb, it is always best to have as much as possible of your workflow's input data pre-downloaded, as in any case, downloading data from the internet can be a major bottleneck. Where possible, running your workflow 'offline' is best practice.</p> <p>Mind your Qs</p> <p>Make sure you are using the right queue for the right job! Requesting the wrong queue can lead to long wait times due to other jobs getting higher priority, or to failures or outright rejection from the scheduler due to invalid resource requests. Always consult the documentation specific to your system before designing, configuring, and running your workflow.</p> <p>Later on in this workshop, we will see some simple examples of how we can dynamically request the queue for Nextflow processes based on the resources they require.</p> <p>Remember to always consult the documentation specific to your HPC before writing and running workflows!</p>"},{"location":"part1/01_3_hpc_architecture/#shared-storage","title":"Shared storage","text":"<p>All nodes are connected to a shared parallel filesystem. This is a large, high-speed storage system where input data, reference files and workflow outputs are kept. Because it is shared across all users, it enables collaborative research and scalable workflows. However, it also introduces constraints around file organisation and performance, which is why workflows must be careful about how they read and write data here.</p> <p>Overwriting Files in Shared Filesystems</p> <p>Because the filesystem is shared, multiple jobs or users writing to the same file, especially if it has a common name, can accidentally overwrite each other\u2019s data or cause race conditions.</p> <p>Most tools and scripts write to files in a way that replaces the entire file. On shared systems like /scratch, where many users and jobs access the same space, this can lead to conflicts or data loss.</p> <p>It\u2019s good practice to:</p> <ul> <li>Use unique filenames (e.g., include job ID or timestamp).</li> <li>Avoid simultaneous writes unless explicitly managed.</li> <li>Add file checks and error handling.</li> </ul>"},{"location":"part1/01_3_hpc_architecture/#the-job-scheduler","title":"The job scheduler","text":"<p>At the centre of everything is the job scheduler. Rather than allowing users to run programs directly, HPCs rely on a scheduling system (e.g. Slurm or PBS Pro) to manage fair access to shared compute resources. When a job is submitted, it enters a queue where the scheduler decides when and where it will run. Jobs are matched to compute nodes based on requested resources like CPU, memory and runtime.</p> <p>Schedulers like PBS Pro and Slurm use queues to group jobs that share similar resource and policy constraints. When you submit a job, it\u2019s placed in the appropriate queue, and the scheduler continuously evaluates all queued jobs to decide which can start next.</p> <p>Each job\u2019s shape is determined by three key factors:</p> <ul> <li>CPU: how many processor cores it needs</li> <li>Memory: how much RAM it requires</li> <li>Walltime: how long it is allowed to run</li> </ul> <p>Once submitted, your job enters a queue. Unlike a simple first-come-first-served queue, the scheduler constantly reshuffles and fits jobs together to maximise system usage.</p> <p>The order in which jobs run depends on several factors:</p> <ul> <li>Job priority: determined by project, queue, and fair-share usage</li> <li>Requested resources: smaller jobs can often \u201cslot in\u201d sooner</li> <li>Queue limits: different queues prioritise short, long, or interactive jobs</li> </ul> <p>Getting the shape right matters. Underestimating the resources you job requires will cause it to fail. What is less immediately obvious is that overestimating your needs is also detrimental, as it makes your job harder to fit. Common outcomes from overestimating your job's requirements include:</p> <ul> <li>Longer queue times: large, awkwardly-shaped jobs wait for space</li> <li>Wasted capacity: unused cores or memory that could have run other jobs</li> <li>Wasted money: HPC providers will charge you for the CPUs and memory you request. Wasted capacity = wasted energy!</li> </ul> <p>Understanding Job Scheduling with Tetris</p> <p>Think of the scheduler like a giant game of Tetris, where every job you submit has its own unique \u201cshape.\u201d When you submit a batch job, you describe the resources it needs, this defines the shape of your job piece. The scheduler\u2019s task is to fit all these different pieces together as efficiently as possible across the available compute nodes.</p> <p></p> <p>Just like in Tetris, the scheduler aims to fill every gap and keep the system running smoothly. Small, well-shaped jobs often fall neatly into open spaces, while larger ones wait for the perfect fit.</p>"},{"location":"part1/01_3_hpc_architecture/#132-submitting-scripts-to-the-scheduler","title":"1.3.2 Submitting scripts to the scheduler","text":"<p>To familiarise ourselves with submitting jobs to the scheduler, we will once again use <code>fastqc</code> as an example. We have an example script for running <code>fastqc</code> in the <code>scripts/</code> directory:</p> Gadi (PBSpro)Setonix (Slurm) scripts/fastqc.pbs.sh<pre><code>#!/bin/bash\n\nmodule load singularity\n\nSAMPLE_ID=\"NA12878_chr20-22\"\nREADS_1=\"../data/fqs/${SAMPLE_ID}.R1.fq.gz\"\nREADS_2=\"../data/fqs/${SAMPLE_ID}.R2.fq.gz\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nsingularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img \\\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> scripts/fastqc.slurm.sh<pre><code>#!/bin/bash\n\nmodule load singularity/4.1.0-slurm\n\nSAMPLE_ID=\"NA12878_chr20-22\"\nREADS_1=\"../data/fqs/${SAMPLE_ID}.R1.fq.gz\"\nREADS_2=\"../data/fqs/${SAMPLE_ID}.R2.fq.gz\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nsingularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img \\\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> <p>The script does a few things:</p> <ol> <li>It loads the <code>singularity</code> module. We'll need this to run the <code>fastqc</code> command when the job gets submitted to the compute node</li> <li>It defines a few bash variables that point to the input FASTQ data</li> <li>It creates an output directory called <code>results/fastqc_${SAMPLE_ID}_logs/</code>, where <code>${SAMPLE_ID}</code> will get evaluated to <code>NA12878_chr20-22</code></li> <li>It runs the <code>fastqc</code> command within a singularity container by prefixing the command with <code>singularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img</code>.</li> </ol> <p>Your container is ready to go!</p> <p>For the sake of expediency, we have pre-downloaded the <code>fastqc</code> singularity container image for you at <code>../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img</code>.</p> <p>This is everything we need to run the job; we just have to submit the script to the HPC scheduler. In doing so, we will provide the following details to the scheduler:</p> <ul> <li>Project name: This is the HPC project that we want to run our job under, used to determine which filesystems we have access to and what project to bill to.</li> <li>Job name: A name to give our job, to help distinguish it from others we or other people may be running; we will call our job <code>fastqc</code></li> <li>Queue: Which compute queue to submit our job to. Different queues have different resource limits. We will use the standard node for our HPCs.</li> <li>Number of nodes and/or CPUs: A job can request differing numbers of nodes and CPUs it requires to run; we will just request 1 CPU on 1 node.</li> <li>Amount of memory: The amount of RAM that our job requires to run; we will request 1 gigabyte.</li> <li>Walltime: The time our job needs to complete; we will request 1 minute.</li> </ul> <p>Exercise: Submitting the fastqc script to the HPC</p> <p>In the VSCode terminal (<code>Ctrl + J</code> (Windows/Linux) / <code>Cmd + J</code> (Mac)), type in the following code. Note the backslashes (<code>\\</code>): these indicate that you will continue the command on the next line. When you type a backslash and press <code>Enter</code>, you will get a <code>&gt;</code> character in your terminal prompt, indicating that you can continue writing your command here. Ensure you put a space before the backslashes.</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>qsub \\\n    -P $PROJECT \\\n    -N fastqc \\\n    -q normalbw \\\n    -l ncpus=1 \\\n    -l mem=1GB \\\n    -l walltime=00:01:00 \\\n    -l storage=scratch/$PROJECT \\\n    -l wd \\\n    scripts/fastqc.pbs.sh\n</code></pre> <p>Let's deconstruct this command:</p> <ul> <li><code>-P $PROJECT</code>: This specifies the project using the environment variable <code>$PROJECT</code>, which is pre-defined with your default NCI project ID.</li> <li><code>-N fastqc</code>: This specifies the job name as <code>fastqc</code>.</li> <li><code>-q normalbw</code>: This specifies the queue as <code>normalbw</code>; this is a general queue on NCI for regular jobs that don't need any special resources.</li> <li><code>-l key=value</code>: The <code>-l</code> parameter lets us specify resources that we require in a <code>key=value</code> format:<ul> <li><code>-l ncpus=1</code>: This specifies that we want 1 CPU to run our job.</li> <li><code>-l mem=1GB</code>: Here we request 1 GB of memory.</li> <li><code>-l walltime=00:01:00</code>: This requests 1 minute of walltime; walltime is specified in the format <code>HH:MM:SS</code></li> <li><code>-l storage=scratch/$PROJECT</code>: This tells Gadi to mount the scratch space for our project; this is not done by default, so we always need to specify it.</li> <li><code>-l wd</code>: This tells Gadi to run the job in the current working directory.</li> </ul> </li> </ul> <pre><code>sbatch \\\n    --account=$PAWSEY_PROJECT \\\n    --job-name=fastqc \\\n    --partition=work \\\n    --reservation=NextflowHPC \\\n    --nodes=1 \\\n    --ntasks=1 \\\n    --cpus-per-task=1 \\\n    --mem=1GB \\\n    --time=00:01:00 \\\n    scripts/fastqc.slurm.sh\n</code></pre> <p>Let's deconstruct this command:</p> <ul> <li><code>--account=$PAWSEY_PROJECT</code>: This specifies the project using the environment variable <code>$PAWSEY_PROJECT</code>, which is pre-defined with your default Pawsey project ID.</li> <li><code>--job-name=fastqc</code>: This specifies the job name as <code>fastqc</code>.</li> <li><code>--partition=work</code>: This specifies the queue (also called \"partitions\" on Setonix) as <code>work</code>; this is a general queue on Pawsey for regular jobs that don't need any special resources.</li> <li><code>--reservation=NextflowHPC</code>: This is specific to this workshop, letting us all work on a reserved compute node without competing for resources with other users.</li> <li><code>--nodes=1 --ntasks=1 --cpus-per-task=1</code>: This specifies that we want 1 CPU on 1 node to run our job.</li> <li><code>--mem=1GB</code>: Here we request 1 GB of memory.</li> <li><code>--time=00:01:00</code>: This requests 1 minute of walltime; walltime is specified in the format <code>HH:MM:SS</code></li> </ul> <p>Once you have written out the command, ensure you have no final backslash, and press <code>Enter</code> to submit the script to the scheduler.</p> <p>Once submitted, you can monitor the progress of your job with the following command:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>qstat -u ${USER}\n</code></pre> <pre><code>squeue -u ${USER}\n</code></pre> <p>This will output a list of all running jobs and their status:</p> Gadi (PBSpro)Setonix (Slurm) Output<pre><code>gadi-pbs:\n                                                                Req'd  Req'd   Elap\nJob ID               Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n-------------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n123456789.gadi-pbs   usr123   normal   fastqc        --    1   1  1024m 00:10 Q   --\n</code></pre> <p>The <code>S</code> column near the end shows the status of the job, with typical codes being <code>Q</code> for queued, <code>R</code> for running, and <code>E</code> for finished or ending jobs.</p> Output<pre><code>JOBID        USER ACCOUNT                   NAME   EXEC_HOST ST     REASON START_TIME       END_TIME  TIME_LEFT NODES   PRIORITY       QOS\n12345678 username pawsey1234                fastqc       n/a PD       None N/A                   N/A      10:00     1      75423    normal\n</code></pre> <p>The <code>ST</code> column near the middle shows the status of the job, with typical codes being <code>PD</code> for pending or queued, <code>R</code> for running, and <code>CG</code> for finished jobs.</p> <p>Once complete, you should see a <code>results/</code> folder; inside, there should be a sub-folder called <code>fastqc_NA12878_chr20-22_logs</code> containing several <code>.zip</code> and <code>.html</code> files - the output results and reports from <code>fastqc</code>:</p> <pre><code>ls results/fastqc_NA12878_chr20-22_logs/\n</code></pre> Output<pre><code>NA12878_chr20-22.R1_fastqc.html NA12878_chr20-22.R2_fastqc.html\nNA12878_chr20-22.R1_fastqc.zip  NA12878_chr20-22.R2_fastqc.zip\n</code></pre> <p>Before moving on, delete the <code>results/</code> directory:</p> <pre><code>rm -r results\n</code></pre>"},{"location":"part1/01_3_hpc_architecture/#133-cleaning-up-job-submission","title":"1.3.3 Cleaning up job submission","text":"<p>The above command is quite long, and would be a pain to write out every time you want to submit a script, especially if you want to run the same script several times with different samples. Luckily, there is a better way of specifying the resources required by a script. Both PBS Pro and Slurm support using special comments at the top of the script file itself for specifying the resources it requires.</p> <p>Exercise: A cleaner job submission</p> <p>A good practice is to specify as many of the scheduler parameters within the header of the script itself as special comments. Note, however, that only static - i.e. unchanging - parameters can use this feature; if you need to dynamically assign resources to jobs, you will still need to use the commandline parameters.</p> Gadi (PBSpro)Setonix (Slurm) <p>Update the <code>scripts/fastqc.pbs.sh</code> script with the following header comments:</p> scripts/fastqc.pbs.sh<pre><code>#!/bin/bash\n#PBS -P vp91\n#PBS -N fastqc\n#PBS -q normalbw\n#PBS -l ncpus=1\n#PBS -l mem=1GB\n#PBS -l walltime=00:01:00\n#PBS -l storage=scratch/vp91\n#PBS -l wd\n\nmodule load singularity\n\nSAMPLE_ID=\"NA12878_chr20-22\"\nREADS_1=\"../data/fqs/${SAMPLE_ID}.R1.fq.gz\"\nREADS_2=\"../data/fqs/${SAMPLE_ID}.R2.fq.gz\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nsingularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img \\\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> <p>Note how we need to explicitly state the project name for both the <code>-P</code> and <code>-l storage</code> parameters.</p> <p>Update the <code>scripts/fastqc.slurm.sh</code> script with the following header comments:</p> scripts/fastqc.slurm.sh<pre><code>#!/bin/bash\n#SBATCH --account=courses01\n#SBATCH --job-name=fastqc\n#SBATCH --partition=work\n#SBATCH --reservation=NextflowHPC\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1GB\n#SBATCH --time=00:01:00\n\nmodule load singularity/4.1.0-slurm\n\nSAMPLE_ID=\"NA12878_chr20-22\"\nREADS_1=\"../data/fqs/${SAMPLE_ID}.R1.fq.gz\"\nREADS_2=\"../data/fqs/${SAMPLE_ID}.R2.fq.gz\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nsingularity exec ../singularity/quay.io-biocontainers-fastqc-0.12.1--hdfd78af_0.img \\\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> <p>Note how we need to explicity state the project name for the <code>--account</code> parameter.</p> <p>With the script updated, you can simply run your HPC submission command without any of the previously supplied parameters. We will also redirect the output of the submission command (which prints a message containing the job ID) to a file called <code>run_id.txt</code> for use in the next lesson.</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>qsub scripts/fastqc.pbs.sh &gt; run_id.txt\n</code></pre> <pre><code>sbatch scripts/fastqc.slurm.sh &gt; run_id.txt\n</code></pre> <p>The job can be monitored the same way as before, and once complete, should produce the exact same output:</p> <pre><code>ls results/fastqc_NA12878_chr20-22_logs/\n</code></pre> Output<pre><code>NA12878_chr20-22.R1_fastqc.html NA12878_chr20-22.R2_fastqc.html\nNA12878_chr20-22.R1_fastqc.zip  NA12878_chr20-22.R2_fastqc.zip\n</code></pre> <p>How are you going?</p> <p>If you're following along so far, let us know by reacting on zoom with a \" Yes\".</p> <p>If you're running into any issues, please react with a \" No\" and we can help out before we move on to the next section.</p>"},{"location":"part1/01_4_smarter/","title":"1.4 Work smarter, not harder!","text":"<p>Learning objectives</p> <ul> <li>Understand how HPC resources affect job scheduling and performance</li> <li>Describe how HPC scheduling and resource limitations shape pipeline configuration.</li> <li>Differentiate between multithreading and scatter\u2013gather approaches to parallelisation</li> <li>Identify how to size resource requests appropriately for each process in a workflow</li> <li>Apply resource-aware design principles to improve job efficiency and reproducibility</li> </ul> <p>HPC systems give us access to large amounts of compute, but that doesn\u2019t mean we should use resources carelessly. Misusing compute leads to long queue times, wasted allocation, unstable workflows and unhappy HPC administrators. Designing resource-aware workflows is essential for performance and fair use.</p>"},{"location":"part1/01_4_smarter/#141-know-thyself-tracking-resource-usage-of-your-jobs","title":"1.4.1 Know thyself: tracking resource usage of your jobs","text":"<p>HPC systems are constantly measuring your resource usage. You can use their built in tools to measure actual use. The tools available to you will depend on the job scheduler and the administrator's implementation of the scheduler.</p> <p>Exercise: Inspect a previous job</p> <p>At the end of the previous lesson, we saved the job ID to a file called <code>run_id.txt</code>. We can use that ID to inspect the resources used by the job:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>JOBID=$(cat run_id.txt)\nqstat -xf ${JOBID} | grep -E \"Resource_List|resources_used\"\n</code></pre> <p>This will generate an output something like:</p> Output<pre><code>resources_used.cpupercent = 50\nresources_used.cput = 00:00:04\nresources_used.jobfs = 0b\nresources_used.mem = 451276kb\nresources_used.ncpus = 1\nresources_used.vmem = 451276kb\nresources_used.walltime = 00:00:08\nResource_List.jobfs = 104857600b\nResource_List.mem = 1073741824b\nResource_List.mpiprocs = 1\nResource_List.ncpus = 1\nResource_List.nodect = 1\nResource_List.place = free\nResource_List.select = 1:ncpus=1:mpiprocs=1:mem=1073741824:job_tags=normalb\nResource_List.storage = scratch/vp91\nResource_List.walltime = 00:01:00\nResource_List.wd = 1\n</code></pre> <p>We can see that in this example run, CPU usage was at 50%. Since we only requested 1 CPU, there is no more room for improvement here. We can also see that 451,276 KB (= 440.7 MB) of memory was used, while we requested 1 GB (= 1024 MB), giving us a memory efficiency of ~43%.</p> <p>Handily, Gadi also produces a summary of the resource usage at the end of its standard output stream. After each of the submitted jobs completed, you should have found two new files present in your working directory, called <code>fastqc.o&lt;JOBNUM&gt;</code> and <code>fastqc.e&lt;JOBNUM&gt;</code>, which are the standard output and standard error streams generated by the job. <code>&lt;JOBNUM&gt;</code> in this case is the numeric part of the job ID; e.g. if the job ID was 12345.gadi-pbs, the standard output file would be <code>fastqc.o12345</code>. The final 12 lines of the <code>fastqc.o&lt;JOBNUM&gt;</code> file will contain the resource summary:</p> <pre><code>JOBNUM=$(echo ${JOBID} | sed -E -e 's/\\.gadi\\-pbs$//g')\ntail -n 12 fastqc.o${JOBNUM}\n</code></pre> Output<pre><code>======================================================================================\n                Resource Usage on 2025-11-07 14:42:47:\nJob Id:             154138075.gadi-pbs\nProject:            vp91\nExit Status:        0\nService Units:      0.00\nNCPUs Requested:    1                      NCPUs Used: 1\n                                        CPU Time Used: 00:00:04\nMemory Requested:   1.0GB                 Memory Used: 440.7MB\nWalltime requested: 00:01:00            Walltime Used: 00:00:08\nJobFS requested:    100.0MB                JobFS used: 0B\n======================================================================================\n</code></pre> <p>Again, we see that 440.7 MB was used out of the total requested 1 GB.</p> <pre><code>JOBID=$(sed -E -e 's/^Submitted batch job //g' run_id.txt)\nsacct -j ${JOBID} --format=JobID,JobName,Elapsed,State,AllocCPUS,TotalCPU,MaxRSS\nseff ${JOBID}\n</code></pre> <p>This will generate an output something like:</p> Output<pre><code>JobID           JobName    Elapsed      State  AllocCPUS   TotalCPU     MaxRSS\n------------ ---------- ---------- ---------- ---------- ---------- ----------\n34324060         fastqc   00:00:17  COMPLETED          2  00:07.945\n34324060.ba+      batch   00:00:17  COMPLETED          2  00:07.942    376996K\n34324060.ex+     extern   00:00:17  COMPLETED          2  00:00.003          0\n\nJob ID: 34324060\nCluster: setonix\nUser/Group: username/username\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:00:08\nCPU Efficiency: 23.53% of 00:00:34 core-walltime\nJob Wall-clock time: 00:00:17\nMemory Utilized: 368.16 MB\nMemory Efficiency: 35.95% of 1.00 GB (1.00 GB/node)\n</code></pre> <p>We can see that in this example run, CPU usage was at 23.53%. Since we only requested 1 CPU, there is no more room for improvement here. We can also see that the memory efficiency was ~36%, using ~368 MB of the requested 1 GB.</p> <p>Before moving on, delete the <code>results/</code> directory, as well as the scheduler outputs.</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>rm -r results fastqc.*\n</code></pre> <pre><code>rm -r results slurm-*\n</code></pre>"},{"location":"part1/01_4_smarter/#142-resource-awareness-right-sizing","title":"1.4.2 Resource awareness: right sizing","text":"<p>At its core, HPC efficiency is about matching the structure of your workflow to the available compute. It is therefore beneficial to be \"resource aware\" in your approach to running jobs. This involves understanding how much time, CPU, and memory each tool in your workflow actually needs and requesting enough.</p> <p>Here we focus on 3 metrics:</p> <ul> <li>Elapsed time: the total walltime for the job</li> <li>CPU time: how much CPU time was actually used</li> <li>Maximum memory (RAM): the peak memory footprint of the job</li> </ul> <p>In the context of running a workflow made up of multiple steps, each running a different tool, we consider each step separately in optimising for efficiency.</p> <p>Let's look again at our workflow:</p> <p></p> <p>As we touched on in HPC for workflows, each stage of this workflow has differeing resource requirements and bottlenecks:</p> Step Dominant resource Characteristics Quality control I/O-bound Reads many files; CPU idle time high Read alignment CPU-bound CPU pegged near 100%; memory stable Variant calling CPU + memory CPU ~90%, high steady memory usage <p>We will observe these constraints in subsequent lessons when we run and optimise our workflows.</p> <p>Where can I find this information?</p> <p>Most mature bioinformatics tools document their approximate resource usage. Keep in mind, documentation will not reflect the reality of your specific dataset and environment. Look at your own resource usage and scaling behaviour to test this.</p> <p>It's also very helpful to have small-scale datasets to initially test your workflows with. Common practices are to subset FASTQs to a small fraction of the reads, and to subset aligned data to the smaller chromosomes (e.g. chr22 in human data). This will help you quickly (and cheaply!) determine how your workflow behaves at each step and how it will scale with larger datasets.</p> <p>The more you practice tuning processes for efficiency, the faster you'll develop an intuition for scaling.</p>"},{"location":"part1/01_4_smarter/#cpu-efficiency","title":"CPU efficiency","text":"<p>CPU efficiency refers to how well the CPUs requested for the job are being utilised. It is commonly expressed as a percentage or as a range from 0-1. It is calculated as:</p> <pre><code>CPU time / walltime / number of CPUs\n</code></pre> <p>Where:</p> <ul> <li>CPU time is the cumulative time that all CPUs were actively working; and</li> <li>Walltime is the real-world time that the job took to run: i.e. the time as measured by a clock on the wall</li> </ul> <p>Values near 1 mean your job used all the CPUs efficiently. Values much lower than 1 mean that the CPUs were sitting idle for a lot of the time; this can be because the job was waiting on I/O, or otherwise you may have over-allocated CPUs to your job.</p> <p>As an example, suppose you requested 4 CPUs for a job which ran for 1 hour (walltime). If each CPU was utilised for 100% of the time, then CPU time would be 4 hours and CPU efficiency would be:</p> <p>4 hours CPU time / 1 hour walltime / 4 CPUs = 100%</p> <p>On the other hand, if the job actually only used 1 of those CPUs for that entire hour, then CPU time would only be 1 hour and CPU efficiency would be:</p> <p>1 hour CPU time / 1 hour walltime / 4 CPUS = 25%</p>"},{"location":"part1/01_4_smarter/#memory-ram-efficiency","title":"Memory (RAM) efficiency","text":"<p>Memory efficiency describes how much of your allocated memory was truly needed. If your job used close to the requested memory, you're right sized. On the other hand, if the maximum memory used by the job is much lower than requested, then you over-allocated. Your job will fail if maximum memory exceeds the requested allocation.</p> <p>Note that it is a good idea to slightly over-request memory, since it is quite difficult to accurately predict exactly how much memory your job will need, and if you don't have enough of a buffer, you may find that slight fluctuations in the required memory will cause some jobs to fail. From a cost perspective, it is better to pay slightly more for a little more memory than pay twice to re-run a failed job.</p>"},{"location":"part1/01_4_smarter/#walltime-awareness","title":"Walltime awareness","text":"<p>As mentioned above, walltime is the real-world time it takes for your job to run from start to finish. Schedulers use this value to plan future jobs being run by yourself and others. Overestimating walltime will keep you job in the queue for longer as it will have to wait for a suitable time slot to run. However, if you underestimate walltime, the job will be killed mid-run. As such, like memory, it is usually a good idea to slightly over-request walltime, esepcially since you are usually only charged for actual walltime used.</p>"},{"location":"part1/01_4_smarter/#143-optimising-your-jobs-for-efficiency","title":"1.4.3 Optimising your jobs for efficiency","text":"<p>Once you understand how your workflow uses resources, you can start to optimise it. Optimisation is about balancing speed, efficiency, and fair use of the system.</p> <p>In bioinformatics workflows, there are 2 main strategies used for increasing efficiency and throughput:</p> Level Definition Where it runs Memory model Example Multi-threading Multiple threads share memory space inside a single process. Threads cooperate to perform a single task faster. One node Shared memory Using the <code>-t</code> or <code>--threads</code> flag on a tool Multi-processing Multiple independent processes run in parallel, each with its own memory space. Often managed by a workflow engine or batch scheduler. One node or multiple nodes Separate memory per process Running <code>fastqc</code> on multiple samples simultaneously <p></p> <p>We can explore parallelisation methods of multi-threading and multi-processing in the context of our variant calling workflow and some small dummy data.</p> <p>Beware of the overheads</p> <p>While parallelism can be a great way to speed up your data processing, it doesn't always make things run faster. Splitting your jobs should only be done where it makes practical sense to do so. Keep in mind, the more you split your work up, the more issues you will have to contend with:</p> <ul> <li>More job scheduling, more file-handling, more merging</li> <li>Increased memory footprint and I/O for each sub-job</li> <li>Diminishing returns when the time saved is offset by coordination cost</li> </ul> <p>See this great explainer of parallelism from the GATK team.</p>"},{"location":"part1/01_4_smarter/#parallelisation-multi-threading","title":"Parallelisation: multi-threading","text":"<p>What is multi-threading?</p> <p>Multithreading means one program is using multiple cores on the same node to complete a single task faster. Our ability to do this is dependent on the tool being used. Some bioinformatics tools support multithreading via a flag like <code>--threads</code> or <code>-t</code>.</p> <p>In our variant calling workflow, some tools work best when given multiple cores on the same node, e.g. alignment with <code>bwa mem</code>. When you run:</p> <pre><code>bwa mem -t 4 ref.fasta sample_R1.fq.gz sample_R2.fq.gz &gt; alignment.sam\n</code></pre> <p>You're telling BWA to use 4 worker threads for parts of the alignment process that can be parallelised. Not all parts of the process can be parallelised.</p> <p>Be aware: not all tools can make use of threads effectively, and some are entirely single-threaded. This can stem from biological reasons, i.e. the computational problem simply needs to be run in a sequential manner. In other cases, the tool itself may simply not implement the <code>--threads</code> flag effectively.</p> <p>For example, the following graph was produced from CPU time and walltime data collected from several runs of <code>bwa mem</code> and <code>fastqc</code>, each with different numbers of threads:</p> <p></p> <p>Notice that for <code>bwa mem</code>, as the number of threads went up, the walltime (i.e. the number of real-world seconds it took for the job to complete) went down. Also notice that the CPU time stays mostly the same. This indicates that the same amount of computational time was being spent on the job, but because it was spread across more threads and run in parallel, it took fewer seconds to complete.</p> <p>While many tools benefit from multi-threading, FastQC is not one of them. It is designed to process one file per thread rather than splitting a single file across multiple threads. As you can see in the example above, the walltime for <code>fastqc</code> fluctuates a bit with increasing numbers of threads, but doesn't significantly decrease over time. The CPU time also stays fairly stable. This indicates that <code>fastqc</code> didn't speed up with additional threads.</p> <p>A note on terminology: threads \u2260 cores</p> <p>Often times you may see terms like \"core\", and \"thread\" used interchangeably, but in reality, these are related but distinct concepts.</p> <ul> <li>A thread is a software-level unit of execution. Each thread represents a single stream of instructions within a process.</li> <li>A core is a physical processing unit on a computer node. Each core can execute one thread at a time.</li> </ul> <p>In practice, when you run a command like:</p> <pre><code>bwa mem -t 8\n</code></pre> <p>You're telling bwa to spawn 8 threads. For those threads to be executed in parallel, you must also request 8 cores from the HPC for those threads to run on. If you were to request only 4 cores, those 8 threads will be competing for 4 cores which causes inefficiency and slower run times. On the other hand, if you ask for more cores than threads, those extra cores will sit idle, wasting allocation and increasing queue time. In summary, always try to request the exact number of cores your job requires. If you have a mixture of tools where one uses lots of threads and another is single-threaded, consider separating them out into separate jobs so that you use the HPC resources efficiently.</p> <p>You may also see the term CPU used interchangeably with core. Technically speaking, the CPU is the physical piece of hardware on the computer, composed of one or more cores. However, in practice, we are much more interested in the number of cores we are using than the number of physical CPUs. When you ask Gadi or Setonix for \"CPUs\", you are actually requesting cores.</p>"},{"location":"part1/01_4_smarter/#parallelisation-multi-processing","title":"Parallelisation: multi-processing","text":"<p>The other main approach to parallelisation is multi-processing, which means running independent processes at the same time, each with its own memory space and input data. Rather than speeding up a single task, it increases overall throughput by running many tasks in parallel. In bioinformatics workflows, this usually looks like processing multiple samples or genomic regions simultaneously.</p> <p>Multi-processing is typically implemented by the user in workflow design, rather than by a tool itself.</p> <p>A common multi-processing pattern is called scatter-gather. This involves initially splitting a dataset up into many independent jobs and later merging the results back together again. For example, within the context of short variant calling, a common practice is to run the variant calling tools on each chromosome separately. Since a sequencing read can only originate from one chromosome, and because these variants only affect one genomic region at a time, we can safely split the data up per chromosome and treat them independently. Once the data has been processed, the per-chromosome results can be merged back together for downstream analysis.</p> <p></p> <p>Does it make sense biologically to process in parallel?</p> <p>Not all parallelisation makes sense: it depends on what you're analysing and how the tool interprets the data. Parallelisation makes sense when the data are independent, e.g. running FastQC on multiple fastq files, aligning multiple samples with bwa-mem, or calling SNPs and indels per chromosome.</p> <p>Paralellisation does not make sense when results depend on comparing all data together e.g. joint genotyping, genome assembly, or detecting structural variants across multiple chromosomes.</p>"},{"location":"part1/01_4_smarter/#144-optimising-your-jobs-for-cost","title":"1.4.4 Optimising your jobs for cost","text":"<p>An important issue when designing workflows for HPC is knowing how much it will cost to run. HPCs will typically charge you based on the proportion of available resources you are using, multiplied by some pre-defined rate for the given queue. Both NCI and Pawsey define an intermediate unit called a service unit (SU) that captures the effective resource cost of your job; the final real-world cost of the job will be proportional to the number of SUs you use.</p> <p>The specific calculation for how many SUs a job will use is system-specific, but generally you are charged based on the proportion of available CPUs or available memory that you use, whichever is larger. More specifically:</p> Gadi (PBSpro)Setonix (Slurm) <p>On Gadi, SUs are calculated as:</p> <p>Queue Charge Rate \u00d7 max(NCPUs, Memory Proportion) \u00d7 Walltime Used</p> <p>Where:</p> <ul> <li>Queue Charge Rate = a specific number of SUs charged per hour for the given queue; queue charge rates are defined on NCI's Queue Limits documentation page</li> <li>NCPUs = the number of CPUs requested</li> <li>Memory Proportion = the proportion of memory per core requested</li> <li>Walltime Used = the total number of hours the job ran for (not the requested walltime)</li> </ul> <p>Note that due to the shared, fair-use nature of HPCs, for billing purposes, NCI divides its memory up evenly per CPU and charges you based on the proportion of either the node's CPUs or memory you use, whichever is greater. For example, on the <code>normal</code> queue, each node has 190 GB of memory and 48 CPUs, meaning there is approximately 4 GB of memory per CPU. If you use less than 4 GB of memory per CPU you request, you will be charged based on the number of CPUs you request; while if you use more than 4 GB of memory per CPU you request, you will be charged based on the amount of memory you use.</p> <p>For purely cost-based optimisation, the ideal ratio of memory per CPU for the <code>normal</code> queue is 190 / 48 = ~4 GB per CPU.</p> <p>This SU calculation is described further on NCI's Job Costs documentaiton page</p> <p>On Setonix, SUs are calculated as:</p> <p>Queue Charge Rate \u00d7 max(CPU Proportion, Memory Proportion, GPU Proportion) \u00d7 Number of Nodes \u00d7 Walltime Used</p> <p>Where:</p> <ul> <li>Queue Charge Rate = a specific number of SUs charged per hour per node for the given node type; CPU nodes get charged 128 SUs per node-hour; GPU nodes get charged 512 SU per node-hour.</li> <li>CPU Proportion = the proportion of available CPUs per node requested</li> <li>Memory Proportion = the proportion of available memory per node requested</li> <li>GPU Proportion = the proportion of available GPUs per node requested</li> <li>Number of Nodes = the number of nodes requested</li> <li>Walltime Used = the total number of hours the job ran for (not the requested walltime)</li> </ul> <p>Note that due to the shared, fair-use nature of HPCs, for billing purposes, Pawsey charges you based on the proportion of available CPUs, memory, or GPUs that you request, whichever is greater. For example, on the <code>work</code> queue, each node has 230 GB of memory and 64 CPUs. If you request 1 CPU (1 / 64) and 23 GB memory (1 / 10), you pay for the memory proportion used. If you request 6 CPUs (~ 1 / 10) and 1 GB of memory (1 / 230), you pay for the CPU proportion used.</p> <p>For purely cost-based optimisation, the ideal ratio of memory per CPU for the <code>work</code> queue is 230 / 64 = ~3.5 GB per CPU.</p> <p>This SU calculation is described further on Pawsey's Setonix General Information page</p> <p></p> <p>The above diagram illustrates this method of evenly dividing up the memory per CPU. In this example, we have a compute node with just 10 CPUs and 40 GB of memory. In this case, the memory is divided evenly up per CPU to give 4 GB per CPU. From a cost perspective, if you claim 1 CPU, you effectively claim 4 GB of memory as well; and vice versa, for every 4 GB of memory you claim, you effectively claim 1 CPU.</p> <p>This billing model is easiest to understand in the extreme cases. If you use the total CPU allocation of a node, no one else can use that node, and so you have effectively used the entire node, regardless of how much of the memory you requested. Similarly, if you request the entire memory allocation of a node, you also use up the entire node's resources, regardless of the number of CPUs you request.</p> <p>Note that on both Gadi and Setonix, you are only charged for the walltime you actually use: you are not charged for any unused walltime. The only consequence of over-requesting walltime is potentially longer wait times for your job to start if there are a lot of other jobs queued up. If in doubt of how long a job will take, there is no financial cost to increasing the walltime requested for a job.</p>"},{"location":"part1/01_5_nf_hpc/","title":"1.5 Running Nextflow on HPC","text":"<p>Learning objectives</p> <ul> <li>Understand how Nextflow interacts with HPC components</li> <li>Learn how executors, queues, and work directories control task execution on HPC</li> <li>Run and inspect a simple workflow using a provided HPC configuration</li> <li>Identify which parts of a Nextflow config correspond to scheduler options</li> <li>Connect HPC principles to the Nextflow workflow management systems.</li> </ul> <p>Nextflow Refresher</p> <p>Core Concepts</p> <p>A nextflow pipeline consists of three primary components:</p> <ul> <li>Processes define what to run. Each process can use any Linux-compatible language (e.g., Bash, Python, R, Perl).</li> <li>Channels define how data flows between processes. Channels asynchronously carry data between processes and can fan-out (parallel tasks) or fan-in (merge results).</li> <li>Workflows Define the order in which processes connect. They orchestrate execution, specifying dependencies and the overall structure of the pipeline.</li> </ul> <p>Each process runs independently. When a channel contains multiple inputs, Nextflow automatically creates parallel tasks, each running in isolation, connected only by data passed through channels.</p> <p></p> <p>Nextflow has a built-in concept called an executor which defines where Nextflow runs the workflow tasks. By default, this is the local executor, which executes all of the tasks on your own computer.</p> <p>This is great for development and small test runs, but as datasets grow, your laptop quickly runs out of CPU and memory. This is where HPCs come in.</p>"},{"location":"part1/01_5_nf_hpc/#151-from-your-laptop-to-the-cluster","title":"1.5.1 From your laptop to the cluster","text":"<p>Nextflow supports several HPC executors, including <code>pbspro</code> and <code>slurm</code>, which we are using today. In earlier lessons, we saw that HPCs are shared, scheduled, and resource-limited. The HPC executors are set up to work within these constraints by acting as intermediaries between your workflow and the HPC scheduler. Their job is to:</p> <ul> <li>Prepare a <code>work/</code> directory within shared storage</li> <li>Submit the workflow's tasks to the scheduler and request the necessary resources like CPUs, memory, and walltime</li> <li>Handle the movement of data between filesystems, including between shared storage and the compute nodes' local filesystems</li> <li>Check for job completion and retrieve logs, outputs, and exit codes</li> <li>Publish the output data to the shared filesystem</li> </ul> <p></p>"},{"location":"part1/01_5_nf_hpc/#152-our-first-hpc-workflow","title":"1.5.2 Our first HPC workflow","text":"<p>We'll use a demo workflow, config-demo-nf to see this in action. This workflow contains a single process that splits a sequence into multiple files.</p> <p></p> <p>Ensure you have a copy of the example workflow</p> <p>You should already have a copy of the <code>config-demo-nf</code> directory in your working directory, as it was copied there as part of the setup process.</p> <p>If it is missing, use the following <code>git</code> command to clone the workflow code base to your working directory now:</p> <pre><code>git clone https://github.com/Sydney-Informatics-Hub/config-demo-nf.git\n</code></pre>"},{"location":"part1/01_5_nf_hpc/#153-configuring-for-the-scheduler","title":"1.5.3 Configuring for the scheduler","text":"<p>If we were to run <code>nextflow run config-demo-nf/main.nf</code> right now without any parameters, the workflow would run entirely on the login node. As we've mentioned already, this is not good practice, and we should instead make sure that we are submitting jobs via the HPC scheduler. In Nextflow, we do this by specifying the executor in the Nextflow configuration.</p> <p>A Nextflow configuration file (<code>nextflow.config</code> or files inside a <code>config/</code> directory) defines how and where your workflow runs without changing the workflow code itself.</p> <p></p> <ul> <li>Executor: Which system to use (e.g., <code>local</code>, <code>slurm</code>, <code>pbspro</code>).</li> <li>Queue: Defines where jobs run within the scheduler (e.g., <code>normal</code>, <code>highmem</code>, <code>gpu</code>).</li> <li>Work Directory: Defines where intermediate files are stored so compute nodes can access them.</li> <li>Resources: CPU, memory, and time per process.</li> <li>Environment: Modules, containers, or conda environments to load.</li> <li>Storage: Where to store temporary and output files.</li> </ul> <p>Configs are powerful on HPC systems, because they are what connect your workflow to the scheduler. They translate Nextflow\u2019s processes into properly submitted batch jobs.</p> <p>Because configs are separate from the workflow logic, you can:</p> <ul> <li>Run the same pipeline on laptop, HPC, or cloud by changing only the config.</li> <li>Tune performance by adjusting resources or queues per process</li> <li>Adapt workflows to site-specific environments (modules, scratch paths, queue names)</li> <li>Share portable workflows that others can run on their HPC without code changes</li> </ul> <p>In short, configs are what make Nextflow workflows portable, scalable, and cluster-aware.</p> <p>Exercise: Running the workflow on the compute nodes</p> Gadi (PBSpro)Setonix (Slurm) <p>We have pre-made a very simple configuration file, <code>pbspro.config</code>, that will allow the example Nextflow pipeline to run on Gadi. Go ahead and run the workflow with the <code>pbspro.config</code> configuration file using the <code>-c pbspro.config</code> option. You will also need to define a new parameter: <code>pbspro_account</code> and pass it the project ID (<code>vp91</code>):</p> <pre><code>module load nextflow/24.04.5\nnextflow run config-demo-nf/main.nf -c config-demo-nf/pbspro.config --pbspro_account vp91\n</code></pre> <p>We have pre-made a very simple configuration file, <code>slurm.config</code>, that will allow the example Nextflow pipeline to run on Setonix. Go ahead and run the workflow with the <code>slurm.config</code> configuration file using the <code>-c slurm.config</code> option. You will also need to define a new parameter: <code>slurm_account</code> and pass it the project ID (<code>courses01</code>):</p> <pre><code>module load nextflow/24.10.0\nnextflow run config-demo-nf/main.nf -c config-demo-nf/slurm.config --slurm_account courses01\n</code></pre> <p>The output of your command should look something like this:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [lethal_gilbert] DSL2 - revision: 4e4c91df36\n\nexecutor &gt;  pbspro (1)\n[a8/5345da] splitSequences | 1 of 1 \u2714\n</code></pre> <pre><code>N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [nice_boltzmann] DSL2 - revision: 4e4c91df36\n\nexecutor &gt;  slurm (1)\n[67/d497fa] splitSequences [100%] 1 of 1 \u2714\n</code></pre> <p>Notice that the executor matches your HPC\u2019s system: <code>slurm</code> on Setonix or <code>pbspro</code> on Gadi.</p> <p>What does each line mean?</p> <ol> <li>The version of Nextflow that was executed</li> <li>The script and version names</li> <li>The executor used (<code>pbspro</code> or <code>slurm</code>)</li> <li>The process that was executed once, which means there is one task. The line starts with a unique hexadecimal value, and ends with the task completion information</li> </ol> <p>Note: We typically don't run nextflow on the login node</p> <p>We've made a point to avoid Nextflow running the individual processes on the login node, but we're still running <code>nextflow run</code> on the login node. This is OK for tiny workflows like this, and we're doing it this way for clarity and simplicity for the purposes of the workshop. However, there are a few reasons to avoid doing this:</p> <ol> <li>While not a computationally-intensive process, the main Nextflow script is still a long-running job that uses up some login node resources.</li> <li>Long-running jobs will get killed when you log out of the HPC, unless you use additional tools like <code>nohup</code>, <code>screen</code> or <code>tmux</code> to keep them running in the background. But, large HPC systems often have multiple login nodes that you will be randomly assigned to, and not all let you specify the exact one to connect to, meaning background jobs can get 'lost'.</li> </ol> <p>Both Gadi and Setonix provide a solution to this. Gadi has a special service called persistent sessions that let you create long-running jobs like Nextflow workflows, leave them running in the background (again, with tools like <code>screen</code> and <code>tmux</code>), disconnect, and reconnect at a later time to check up on their progress. Similarly, Setonix has workflow nodes.</p> <p>When running your own real-world workflows, you will need to use these nodes to ensure you pipelines don't get killed and don't eat up valuable login node resources.</p>"},{"location":"part1/01_5_nf_hpc/#task-directories-and-the-work-folder","title":"Task directories and the <code>work/</code> folder","text":"<p>When you run a Nextflow pipeline, it automatically creates a <code>work/</code> directory. This is where all computation happens behind the scenes. Inside this directory, each process execution (or task) runs in its own isolated subdirectory, identified by a unique hash, in the above example, <code>work/6b/e8feb6</code> (NOTE: your unique hash will be different).</p> <p>Note</p> <p>You can execute <code>tree work/</code> to view the work directory structure.</p> <pre><code>tree work/\n</code></pre> Output<pre><code>work\n\u2514\u2500\u2500 6b\n    \u2514\u2500\u2500 e8feb6a83bb78a7a6661ccc1211857\n        \u251c\u2500\u2500 seq_1\n        \u251c\u2500\u2500 seq_2\n        \u251c\u2500\u2500 seq_3\n        \u2514\u2500\u2500 sequence.fa -&gt; /scratch/PROJECT/USER/nextflow-on-hpc-materials/part1/config-demo-nf/sequence.fa\n</code></pre> <p>Here\u2019s what happens inside each task directory:</p> <ol> <li>Setup: Nextflow stages (copies or links) the input files, plus a small script (.command.sh) that defines what to run.</li> <li>Execution: The process runs inside that folder, writing its results there.</li> <li>Cleanup: Nextflow collects the output files and makes them available for downstream processes or publishing.</li> </ol> <p>Each directory is independent so tasks don\u2019t share writable space. If one process needs data from another, it\u2019s passed through Nextflow channels, not shared files. This isolation is especially important on HPC systems, where tasks may run on different compute nodes.</p>"},{"location":"part1/01_5_nf_hpc/#154-profiles","title":"1.5.4 Profiles","text":"<p>Another very useful feature of Nextflow is the ability to bundle up configuration options into profiles. This can help to simplify the command line arguments to Nextflow by using the <code>-profile &lt;profile name&gt;</code> syntax, rather than having to provide the path to the relevant configuration file. We have already set up the <code>nextflow.config</code> file to define two profiles, <code>pbspro</code> and <code>slurm</code>, which import the relevant configuraiton files when they are used:</p> nextflow.config<pre><code>// Define HPC profiles to run with job scheduler\nprofiles {\n  // Use this profile to interact with the scheduler on setonix\n  slurm { includeConfig \"slurm.config\" }\n\n  // Use this profile to interact with the scheduler on gadi\n  pbspro { includeConfig \"pbspro.config\" }\n}\n</code></pre> <p>Exercise: Running the workflow on the compute nodes with profiles</p> <p>Run the workflow once more, this time using the executor profiles:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code> nextflow run config-demo-nf/main.nf -profile pbspro --pbspro_account vp91\n</code></pre> <pre><code>nextflow run config-demo-nf/main.nf -profile slurm --slurm_account courses01\n</code></pre> <p>The output of your command should be the same as before:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [lethal_gilbert] DSL2 - revision: 4e4c91df36\n\nexecutor &gt;  pbspro (1)\n[a8/5345da] splitSequences | 1 of 1 \u2714\n</code></pre> <pre><code>N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [nice_boltzmann] DSL2 - revision: 4e4c91df36\n\nexecutor &gt;  slurm (1)\n[67/d497fa] splitSequences [100%] 1 of 1 \u2714\n</code></pre> <p>Now that we've recapped the basics of Nextflow and seen how a simple Nextflow pipeline can be run on an HPC, in the next section we will look at how we can start running some more complex workflows on HPCs, and how we can configure them to more efficiently utilise the resources available to them.</p> <p>Need a break?</p> <p>We've gone through a lot so far, so before we continue on, feel free to take a break!</p> <p>Also, if you have run into any issues so far please react with a \" No\" and we can help out.</p>"},{"location":"part1/01_6_nfcore_intro/","title":"1.6 Intro to nf-core","text":"<p>Learning objectives</p> <ul> <li>Understand what nf-core is</li> <li>Identify where to find and access nf-core pipelines</li> <li>Recognise the basic structure and key components of an nf-core workflow</li> </ul> <p>We have now seen how a Nextflow pipeline can be configured to run on an HPC. In tomorrow's section of the workshop, we will further explore optimising a custom Nextflow pipeline to efficiently utilise HPC resources. However, whenever you are considering building a workflow, it is always important to check whether a suitable tool already exists - after all, the goal of Nextflow is to build reproducible workflows, and we shouldn't re-invent the wheel if we don't have to! For the rest of this session, we will be looking at the nf-core project, which aims to address this very issue and provide a collection of bioinformatics Nextflow pipelines.</p>"},{"location":"part1/01_6_nfcore_intro/#161-what-is-nf-core","title":"1.6.1 What is nf-core?","text":"<p>nf-core is a community-driven effort to develop and curate open-source bioinformatics workflows built with Nextflow.</p> <p>The project has a standardised set of best practices, guidelines, and templates for building modular, scalable, and portable bioinformatics workflows. Every workflow is well-documented, tested, and designed to work across multiple platforms, including cloud and HPC.</p> <p>The key Features of nf-core workflows are:</p> <ul> <li>Documentation: nf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won\u2019t be left in the dark.</li> <li>CI Testing: Every time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.</li> <li>Stable Releases: nf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.</li> <li>Packaged software: Pipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.</li> <li>Portablility and reproducibility: nf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.</li> <li>Cloud-ready: nf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.</li> </ul> <p>nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology</p>"},{"location":"part1/01_6_nfcore_intro/#162-where-to-find-nf-core-pipelines","title":"1.6.2 Where to find nf-core pipelines","text":"<p>The nf-core website - https://nf-co.re - hosts a list of all of the current nf-core pipelines, as well as their documentation, information for developers and users, and links to community forums, training sessions, and more.</p> <p>The full list of available piplines are available at https://nf-co.re/pipelines/, and at the time of writing, this includes 84 stable workflows, 44 more that are under development, and 12 that have been archived.</p> <p>Each workflow has a dedicated page that includes expansive documentation that is split into 6 sections:</p> <ul> <li>Introduction: An introduction and overview of the workflow</li> <li>Usage: Documentation and descriptions of how to execute the workflow</li> <li>Parameters: Documentation for all workflow parameters</li> <li>Output: Descriptions and examples of the expected output files</li> <li>Results: Example output files generated from the full test dataset on AWS</li> <li>Releases: Workflow version history</li> </ul> <p>All of the workflow code is hosted on GitHub under the <code>nf-core</code> project. For example, today we will be working with the <code>sarek</code> pipeline, which is hosted on GitHub at https://github.com/nf-core/sarek. Helpfully, unless you are actively developing workflow code yourself, you usually won\u2019t need to clone the workflow code from GitHub; instead, you can use Nextflow\u2019s built-in functionality to pull a workflow:</p> <pre><code>nextflow pull nf-core/&lt;pipeline&gt;\n</code></pre> <p>Nextflow's <code>run</code> command will also automatically pull the workflow if it was not already available locally:</p> <pre><code>nextflow run nf-core/&lt;pipeline&gt;\n</code></pre> <p>By default, Nextflow will pull the default git branch of the pipeline unless a specific version is specified with the <code>-revision</code> or <code>-r</code> flag.</p> <p>Note that in today's workshop, for full transparency and consistency, we will be pulling the pipeline directly from GitHub rather than using the shortcut method above.</p>"},{"location":"part1/01_6_nfcore_intro/#163-introducing-todays-pipeline-nf-coresarek","title":"1.6.3 Introducing today's pipeline: <code>nf-core/sarek</code>","text":"<p>As mentioned above, the rest of today's workshop will be focussing on the <code>nf-core/sarek</code> pipeline. As part of the workshop setup, the pipeline source code was downloaded into your workspace. You should see the <code>sarek/</code> folder in your current directory:</p> <pre><code>ls -1\n</code></pre> Output<pre><code>config/\nREADME.md\nsarek/\nscripts/\nsingularity/\n</code></pre> <p>The <code>nf-core/sarek</code> pipeline is a large workflow dedicated to performing variant calling on genome sequencing data. It is highly configurable and incoroprates a wide variety of tools and methods for detecting both germline and somatic variants.</p> <p></p> <p>The pipeline consists of three main stages:</p> <ul> <li>Pre-processing: This stage takes in sequencing FASTQ files and performs quality control, read trimming, genome alignment (also called mapping), marking of duplicate reads, and recalibrating base quality scores.</li> <li>Variant calling: This is the main workhorse of the pipeline and is the part that actually detects germline and/or somatic variants in the sequencing data. It uses several tools to achieve this, including <code>deepvariant</code>, <code>bcftools</code>, and the <code>GATK</code> suite.</li> <li>Annotation: This stage takes the variants that were called and annotates them using public databases. Annotations include the predicted effects of the variants, such as potential frameshift or stop-gain mutations, and their clinical relevance.</li> </ul> <p>As you can imagine, this is a very complex pipeline with lots of options and parameters, and could take a long time to run in its default mode. Luckily, it also includes a wide range of options for selecting which parts of the pipeline to run or skip. We will be making heavy use of these options today so that we can just run a small section of the workflow.</p> <p>We will be providing the pipeline with FASTQ files and just running the <code>mapping</code> part of the pre-processing stage. This takes in raw sequencing reads in the FASTQ format and aligns those reads to positions in the genome. The actualy implementation of this in <code>sarek</code> involves a few key stages:</p> <ol> <li><code>FASTQC</code>: This runs <code>fastqc</code> for generating quality control reports for the input sequencing data. This is the exact same tool we used in the earlier lessons.</li> <li><code>FASTP</code>: The <code>fastp</code> tool is a bit of a Swiss Army Knife, and does a number of different tasks, including quality-based filtering and trimming of sequencing reads, splitting reads into multiple FASTQ files, and quality control reporting (similar to <code>fastqc</code>). The splitting of FASTQ reads is particularly useful as it can be used for parallel processing in a scatter-gather pattern.</li> <li><code>TABIX_BGZIPTABIX_INTERVAL_COMBINED</code>: This creates a compressed <code>.gz</code> file of an intervals file and additionally indexes it. Intervals files are another common file in genomics used to easily specify genomic regions of interest.</li> <li><code>BWAMEM1_MEM</code>: This runs the tool <code>bwa mem</code> to perform the actual alignment of reads to the genome. This will run once for every split FASTQ file generated by <code>FASTP</code>.</li> <li><code>MERGE_BAM</code>: This performs the \"gather\" part of the scatter-gather pattern. Once the reads from each FASTQ have been aligned, the resulting BAM files are merged back into a single file per sample.</li> <li><code>INDEX_MERGE_BAM</code>: This creates an index of the final BAM file for each sample. Indexes are important for quickly searching through large sequencing datasets.</li> <li><code>BAM_TO_CRAM_MAPPING</code>: This creates a CRAM file from the final BAM file per sample. CRAM files are compressed BAM files, and are common in genomics, where data sets are typically very large.</li> <li><code>MULTIQC</code>: This run the <code>multiqc</code> tool to generate a final summary report of the whole pipeline run.</li> </ol> <p></p> Additional content: the structure of a typical nf-core pipeline <p>For those who are interested in diving a bit deeper into the structure of an nf-core pipeline, let's list the directory structure of the <code>sarek</code> pipeline:</p> <pre><code>ls sarek/\n</code></pre> Output<pre><code>assets/\nbin/\nCHANGELOG.md\nCITATIONS.md\nCODE_OF_CONDUCT.md\nconf/\ndocs/\nLICENSE\nmain.nf\nmodules/\nmodules.json\nnextflow_schema.json\nnextflow.config\nnf-test.config\nREADME.md\nsubworkflows/\ntests/\ntower.yml\nworkflows/\n</code></pre> <p>You will see that there are quite a lot of files and folders. Some of these are simply housekeeping like the <code>LICENCE</code>, <code>CHANGELOG.md</code>, <code>CITATIONS.md</code>, and <code>CODE_OF_CONDUCT.md</code> files. The most important files and folders that we are interested in are described below:</p> File/folder Description main.nf This is the entry point to workflow and the actual script that needs to be run to execute the pipeline. It is used primarily to define and initialise various parameters relevant to the pipeline. workflows/ This folder houses the actual <code>sarek</code> workflow, which is called from <code>main.nf</code> subworkflows/ This folder houses smaller, self-contained, modular workflows that are called within the greater <code>sarek</code> pipeline modules/ This houses Nextflow files that define individual processes. Breaking up your processes into modules like this is considered best practice. conf/ This houses configuration files specific to various modules in the pipeline nextflow.config This is the default configuration file and defines default parameters and profiles, and imports the module-specific configuration files defined in <code>conf/</code> bin/ The <code>bin/</code> folder can house executable scripts that can be called directly from your Nextflow processes. In <code>sarek</code>, the only executable script in here is a python script to generate licence messages. <p>Diving a little deeper, you will see that the <code>modules/</code> and <code>subworkflows/</code> directories both contain two sub-folders: <code>local/</code> and <code>nf-core/</code>:</p> <pre><code>ls sarek/modules sarek/subworkflows\n</code></pre> Output<pre><code>sarek/modules:\nlocal   nf-core\n\nsarek/subworkflows:\nlocal   nf-core\n</code></pre> <p>Any modules and sub-workflows that originated from the nf-core GitHub repository will be placed in the <code>nf-core</code> sub-folder, and typically represent common tools and workflows that are useful in many pipelines. The <code>local</code> sub-folders, on the other hand, are for processes that were developed specificially for the current pipeline.</p> <p></p> <p>As you can see, nf-core pipelines can be quite complex. This is due to the attempt to heavily standardise and modularise these workflows. There are significant benefits to this, primarily in that it gives everyone a common template to work from and helps to break down the workflows into smaller, more manageable and maintainable chunks. However, it can also make it difficult to analyse and troubleshoot the code when developing them.</p> <p>The configuration for <code>nf-core/sarek</code> is split up into many different files. The main configuration file, <code>nextflow.config</code>, is set up to:</p> <ul> <li>Define and set the defaults for various parameters, including input files and the reference genome.</li> <li>Define several profiles that specify groups of settings important for various backends like docker, singularity, and conda. There are also several test profiles defined.</li> <li>Import module-specific configuration files.</li> </ul> <p>One of the major configuration files that is importedby <code>nextflow.config</code> is <code>conf/base.config</code>. This file defines the default resources for processes, as well as resources that are applied to many processes at once with the <code>withLabel</code> and <code>withName</code> selectors:</p> Default resource configuration in conf/base.config<pre><code>process {\n\n    // TODO nf-core: Check the defaults for all processes\n    cpus   = { 1      * task.attempt }\n    memory = { 6.GB   * task.attempt }\n    time   = { 4.h    * task.attempt }\n</code></pre> Specific resource configuration for FASTP in conf/base.config<pre><code>withName: 'FASTP'{\n    cpus   = { 12   * task.attempt }\n    memory = { 4.GB * task.attempt }\n}\n</code></pre> Resource configuration fo the 'process_medium' label in conf/base.config<pre><code>withLabel:process_medium {\n    cpus   = { 6     * task.attempt }\n    memory = { 36.GB * task.attempt }\n    time   = { 8.h   * task.attempt }\n}\n</code></pre> <p>This highlights the layered nature of Nextflow's configuration; initially, sensible default values for CPUs, memory, and walltime are defined, followed by more specific values for jobs that have different computational requirements, e.g. more CPUs but less memory for the <code>FASTP</code> process.</p> <p>Note also the use of the curly braces and the <code>* task.attempt</code> in each line. This is an example of a dynamically calculated resource. Nextflow keeps track of how many times a given task has been attempted and stores it in the variable <code>task.attempt</code>; this can be used in case of a failure to increase the required resources on the next try. In this case, by default, jobs will be given 1 CPU, 6GB of memory, and 4h of walltime on the first try; on the second try, they will get 2 CPUs, 12GB of memory, and 8h of time. <code>conf/base.config</code> also sets <code>maxRetries = 1</code>, so this will only happen the one time, and failures won't cause an infinite loop of retries.</p> <p>The pros and cons of task.attempt</p> <p>While <code>task.attempt</code> can be very useful for re-trying a process with more resources when it fails, it is a somewhat crude method of doing so. Doubling your resources upon failure will also double the cost of running the job. Where possible, you should try to be more nuanced with such dynamic requests and identify during development which resources are the limiting factors for a process. You can also try different approaches to increasing a resource with each attempt, such as adding a small amount each time, rather than doubling:</p> <p><code>memory = { 32.GB + 4.GB * task.attempt }</code></p> <p>A more advanced method is to calculate the size of input files within the process definition and adjust the resources required accordingly:</p> <pre><code>process TEST {\n\n    // Get the input file size (in bytes) and add a 1 GB buffer\n    memory { x.size().B + 1.GB }\n\n    input:\n    path x\n\n    ...\n\n}\n</code></pre> <p>Let's now move to the next section, where we will write a script to run the pipeline!</p>"},{"location":"part1/01_7_nfcore_run/","title":"1.7 Running nf-core on HPC","text":"<p>Learning objectives</p> <ul> <li>Build a basic run script for a nf-core pipeline</li> <li>Run an nf-core pipeline</li> <li>Understand the need for HPC-specific configurations</li> </ul>"},{"location":"part1/01_7_nfcore_run/#171-an-overview-of-the-inputs","title":"1.7.1 An overview of the inputs","text":"<p>As mentioned in the previous section, we will be running a small section of the overall <code>sarek</code> workflow: the <code>mapping</code> stage. This stage requires a few distinct inputs:</p> <ul> <li>FASTQ files: These are plain text files that contain the sequences of the reads that came from the sequencing machine.</li> <li>A samplesheet: A <code>.csv</code> file that describes each sample, its name, and where to find its associated FASTQ files.</li> <li>A FASTA file: This holds the actual sequence of the chromosomes in the reference genome.</li> <li>A BWA index: This is a group of files that is used by the alignment tool <code>bwa mem</code> to efficiently find where in the genome a particular sequencing read best maps to.</li> </ul> <p>We have two samplesheets prepared for today's workshop, both within the <code>../data/fqs</code> directory. The first is <code>samplesheet.fq.csv</code>. You can display it on the command line with:</p> <pre><code>cat ../data/fqs/samplesheet.fq.csv\n</code></pre> <p>Viewing the samplesheet in VSCode</p> <p>Unfortunately, because the <code>data/</code> folder is in the parent directory to where we are working, we can't see it in the VSCode explorer. However, if you wish to open the samplesheet in the VSCode editor, you can still do so via the terminal using the <code>code</code> command:</p> <pre><code>code ../data/fqs/samplesheet.fq.csv\n</code></pre> <p>The samplesheet looks like this:</p> samplesheet.fq.csv<pre><code>patient,sample,lane,fastq_1,fastq_2\nNA12877,test_sample1,all,../data/fqs/NA12877_chr20-22.R1.fq.gz,../data/fqs/NA12877_chr20-22.R2.fq.gz\nNA12878,test_sample2,all,../data/fqs/NA12878_chr20-22.R1.fq.gz,../data/fqs/NA12878_chr20-22.R2.fq.gz\nNA12889,test_sample3,all,../data/fqs/NA12889_chr20-22.R1.fq.gz,../data/fqs/NA12889_chr20-22.R2.fq.gz\n</code></pre> <p>The first line is a header line: it describes the \"column names\" of the table that the CSV represents. We have four columns in this file:</p> <ul> <li><code>patient</code>: An identifier for the individual the sample came from. Note that more than one sample may originate from an individual, such as in the case of paired tumor and normal samples.</li> <li><code>sample</code>: A unique identifier for the sample. Each line must have a unique value for this column.</li> <li><code>lane</code>: This identifies the lane of the flow cell that the sequencing read originated from.</li> <li><code>fastq_1</code>: The path to the read 1 FASTQ file for that sample.</li> <li><code>fastq_2</code>: The path to the read 2 FASTQ file for that sample.</li> </ul> <p>We can also see that we have three samples defined in our samplesheet. Samplesheets make it very easy to add new samples to our workflow run, simply by adding a new line and filling out the relevant columns.</p> <p>The second samplesheet we have prepared is <code>samplesheet.single.csv</code>:</p> samplesheet.single.csv<pre><code>patient,sample,lane,fastq_1,fastq_2\nNA12877,test_sample1,all,../data/fqs/NA12877_chr20-22.R1.fq.gz,../data/fqs/NA12877_chr20-22.R2.fq.gz\n</code></pre> <p>As you can see, this contains just a single sample. For the inital part of this lesson, we will work with this samplesheet in order to keep things simple and quick.</p> <p>Samplesheet columns for sarek</p> <p>The <code>sarek</code> pipeline can use a lot of different tools and start and stop at various stages. This means there are different columns required in the samplesheet depending on what you are running. Check with the usage documentation to determine the correct columns for your run. In our case, we are using the minimal configuration when starting with the <code>mapping</code> stage.</p> Additional content: A deeper look at the input files <p>For those who are interested, here we look a bit closer at the structure of these input files.</p>"},{"location":"part1/01_7_nfcore_run/#fastq-files","title":"FASTQ files","text":"<p>FASTQ files are plain text files that are structured in a particular way to capture the sequences of the reads in our sample as well as the quality of those sequences. Each read is represented by four lines in a FASTQ file:</p> Two reads from a FASTQ file<pre><code>@HSQ1004:134:C0D8DACXX:1:1105:9389:51835\nTGTGTTCCTGATTTGGCTCCTGGCTTGACTGTTGGTGTATAAGAATGTTAATGATTTTTGCACATTGATTTTGTATCCTGAGACTTTGCTGAAGTTATCAG\n+\nCCCFFFFFHHGHHJJJJJJJJJJJJJJIJJJGHIICG?FHGIJGIGIDGIIIIIIJJGIGGGIJJIJJIJJJJIIEHEFGGHFFDFFFEEDCEECDEECDD\n@HSQ1004:134:C0D8DACXX:1:2101:9406:116923\nAGGTTTTATTTTTAAATTTTTTTCTTTAGTTTTGTCTGACTGTGTTGATTCAAAGGACTGATCTTTGAGCTCTGAGATTCTTTCCTCAACTTGATCTATTC\n+\n@@?DBDDEBFFFFHIIIIDGHIIGEIIIG9DGGGIIGIIHGHFCBFC&gt;GHGHGHCGEGGGIIEGIG;@=EHCEE@?:BEDADE@@@@;@ACACDCC&gt;CD;@\n</code></pre> <p>Each set of four lines is defined as follows:</p> <ol> <li>The read name. This often includes information about the flow cell it came from.</li> <li>The sequence of the read. This is the actual As, Cs, Gs and Ts of the read.</li> <li>A <code>+</code> symbol. This line isn't normally used for anything and serves mostly as a visual separator between the sequence and its quality scores.</li> <li>The quality scores for the read. The characters on this line represent quality scores for the corresponding position in the sequence. The quality scores are encoded as ASCII characters, starting at ASCII code 33 (<code>!</code>) for a value of <code>0</code> and increasing from there. The quality scores themselves are called Phred quality scores, which are logarithmically related to the probability of an error at that base (higher scores being of better quality). This encoding scheme is referred to as Phred-33.</li> </ol> <p>We are working with paired-end sequencing data, where every sequencing read has two mates originating from either end of the original DNA fragment. These are typically separated into two ordered FASTQ files, one containing the first mates (or \"read 1\") and the other containing the second mates (or \"read 2\").</p> <p>FASTQ files are also often compressed as gzip files. This helps to reduce storage space for large sequencing samples.</p>"},{"location":"part1/01_7_nfcore_run/#fasta-files","title":"FASTA files","text":"<p>A FASTA file is another a plain text file type that holds the full sequence of the reference genome that the sequencing data will be aligned to. The FASTA format is much simpler than FASTQ. Each sequence starts with a line beginning with a <code>&gt;</code> character. This line provides the name for the sequence, e.g. <code>chr20</code>. The actual sequence starts on the following line and can be split into any number of lines. The sequence ends when either a new one begins (with a new line starting with <code>&gt;</code>) or the file ends.</p> An example FASTA file<pre><code>&gt;seq1\nATCGGTCA\n&gt;seq2\nAATTGGCC\nGGCCTTAA\n&gt;seq3\nAATGGATA\nCTTGATAC\nCCTG\n</code></pre> <p>The FASTA file we are working with today (<code>Hg38.subsetchr20-22.fasta</code>) contains three sequences: human chromosomes 20-22. These are the smallest autosomal chromosomes, and were picked for this workshop to ensure speedy alignment. You'll notice that the first 600 lines of the <code>chr20</code> sequence are just full of <code>N</code> characters; this is a \"mask\" character, and is used to essentially block out regions of the genome that are unreliably sequenced, as is the case with the chromosome ends. It isn't until line 602 that we see some actual A's, C's, G's and T's. In total, there are more than half a million lines in the file.</p> Hg38.subsetchr20-22.fasta<pre><code>&gt;chr20\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n...\nATTATCCATGAGGAGGGAGTGCAGACAAAGCAAAGAAGGAGGATGTTTGGAGAGGGGTAGTCTTTGAGTGGAGCCTTTAGGGATGAGAAGGGTGAATTGA\nGATATACCGGGAAAGTAGAAAAGATAAAACACGTATTAAAT\n&gt;chr21\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n</code></pre> <p>The FASTA file is typically associated with two additional helper files: the FASTA index (<code>.fai</code>) and dictionary (<code>.dict</code>) files. Both contain information about the sequences in the main FASTA file and are used by tools for quickly finding sequences within it, especially when the genome is very large.</p>"},{"location":"part1/01_7_nfcore_run/#bwa-index","title":"BWA Index","text":"<p>The job of <code>bwa mem</code> is to take sequencing reads in the FASTQ format and try to align them to a reference genome. For all but the most trivially-sized genomes, this is a very computationally-intensive task. Therefore, <code>bwa mem</code> makes use of an index to more efficiently look up where in the genome specific sequences map to. BWA's index format consists of a directory containing multiple files:</p> An example BWA index<pre><code>Homo_sapiens_assembly38.20-22.fasta.amb  Homo_sapiens_assembly38.20-22.fasta.pac\nHomo_sapiens_assembly38.20-22.fasta.ann  Homo_sapiens_assembly38.20-22.fasta.sa\nHomo_sapiens_assembly38.20-22.fasta.bwt\n</code></pre> <p>Note that different alignment tools will use their own specific index format; this format is specific to <code>bwa mem</code> and can't be used with other tools.</p>"},{"location":"part1/01_7_nfcore_run/#172-write-a-simple-run-script","title":"1.7.2 Write a simple run script","text":"<p>To run the mapping stage of <code>sarek</code>, we need to execute the <code>nextflow run</code> command and include several parameters. We will use a run script to compose this command bit-by-bit to keep things organised.</p> <p>Exercise: Create a run script for nf-core/sarek</p> <p>In the current working directory, create a new blank file and name it <code>run.sh</code>. You can do this via VSCode's interface by right-clicking on the current directory (<code>part1</code>) in the explorer side bar, clicking on \"New File...\", writing \"run.sh\" and hitting the Enter key. You can also do this via the terminal:</p> <pre><code>touch run.sh\n</code></pre> <p>You should also make the file executable by running the following command:</p> <pre><code>chmod +x run.sh\n</code></pre> <p>Next, we'll start the run command by adding the following lines:</p> Gadi (PBSpro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run sarek/main.nf \\\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run sarek/main.nf \\\n</code></pre> <p>So far, this is pretty straight-forward: we first tell the OS that we're running a bash script with the <code>#!/bin/bash</code> comment. Then, we load the nextflow and singularity modules; these are necessary for ensuring that the <code>nextflow</code> command is available to us, and that Nextflow is able to use Singularity for running containers. Finally, we run the <code>main.nf</code> Nextflow script within the <code>sarek</code> repository.</p> <p>As it exists, this won't run: the <code>sarek</code> pipeline requires you to give it a samplesheet as input, and we also need to configure a few other parameters to ensure that we only run our small <code>mapping</code> stage of the pipeline. That's why we've ended the <code>nextflow run</code> line with a backslash (<code>\\</code>), indicating that the command continues on the following line.</p> <p>Add the following lines to define the inputs to the <code>mapping</code> stage:</p> run.sh<pre><code>    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n</code></pre> <p>We've provided here the samplesheet CSV file to the <code>--input</code> parameter, as well as the FASTA file and its paired index (FAI) and dictionary files to the <code>--fasta</code>, <code>--fasta_fai</code>, and <code>--dict</code> parameters. We also specified the path to the BWA index directory with <code>--bwa</code>.</p> <p>Finding more information about the sarek parameters</p> <p>The sarek parameters documentation contains information about all of the parameters supported by <code>sarek</code> and the valid arguments to them. The usage page in the docs also contains useful information about which parameters are required by the various stages of the pipeline.</p> <p>Next, we'll specify that we want to run the pipeline from the <code>mapping</code> step, and skip several downstream tools, including <code>markduplicates</code>, <code>baserecalibrator</code>, <code>mosdepth</code>, and <code>samtools</code>. This will have the effect of just running the <code>mapping</code> step, followed by a final run of MultiQC; in total we will run only eight distinct processes, which will take only a few minutes for our dataset:</p> run.sh<pre><code>    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n</code></pre> <p>Wrapping up, we will tell the pipeline where to place our outputs, as well as configure a few additional parameters to make things run a bit smoother in our small example:</p> run.sh<pre><code>    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true\n</code></pre> <p>The <code>--no_intervals true</code> parameter tells <code>sarek</code> that we don't want to worry about splitting our data up into distinct genomic intervals. Intervals are a very useful feature for large datasets, as it lets us parallelise large genomic sequencing data into chunks that can be processed simultaneously - and takes advantage of the parallel nature of HPCs! However, in our very small example here, it would actually cause things to run slower by adding more jobs and waiting for them to start and finish.</p> <p>The next line, <code>--igenomes_ignore true</code> stops the pipeline from downloading some additional files from public databases; again, this can be useful in a proper run, but for our purposes it is more of a nuisance.</p> <p>At the end, the <code>run.sh</code> script should look like the following:</p> Gadi (PBSpro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true\n</code></pre> <p>How are you going?</p> <p>If you're following along so far, let us know by reacting on zoom with a \" Yes\".</p> <p>If you're running into any issues, please react with a \" No\" and we can help out before we move on to the next section.</p>"},{"location":"part1/01_7_nfcore_run/#173-not-quite-ready","title":"1.7.3 Not quite ready!","text":"<p>We've created a full run script for our pipeline, but we won't run it just yet, because we haven't actually told Nextflow to use the HPC scheduler. If we were to run the workflow, it would attempt to run on the login node where we are currently working!</p> <p>In addition, we haven't configured Nextflow to use Singularity yet, so Nextflow will assume all of the necessary software is installed and ready to use - which it isn't.</p> <p>In the next section, we will build up a configuration file that tells Nextflow to both use Singularity and to talk to the HPC's scheduler to submit jobs to the compute nodes.</p>"},{"location":"part1/01_8_nfcore_config/","title":"1.8 Configuring nf-core","text":"<p>Learning objectives</p> <ul> <li>Understand Nextflow executors</li> <li>Understand how Nextflow uses executor definitions to talk to the HPC schedulere</li> <li>Understand the singularity configuration scope</li> <li>Know how to dynamically define the queue based on required resources</li> </ul>"},{"location":"part1/01_8_nfcore_config/#181-executors","title":"1.8.1 Executors","text":"<p>Executors are the back-end that Nextflow talks to to run the tasks in your workflow. By default, Nextflow will assume that you want to run everything on the same computer that you ran the <code>nextflow run</code> command on. But, as we have learned, that is definitely not how we want to run things on a shared HPC system: we want each task to be submitted to the scheduler to run on a compute node with all the appropriate resources.</p> <p>We saw earlier today that there are a fair few parameters that need to be configured when submitting jobs to an HPC, and these differ between systems. Luckily, Nextflow includes native support for the two HPC schedulers that we are working with in this workshop: PBSPro and Slurm.</p> <p>To set up Nextflow to use an HPC executor, we simply define the <code>process.executor</code> configuration option in the Nextflow configuration. We can also configure a few other parameters to control how many jobs get submitted to the HPC and how frequently; this is useful with large pipelines to avoid overwhelming the system (and angering the admins!). To keep things clean, we will create a new, blank configuration file called either <code>gadi.config</code> or <code>setonix.config</code>, depending on your system, and specify it in our run command. As we will see, Nextflow lets you layer configurations on top of one another and combines them in a predictable way to allow fine control of how each process runs.</p> <p>Exercise: Define the HPC executor</p> <p>Start by creating a new blank file in the <code>config</code> directory called <code>hpc.config</code>. You can do this via the VSCode explorer window (right-click the <code>config</code> folder and select \"New File...\") or via the terminal:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>touch config/gadi.config\n</code></pre> <pre><code>touch config/setonix.config\n</code></pre> <p>Open the new file in the VSCode editor and add a <code>process {}</code> scope:</p> <pre><code>process {\n\n}\n</code></pre> <p>Within the process scope, define the <code>executor</code> option and set it to the relevant executor for your system:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n}\n</code></pre> <p>We also want to set some limits to how many jobs can be submitted at once and how frequently they get submitted. These settings are important, because many large pipelines can create potentially hundreds of jobs that may overwhelm the system. Most HPCs will set a limit for how many jobs a user can submit at once, and your pipeline may fail if it tries to submit more than this limit.</p> <p>For our purposes, we will keep our queued job limit to 30, and limit the number of jobs we can submit at once to 20 per minute. We will also tell Nextflow to request for status updates on our jobs once every 15 seconds.</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n</code></pre> <p>Now we have defined our executor and some relevant settings for it, we will need to tell Nextflow to actually use this new configuration file; by default, Nextflow will only use the <code>nextflow.config</code> file in the project directory, and will only load other configuration files when explicitly told to do so.</p> <p>In the <code>run.sh</code> script, update the following highlighted lines by adding a <code>\\</code> to the end of the old command and adding the new configuration file with the <code>-c</code> option:</p> Gadi (PBSpro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/gadi.config\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/setonix.config\n</code></pre> <p>The new line adds the <code>-c &lt;config&gt;</code> option to the <code>nextflow run</code> command. We provide the path to our new configuration file which tells Nextflow to load it and merge it with the existing configuration set up by the <code>nextflow.config</code> file. Note that the settings in configuration files provided by the <code>-c</code> command will take precedence over those set in the <code>nextflow.config</code> file, so if any options are specified in both files, the setting in <code>config/gadi.config</code> or <code>config/setonix.config</code> will be used. We will explore layering configurations further in the next section of the workshop.</p> <p>Go ahead and try running the script:</p> <pre><code>./run.sh\n</code></pre> <p>What happens?</p> Result... <p>The workflow will begin to run and submit several jobs to the executor:</p> Initial output<pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `sarek/main.nf` [exotic_cray] DSL2 - revision: 3954909713\n\n...\n\nexecutor &gt;  pbspro (1)\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                   -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                   -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                               -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                  -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                               -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                  -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                      -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                     -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                         -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                              -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                                       -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED           [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR                             -\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ                               -\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ                               -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP            -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                 -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                  -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                    -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                            -\n[5f/63da05] process &gt; NFCORE_SAREK:SAREK:FASTQC (test_sample1-all)                                [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTP                                                    [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM     -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM     -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN   -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:MERGE_BAM                       -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM                 -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_TO_CRAM_MAPPING                                      -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS                             -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:MOSDEPTH                                   -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_TO_BAM                                              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP          -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM          -\n[-        ] process &gt; NFCORE_SAREK:SAREK:MULTIQC\n</code></pre> <p>However, soon after the jobs start running on the compute nodes, they fail:</p> Final output<pre><code>-[nf-core/sarek] Pipeline completed with errors-\nERROR ~ Error executing process &gt; 'NFCORE_SAREK:SAREK:FASTP (test_sample1-all)'\n\nCaused by:\nProcess `NFCORE_SAREK:SAREK:FASTP (test_sample1-all)` terminated with an error exit status (127)\n\n...\n\nCommand exit status:\n127\n\nCommand output:\n(empty)\n\nCommand error:\n.command.sh: line 11: fastp: command not found\n\nWork dir:\n/scratch/er01/PIPE-5866-NF4HPC/nextflow-on-hpc-materials/part1/work/92/74509fb605260a345f7c3f29419f1c\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n-- Check '.nextflow.log' file for details\nERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting\n\n-- Check '.nextflow.log' file for details\n</code></pre> <p>The above example shows exactly what went wrong: <code>fastp: command not found</code>. We haven't yet configured Nextflow to use Singularity, so it is assuming the software is installed on the compute node. In the next section, we will set up Nextflow to use Singularity to run each tool.</p>"},{"location":"part1/01_8_nfcore_config/#182-containers-in-nf-core","title":"1.8.2 Containers in nf-core","text":"<p>To start using containers in Nextflow, we need to enable the <code>singularity</code> backend and define the cache directory. This is the directory where Singularity should store all downloaded containers so that it doesn't need to download them over and over again whenever the same tool is required. As part of the setup work we did earlier today, we have already created this cache directory within the parent folder, at <code>../singularity/</code>.</p> <p>Exercise: Define the singularity configuration</p> <p>At the bottom of our configuration file, we will now add a <code>singularity</code> scope and enable the containerisation software. At the same time, we will also define the Singularity cache directory.  We can define this in the <code>singularity</code> configuration scope by setting the <code>cacheDir</code> option. We will provide the full path to this directory, which is at <code>/scratch/&lt;PROJECT&gt;/&lt;USER&gt;/nextflow-on-hpc-materials/singularity</code>. The current project ID and username are both accessible as environment variables on both Gadi and Setonix; we access these with the groovy function <code>System.getenv()</code>:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>Enabling the use of Singularity will tell Nextflow to run our tasks using a <code>singularity exec</code> command, similar to what we used earlier today. However, you may remember that the <code>singularity</code> command isn't available to use by default on the HPC systems: we needed to run <code>module load</code> first. If we tried to run the workflow now, we would get an error that <code>singularity</code> couldn't be found. Luckily, Nextflow has us covered here once again: the <code>process.module</code> configuration option lets us define modules that we want to load when running a process. Go ahead and update the <code>process</code> scope to define the <code>singularity</code> module:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre>"},{"location":"part1/01_8_nfcore_config/#183-configuring-hpc-resources","title":"1.8.3 Configuring HPC resources","text":"<p>We now have a configuration file with both our executor defined and singularity enabled. There are just a few finishing touches we need to make, primarily around defining the HPC queue that we want to use and some additional options for how to handle files. These include:</p> <pre><code>- Specifically telling the HPC scheduler which HPC project we want to use\n- Defining the HPC queue that we want our jobs submitted to\n- Defining how files should be staged in the working directories\n- Defining how Nextflow should determine when to use the outputs of previous runs when using the `-resume` flag\n- Enabling the trace file so we can track our resource usage and optimise our jobs\n</code></pre> <p>Exercise: Finalise the config with resource requirements</p> <p>Let's start with defining the project we want to use. Most HPCs will assign each user a default project to use when submitting jobs, but it is good practice to be explicit about it, especially if you are part of several HPC projects and switch between the often. We can do this with the <code>process.clusterOptions</code> setting, which lets us pass arbitrary parameters to the scheduler:</p> Gadi (PBSpro)Setonix (Slurm) <p>On Gadi, we set the project via the <code>-P</code> option. We will again use the groovy function <code>System.getenv()</code> to grab the value of the <code>$PROJECT</code> environment variable, which holds our default project ID, and pass that to the <code>-P</code> option:</p> <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>On Gadi, we also need to explicitly tell the HPC to mount the scratch space for our project. Normally, we would do this with the <code>-l storage=scratch/&lt;PROJECT ID&gt;</code> option to the <code>qsub</code> command; in this case, we could use the <code>clusterOptions</code> setting to specify the scratch space parameter as well. However, the Nextflow version that is available on Gadi has a special configuration option built in that lets us simplify this a bit: instead, we can use the <code>storage</code> option within the <code>process</code> scope to specify the scratch space (and any other storage locations we might like to mount). Note that this is NOT a standard Nextflow feature, and is only specific to Gadi:</p> <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>On Setonix, we set the project via the <code>--account</code> option. We will again use the groovy function <code>System.getenv()</code> to grab the value of the <code>$PAWSEY_PROJECT</code> environment variable, which holds our default project ID, and pass that to the <code>--account</code> option. At the same time, we will also use the <code>clusterOptions</code> configuration to specify the <code>--reservation=NextflowHPC</code> flag and tell Nextflow to request this reserved node when submitting each process.</p> <pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>The next thing that we will define is the HPC queue that we wish to submit our jobs to. For the purposes of our example today, we only need the basic queue on each system. However, it is good practice to dynamically specify the queue based on resource requirements; that way, large jobs won't fail or be rejected entirely by the HPC scheduler due to invalid resource requests.</p> <p>Nextflow lets us define the queue that we want via the <code>queue</code> option in the <code>process</code> scope. We can dynamically specify the queue by using curly braces to wrap around a conditional statement that tests how much memory each job needs:</p> Gadi (PBSpro)Setonix (Slurm) <p>On Gadi, the <code>normalbw</code> queue supports tasks with up to 128GB of memory. If we need more than that, we want to use the <code>hugemembw</code> queue. We can achieve this using a short-hand if-else statement in groovy: <code>&lt;condition&gt; ? &lt;value if true&gt; : &lt;value if false&gt;</code>. We can ask whether the memory required by the current task (<code>task.memory</code>) is less than 128GB; if so, we set <code>queue</code> to <code>normalbw</code>, otherwise we set it to <code>hugemembw</code>. By wrapping the whole statement in curly braces, we ensure that it is evaluated when the task runs:</p> <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    queue = { task.memory &lt; 128.GB ? 'normalbw' : 'hugemembw' }\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>Read the docs!</p> <p>Remember to check NCI's \"Queue Limits\" page when configuring the <code>queue</code> for your pipelines on Gadi, as it contains important information about how and when to select each particular queue. We are using a fairly naive selection method today for simplicity, but more complex queue selection methods are possible and advisable for larger pipelines.</p> <p>On Setonix, the <code>work</code> queue supports tasks with up to 230GB of memory. If we need more than that, we want to use the <code>highmem</code> queue. We can achieve this using a short-hand if-else statement in groovy: <code>&lt;condition&gt; ? &lt;value if true&gt; : &lt;value if false&gt;</code>. We can ask whether the memory required by the current task (<code>task.memory</code>) is less than 230GB; if so, we set <code>queue</code> to <code>work</code>, otherwise we set it to <code>highmem</code>. By wrapping the whole statement in curly braces, we ensure that it is evaluated when the task runs:</p> <pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n    queue = { task.memory &lt; 230.GB ? 'work' : 'highmem' }\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>Read the docs!</p> <p>Remember to check Pawsey's \"Running Jobs on Setonix\" page when configuring the <code>queue</code> for your pipelines on Setonix, as it contains important information about how and when to select each particular queue. We are using a fairly naive selection method today for simplicity, but more complex queue selection methods are possible and advisable for larger pipelines.</p> <p>We want to add just a couple of extra options to the process definition. The first option is the <code>stageInMode</code> option. We will explicitly tell Nextflow that we want to use symbolic links. These are essentially shortcuts that point to another file on the system, and let us refer to inputs within our working directory without physically copying them in, which would use up lots of additional storage space. To set this, we define <code>stageInMode = 'symlink'</code> in the <code>process</code> scope.</p> <p>The second option we want to set is the <code>cache</code> mode. Nextflow lets us use the outputs of previous runs when re-running a pipeline, by specifying the <code>-resume</code> flag on the command line. This is very useful for avoiding re-running jobs when we don't have to. By default, Nextflow uses various features of a file, including its timestamp, to determine if it has changed and whether a job needs to be re-run or not. However, the shared filesystem on HPCs can interfere with the timestamps and cause jobs to re-run when they don't need to. Nextflow provides a workaround for this, by letting us use a <code>lenient</code> mode that ignores the timestamp.</p> <p>Let's set these two options now:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    queue = { task.memory &lt; 128.GB ? 'normalbw' : 'hugemembw' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n    queue = { task.memory &lt; 230.GB ? 'work' : 'highmem' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>We now have a fully-functioning HPC configuration file! We will, however, add just one more feature that will help us monitor the resources we are using and optimise our workflow. This is the <code>trace</code> file, and the next few lines that we add will enable it, set its file name (including a time stamp), and set the values that we want to keep track of:</p> Gadi (PBSpro)Setonix (Slurm) <pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    queue = { task.memory &lt; 128.GB ? 'normalbw' : 'hugemembw' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n\nparams.trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\ntrace {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/trace-${params.trace_timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss'\n}\n</code></pre> <pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n    queue = { task.memory &lt; 230.GB ? 'work' : 'highmem' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n\nparams.trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\ntrace {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/trace-${params.trace_timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss'\n}\n</code></pre> <p>Let's quickly break down this new code:</p> <ul> <li><code>params.trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')</code>: This sets a custom parameter called <code>trace_timestamp</code> and sets it to the current date and time. This will let us create a unique file for every run.</li> <li><code>trace { ... }</code>: This defines the trace file scope, and all options within are specific to defining that file.<ul> <li><code>enabled = true</code>: This simply enables the use of the trace file</li> <li><code>overwrite = false</code>: This prevents a trace file from being overwritten</li> <li><code>file = \"./runInfo/trace-${params.trace_timestamp}.txt\"</code>: This sets the file path for the trace file, using the <code>trace_timestamp</code> parameter we set just above</li> <li><code>fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss'</code>: This defines the fields that we want to capture within the trace file, including the task name, its status, how long it ran for, and how efficiently it used the CPUs and memory provided to it.</li> </ul> </li> </ul> <p>And that's it! You are now ready to re-run the workflow and Nextflow will now know how to submit the jobs to your assigned HPC and how to use Singularity to run each job.</p> <pre><code>./run.sh\n</code></pre> Result... <p>After a few moments as the pipeline starts up, you should notice the tasks getting submitted to the HPC:</p> Output<pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `sarek/main.nf` [exotic_cray] DSL2 - revision: 3954909713\n\n...\n\nexecutor &gt;  pbspro (1)\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                   -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                   -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                               -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                  -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                               -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                  -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                      -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                     -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                         -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                              -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                                       -\n[-        ] process &gt; NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED           [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR                             -\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ                               -\n[-        ] process &gt; NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ                               -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP            -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                 -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                  -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                    -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                            -\n[5f/63da05] process &gt; NFCORE_SAREK:SAREK:FASTQC (test_sample1-all)                                [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTP                                                    [  0%] 0 of 1\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM     -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM     -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN   -\n[-        ] process &gt; NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:MERGE_BAM                       -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM                 -\n[-        ] process &gt; NFCORE_SAREK:SAREK:BAM_TO_CRAM_MAPPING                                      -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS                             -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:MOSDEPTH                                   -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_TO_BAM                                              -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP          -\n[-        ] process &gt; NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM          -\n[-        ] process &gt; NFCORE_SAREK:SAREK:MULTIQC\n</code></pre> <p>You might notice that the jobs take a while to start. The reason for this is because we're using the <code>sarek</code> default resource requirements for each process. <code>sarek</code> has been designed around whole genome sequencing data, which is usually quite large and requires a lot of computing power to run. However, for our workshop today, we are working with a very small test dataset that only requires 1 CPU for each job and at most 5GB of memory. If your pipeline has finished, you can inspect the trace file within the <code>runInfo</code> folder and see that the CPU and memory percentage is quite low for many of the tasks:</p> <pre><code># Your trace file will have a unique name based on the time it was run\ncat runInfo/trace-2025-11-18_13-14-15.txt\n</code></pre> Output<pre><code>name    status  exit    duration        realtime        cpus    %cpu    memory  %mem    peak_rss\nNFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (no_intervals)        COMPLETED       0       12.9s   0ms     1       78.4%   1 GB    0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQC (test_sample1-all)    COMPLETED       0       13.3s   2s      4       201.4%  4 GB    0.1%    328.9 MB\nNFCORE_SAREK:SAREK:FASTP (test_sample1-all)     COMPLETED       0       16.3s   1s      12      85.9%   4 GB    0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       9.6s    0ms     24      102.3%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       12.8s   1s      24      96.4%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       9.8s    0ms     24      105.9%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       11.5s   0ms     24      104.2%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       10.4s   0ms     24      27.1%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       13.4s   1s      24      44.5%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       12.5s   0ms     24      230.8%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       14.5s   0ms     24      275.7%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       11.5s   0ms     24      34.9%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       13.4s   0ms     24      106.8%  30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       15.4s   0ms     24      90.1%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  COMPLETED       0       12.4s   1s      24      53.0%   30 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:MERGE_BAM (test_sample1)    COMPLETED       0       14.3s   0ms     2       78.0%   12 GB   0.0%    2 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (test_sample1)      COMPLETED       0       9.9s    0ms     1       87.4%   1 GB    0.0%    4 MB\nNFCORE_SAREK:SAREK:BAM_TO_CRAM_MAPPING (test_sample1)   COMPLETED       0       14.9s   1s      2       65.0%   4 GB    0.0%    4 MB\nNFCORE_SAREK:SAREK:MULTIQC      COMPLETED       0       29.9s   20.3s   4       72.6%   12 GB   0.2%    661.9 MB\n</code></pre> <p>The amount of memory each job requested is in the <code>mem</code> column of this file, while the actual amount used is in the final <code>peak_rss</code> column. Note how most jobs are requesting several GB of memory, but are actually only using a few MB:</p> <pre><code># Pull out just the task name, memory requested, and memory used columns\ncut -f 1,8,10 runInfo/trace-2025-11-18_13-14-15.txt\n</code></pre> Output<pre><code>name    memory  peak_rss\nNFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (no_intervals)        1 GB    2 MB\nNFCORE_SAREK:SAREK:FASTQC (test_sample1-all)    4 GB    328.9 MB\nNFCORE_SAREK:SAREK:FASTP (test_sample1-all)     4 GB    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  30 GB   2 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:MERGE_BAM (test_sample1)    12 GB   2 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (test_sample1)      1 GB    4 MB\nNFCORE_SAREK:SAREK:BAM_TO_CRAM_MAPPING (test_sample1)   4 GB    4 MB\nNFCORE_SAREK:SAREK:MULTIQC      12 GB   661.9 MB\n</code></pre> <p>Also note how many instances of <code>BWAMEM1_MEM</code> ran: 12. This is a scatter-gather pattern at work: the <code>FASTP</code> job breaks up the input FASTQs into multiple smaller files (12 in this case), each of which gets independently processed by <code>bwa mem</code>. The BAMs generated by <code>bwa mem</code> then get merged back together by the <code>MERGE_BAM</code> process. However, for our dataset, 12 parallel <code>bwa mem</code> processes is a bit overkill.</p> <p>These are signs that we could significantly optimise the pipeline for our dataset, which is what we will do in the next lesson.</p> <p>Note</p> <p>If your pipeline hasn't finished after a few minutes, you can cancel the run with a <code>Ctrl + C</code> keyboard combination. In the final section for today, we will create another configuration file to layer on top of our exisitng configuration and fine-tune our tasks to run more efficiently.</p> <p>How are you going?</p> <p>If you're following along so far, let us know by reacting on zoom with a \" Yes\".</p> <p>If you're running into any issues, please react with a \" No\" and we can help out before we move on to the next section.</p>"},{"location":"part1/01_8_nfcore_config/#184-nf-core-configs","title":"1.8.4 nf-core configs","text":"<p>You might be wondering: since these HPCs are so widely used by many researchers, has anyone else created similar institutional configurations that we could use instead of building our own? And the answer is yes! In fact, nf-core host a wide variety of these community-built configurations on their webstie.</p> <p>In fact, the configs we have built in this lesson are slightly simplified versions of the published NCI Gadi and Pawsey Setonix configs on the nf-core website:</p> Gadi (PBSpro)Setonix (Slurm) Our gadi.config file<pre><code>process {\n    executor = 'pbspro'\n    module = 'singularity'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    queue = { task.memory &lt; 128.GB ? 'normalbw' : 'hugemembw' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n\nparams.trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\ntrace {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/trace-${params.trace_timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss'\n}\n</code></pre> nf-core nci_gadi.config file<pre><code>// NCI Gadi nf-core configuration profile\nparams {\n    config_profile_description = 'NCI Gadi HPC profile provided by nf-core/configs'\n    config_profile_contact = 'Georgie Samaha (@georgiesamaha), Matthew Downton (@mattdton)'\n    config_profile_url = 'https://opus.nci.org.au/display/Help/Gadi+User+Guide'\n    project = System.getenv(\"PROJECT\")\n}\n\n// Enable use of Singularity to run containers\nsingularity {\n    enabled = true\n    autoMounts = true\n}\n\n// Submit up to 300 concurrent jobs (Gadi exec max)\n// pollInterval and queueStatInterval of every 5 minutes\n// submitRateLimit of 20 per minute\nexecutor {\n    queueSize = 300\n    pollInterval = '5 min'\n    queueStatInterval = '5 min'\n    submitRateLimit = '20 min'\n}\n\n// Define process resource limits\nprocess {\n    executor = 'pbspro'\n    storage = \"scratch/${params.project}\"\n    module = 'singularity'\n    cache = 'lenient'\n    stageInMode = 'symlink'\n    queue = { task.memory &lt; 128.GB ? 'normalbw' : (task.memory &gt;= 128.GB &amp;&amp; task.memory &lt;= 190.GB ? 'normal' : (task.memory &gt; 190.GB &amp;&amp; task.memory &lt;= 1020.GB ? 'hugemembw' : '')) }\n    beforeScript = 'module load singularity'\n}\n\n// Write custom trace file with outputs required for SU calculation\ndef trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\ntrace {\n    enabled = true\n    overwrite = false\n    file = \"./gadi-nf-core-trace-${trace_timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,rss'\n}\n</code></pre> Our setonix.config file<pre><code>process {\n    executor = 'slurm'\n    module = 'singularity/4.1.0-slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n    queue = { task.memory &lt; 230.GB ? 'work' : 'highmem' }\n    stageInMode = 'symlink'\n    cache = 'lenient'\n}\n\nexecutor {\n    queueSize = 30\n    submitRateLimit = '20 min'\n    pollInterval = '15 sec'\n    queueStatInterval = '15 sec'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n\nparams.trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\ntrace {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/trace-${params.trace_timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss'\n}\n</code></pre> nf-core pawsey_setonix.config file<pre><code>// Pawsey Setonix nf-core configuration profile\nparams {\n    config_profile_description = 'Pawsey Setonix HPC profile provided by nf-core/configs'\n    config_profile_contact     = 'Sarah Beecroft (@SarahBeecroft), Georgie Samaha (@georgiesamaha)'\n    config_profile_url         = 'https://support.pawsey.org.au/documentation/display/US/Setonix+User+Guide'\n    max_cpus                   = 64\n    max_memory                 = 230.GB\n}\n// Enable use of Singularity to run containers\nsingularity {\n    enabled     = true\n    autoMounts  = true\n    autoCleanUp = true\n}\n// Submit up to 1024 concurrent jobs\nexecutor {\n    queueSize = 1024\n}\n// Define process resource limits\n// See: https://support.pawsey.org.au/documentation/pages/viewpage.action?pageId=121479736#RunningJobsonSetonix-Overview\nprocess {\n    resourceLimits = [\n        memory: 230.GB,\n        cpus: 64\n    ]\n    executor       = 'slurm'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')}\"\n    module         = 'singularity/4.1.0-slurm'\n    cache          = 'lenient'\n    stageInMode    = 'symlink'\n    queue = { task.memory &lt;= 230.GB ? 'work' : 'highmem' }\n}\n</code></pre> <p>You can see that there are many similarities between our configuration and the nf-core version. The nf-core configs contain a few additional parameters and some more detailed queue selection logic, but ultimately they accomplish the same task. So, why didn't we introduce these earlier? Because by building one from scratch, you have now learned about how Nextflow is configured to use executors to communicate with HPC schedulers and request resources for your jobs.</p> <p>Also know that these community-built configurations are a good starting point, but may need to be slightly altered for your particular use case. In particular, depending on what you are running, you may desire a more complex queue selection logic that takes CPU usage into account as well as memory, and if you have a pipeline that uses GPUs, you will need to include logic to request a GPU queue.</p>"},{"location":"part1/01_9_nfcore_layer/","title":"1.9 Layering Nextflow configurations","text":"<p>Learning objectives</p> <ul> <li>Learn how Nextflow configurations can be layered on top of one another</li> <li>Understand the configuration priorities</li> <li>Learn how to fine-tune processes using the <code>withName</code> directive</li> <li>Understand when you would want to fine-tune process requirements</li> </ul> <p>We have now managed to configure and run an nf-core pipeline, but we have also seen that for our test dataset it isn't very well optimised: by default, the <code>sarek</code> pipeline requests many more CPUs and much more RAM than necessary for our purposes. This problem isn't just relevant to small test datasets; sometimes you might find when running a large dataset that the pipeline hasn't been optimised quite as well as you would like and is requesting fewer resources than you need. In these cases, what we would like to do is have a fine level of control over each process and the resources they request.</p> <p>We have already seen how we can define a custom configuration file and layer it over the top of the default configuration using the <code>-c</code> option to Nextflow. We have also seen how the <code>sarek</code> pipeline defines the resources required by many of its processes within the <code>conf/base.config</code> file using <code>withLabel</code> and <code>withName</code> directives. In this final section of today's workshop, we will try to optimise the processes that we are running to more efficiently use the HPC resources by defining a new custom configuration file.</p>"},{"location":"part1/01_9_nfcore_layer/#191-configuration-priorities","title":"1.9.1 Configuration priorities","text":"<p>Before we get started, it's important to understand how Nextflow prioritises configuration files. Because we can provide configuration information at various levels and using multiple files, it is possible for some options in these places to overlap, and Nextflow needs to know which ones to give precedence to. Referring to the Nextflow configuration documentation, configuration files are prioritised in the following order, from lowest to highest priority:</p> <ol> <li>The config file $HOME/.nextflow/config</li> <li>The config file nextflow.config in the project directory (i.e. at the same level as the <code>main.nf</code> script)</li> <li>The config file nextflow.config in the launch directory (i.e. the directory in which you run <code>nextflow run ...</code>)</li> <li>Config files specified using the <code>-c &lt;config-files&gt;</code> option</li> </ol> <p>Furthermore, when using the <code>-c</code> option, multiple configuration files can be provided, separated by commas, and are prioritised from lowest to highest in the order they are specified.</p> Example: A very simple layered configuration <p>Consider the following (very basic) Nextflow file:</p> example.nf<pre><code>params.value = \"hello\"\n\nworkflow {\n    println params.value\n}\n</code></pre> <p>If we run this 'workflow', it will print the contents of the <code>value</code> parameter, i.e. \"hello\":</p> <pre><code>nextflow run example.nf\n</code></pre> Output<pre><code>N E X T F L O W   ~  version 24.10.5\n\nLaunching `example.nf` [tender_kay] DSL2 - revision: 573919f401\n\nhello\n</code></pre> <p>Now suppose we create a <code>nextflow.config</code> file and set <code>value</code> to something different:</p> nextflow.config<pre><code>params.value = \"bye\"\n</code></pre> <p>Now, the workflow will print \"bye\"</p> <pre><code>nextflow run example.nf\n</code></pre> Output<pre><code>N E X T F L O W   ~  version 24.10.5\n\nLaunching `example.nf` [tender_kay] DSL2 - revision: 573919f401\n\nbye\n</code></pre> <p>If we create another config file, define <code>params.value</code> in there, and layer it on top, that value will be used:</p> custom.config<pre><code>params.value = \"seeya\"\n</code></pre> <pre><code>nextflow run example.nf -c custom.config\n</code></pre> Output<pre><code>N E X T F L O W   ~  version 24.10.5\n\nLaunching `example.nf` [tender_kay] DSL2 - revision: 573919f401\n\nseeya\n</code></pre> <p>And if we create a second custom config, define yet another value for <code>params.value</code>, and layer it on top as well, that value will be used:</p> another_custom.config<pre><code>params.value = \"ciao\"\n</code></pre> <pre><code>nextflow run example.nf -c custom.config,another_custom.config\n</code></pre> Output<pre><code>N E X T F L O W   ~  version 24.10.5\n\nLaunching `example.nf` [tender_kay] DSL2 - revision: 573919f401\n\nciao\n</code></pre> <p>Process directives, such as CPU and memory requirements, can be configured in a number of ways, and these too are evaluated in a particular order by Nextflow. Briefly, they are prioritised in the following order, from lowest to highest priority:</p> <ol> <li>Default process configuration settings in the configuration files (e.g. <code>process.cpu = 1</code>)</li> <li>Process directives defined in the process definition</li> <li>Process configuration settings within a matching <code>withLabel</code> selector</li> <li>Process configuration settings within a matching <code>withName</code> selector</li> </ol> Example: Configuring default resources and using process selectors <p>Consider the following <code>process {}</code> scope within a configuration file:</p> <pre><code>process {\n    cpus = 4\n    withLabel: hello { cpus = 8 }\n    withName: bye { cpus = 16 }\n}\n</code></pre> <p>This configuration will have the following consequences:</p> <ul> <li>By default, all processes will be given 4 CPUs, unless their process definitions contain a <code>cpus</code> directive</li> <li>Any process given the <code>label</code> \"hello\" will instead be given 8 CPUs</li> <li>Any process named \"bye\" will be given 16 CPUs</li> </ul>"},{"location":"part1/01_9_nfcore_layer/#192-optimising-nf-coresarek-for-our-data","title":"1.9.2 Optimising <code>nf-core/sarek</code> for our data","text":"<p>We saw that <code>sarek</code> wasn't well-optimised for such a small test dataset. In this next exercise, we will build a new custom configuration file to explicitly set the resources we want for each of the stages.</p> <p>How much will this improve our run?</p> <p>It's hard to say how much these optimisations will improve queue times: if the HPC is relatively quiet on the day, our queue times may already be quite short and so we won't see any significant improvement. When the HPC is being heavily used, we may notice that the poorly-optimised jobs take much longer to start and the following changes may help to reduce those wait times by letting our jobs 'slot in' to the gaps between larger jobs from other users.</p> <p>Also note that for such a small dataset, we won't see a noticable cost reduction - we aren't spending that much to begin with! However, the goal of this exercise is to teach best practices that can be applied to much larger runs - in which case it could make the difference of hundreds of dollars!</p> <p>Exercise: Fine-tune <code>nf-core/sarek</code></p> <p>Start by creating a new blank file within the <code>config/</code> folder called <code>custom.config</code> and open it up in VSCode.</p> <pre><code>touch config/custom.config\n</code></pre> <p>We have eight distinct processes that we want to fine-tune:</p> <ul> <li><code>TABIX_BGZIPTABIX_INTERVAL_COMBINED</code></li> <li><code>FASTQC</code></li> <li><code>FASTP</code></li> <li><code>BWAMEM1_MEM</code></li> <li><code>MERGE_BAM</code></li> <li><code>INDEX_MERGE_BAM</code></li> <li><code>BAM_TO_CRAM_MAPPING</code></li> <li><code>MULTIQC</code></li> </ul> <p>From the trace file we received from the previous run of <code>sarek</code>, we saw that the processes were requesting between 1 and 24 CPUs, and up to 30 GB of memory for the <code>BWAMEM1_MEM</code> process. For our example dataset, these values are overkill. Instead, we can get away with just 1-2 CPUs and 1-2GB of memory for each task. We'll also give each task just 2 minutes to complete, which is more than enough time.</p> <p>Let's translate this into the Nextflow configuration format:</p> config/custom.config<pre><code>process {\n\n    withName: 'TABIX_BGZIPTABIX_INTERVAL_COMBINED' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'FASTQC' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'FASTP' {\n        cpus = 2\n        memory = 2.GB\n        time = 2.min\n    }\n\n    withName: 'BWAMEM1_MEM' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'MERGE_BAM' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'INDEX_MERGE_BAM' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'BAM_TO_CRAM_MAPPING' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n    withName: 'MULTIQC' {\n        cpus = 1\n        memory = 1.GB\n        time = 2.min\n    }\n\n}\n</code></pre> <p>We are now giving most tasks 1 CPU and 1 GB of memory, except for <code>FASTP</code>, which we'll give 2 CPUs and 2 GB of memory. When running <code>sarek</code>, giving <code>FASTP</code> multiple CPUs causes it to also split the FASTQ up, thereby implementing a scatter-gather pattern. So, in this case, we are scattering our input FASTQ into two chunks and aligning them individually.</p> <p>Now that we have our custom configuration file created, we need to update our run script once again and add the new file to the <code>-c</code> option:</p> Gadi (PBSpro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/gadi.config,config/custom.config\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.single.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/setonix.config,config/custom.config\n</code></pre> <p>And now we're ready to re-run the pipeline!</p> <pre><code>./run.sh\n</code></pre> <p>After a few minutes, the pipeline should finish. We can again inspect the trace file from the run to see how much CPU and memory was requested and used:</p> <pre><code># Your trace file will have a unique name based on the time it was run\ncut -f 1,6,8,10 runInfo/trace-2025-11-18_14-15-16.txt\n</code></pre> Output<pre><code>name    cpus    memory  peak_rss\nNFCORE_SAREK:SAREK:FASTP (test_sample1-all)     2       2 GB    2 MB\nNFCORE_SAREK:SAREK:FASTQC (test_sample1-all)    1       1 GB    218.8 MB\nNFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (no_intervals)        1       1 GB    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  1       1 GB    2 MB\nNFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test_sample1)  1       1 GB    2 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:MERGE_BAM (test_sample1)    1       1 GB    4 MB\nNFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (test_sample1)      1       1 GB    2 MB\nNFCORE_SAREK:SAREK:BAM_TO_CRAM_MAPPING (test_sample1)   1       1 GB    18.3 MB\nNFCORE_SAREK:SAREK:MULTIQC      1       1 GB    686.6 MB\n</code></pre> <p>We can see that <code>FASTQC</code> and <code>MULTIQC</code> are now using a much larger proportion of the memory assigned to them (in this case 200-700MB out of a total 1GB), so we are much more efficiently using our resources for these jobs. The other jobs are still only using a few MB of memory to run, but for such small jobs there isn't too much utility in optimising these any further; from a cost perspective, these jobs are already under-utilising memory per CPU, so there would be no benefit to reducing the request furhter; and we could also start running into failures due to fluctuations in the memory used between runs.</p> <p>We can also see that we are now running only two instances of the <code>BWAMEM1_MEM</code> process. Since we gave <code>FASTP</code> just two threads to work with, it ended up splitting our FASTQ file into two chunks, each of which was run through an independent <code>BWAMEM1_MEM</code> process. This is much more sensible given our small dataset!</p> <p>The right config for the right situation</p> <p>We have now built two separate configuration files - an HPC-specific file (<code>gadi.config</code> / <code>setonix.config</code>) and our optimisation file (<code>custom.config</code>).</p> <p>We also know that <code>sarek</code> has multiple levels of configuration: <code>nextflow.config</code>, <code>conf/base.config</code>, and <code>conf/modules/*.config</code>.</p> <p>Separating out our configuration files is good practice as it makes our pipelines more portable and reproducible.</p> <p>Typically, we have three levels of configuration:</p> <ol> <li>Pipeline-level configuration. This includes the <code>nextflow.config</code> file and any other config files that are bundled with the pipeline. These should never be changed when running the pipeline and should be designed to work everywhere.</li> <li>Institute-level configuration. This is what we built in the previous lesson (<code>gadi.config</code> and <code>setonix.config</code>). These are configuration files that are specific to the hardware and the systems that the pipeline is being run on. Ideally, they should be reusable for all Nextflow pipelines that run on that system. They should not need to be changed between pipelines on the same system.</li> <li>Run-level configuration. This is what we have just built (<code>custom.config</code>). These are necessary when the default values provided by the pipeline aren't tuned well-enough to your specific data.</li> </ol>"},{"location":"part1/01_9_nfcore_layer/#193-scaling-up-to-multiple-samples","title":"1.9.3 Scaling up to multiple samples","text":"<p>Now that we have a fully-functioning run script and custom configuration, we can try scaling up to multiple samples.</p> <p>Exercise: Run mapping on multiple samples</p> <p>Update the <code>run.sh</code> script to use the full samplesheet with all three test samples. At the same time, add the <code>-resume</code> flag so that we don't have to re-run the previously run jobs for the first sample. When doing so, be sure to add a space and a backslash (<code>\\</code>) to the preceding line to indicate that the command continues on the next line:</p> Gadi (PBSpro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.fq.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/gadi.config,config/custom.config \\\n    -resume\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run sarek/main.nf \\\n    --input ../data/fqs/samplesheet.fq.csv \\\n    --fasta ../data/ref/Hg38.subsetchr20-22.fasta \\\n    --fasta_fai ../data/ref/Hg38.subsetchr20-22.fasta.fai \\\n    --dict ../data/ref/Hg38.subsetchr20-22.dict \\\n    --bwa ../data/ref \\\n    --step mapping \\\n    --skip_tools markduplicates,baserecalibrator,mosdepth,samtools \\\n    --outdir results \\\n    --no_intervals true \\\n    --igenomes_ignore true \\\n    -c config/setonix.config,config/custom.config \\\n    -resume\n</code></pre> <p>Go ahead and re-run the script. The <code>-resume</code> flag will mean that the previously-run tasks for the first sample (<code>FASTQC</code>, <code>FASTP</code>, <code>BWAMEM1_MEM</code>, etc.) will not be re-run, but instead their outputs will be reused. Only the new samples will be run through these processes. <code>MULTIQC</code> will re-run at the end of the pipeline as it needs to summarise the results from all three samples.</p>"},{"location":"part2/20_intro/","title":"2.0 Introduction","text":"<p>Learning objectives</p> <ul> <li>Recall the overall structure of a modular Nextflow pipeline, including the roles of <code>main.nf</code>, <code>modules/</code>, and configuration files</li> <li>Recognise the importance of separating workflow logic from system-specific configuration for portability and reproducibility</li> <li>Identify which parts of the pipeline need to be adapted when moving between systems (e.g. from local to HPC)</li> </ul>"},{"location":"part2/20_intro/#201-log-back-in-to-your-assigned-hpc","title":"2.0.1 Log back in to your assigned HPC","text":"<p>Reconnect to your HPC through VSCode, the same way we did in Part 1.</p> <p>Log in to your assigned HPC with the user account and password provided to you on day 1:</p> GadiSetonix <pre><code>ssh &lt;username&gt;@gadi.nci.org.au\n</code></pre> <pre><code>ssh &lt;username&gt;@setonix.pawsey.org.au\n</code></pre> <p>Note</p> <p>Be sure substitute your assigned user name for <code>&lt;username&gt;</code> in the above code example.</p> <p>Navigate to the scratch space for the workshop project, then open your cloned part2 repository. Note that <code>$USER</code> is an environment variable that automatically expands to your username, so you do not need to replace this with your training username.</p> GadiSetonix <pre><code>cd /scratch/vp91/$USER/nextflow-on-hpc-materials/part2\n</code></pre> <pre><code>cd /scratch/courses01/$USER/nextflow-on-hpc-materials/part2\n</code></pre>"},{"location":"part2/20_intro/#202-configuring-a-custom-pipeline","title":"2.0.2 Configuring a custom pipeline","text":"<p>Part 2 of this workshop builds on the foundational HPC and Nextflow configuration concepts introduced in Part 1. We will now apply these concepts to configure a custom variant calling pipeline for efficient execution on HPC systems.</p> <p>To keep the focus on configuration, the pipeline code and logic are provided for you and we will not be reviewing the contents of input and output files in detail, beyond configuration needs. </p> <p>Learn to build custom Nextflow workflows</p> <p>See our Nextflow For the Life Sciences materials for an introduction to building Nextflow workflows. This workshop expands on Nextflow for the Life Sciences.   </p> <p>We\u2019ll begin by getting the pipeline running on the HPC, then progressively explore how to benchmark performance, understand HPC-specific constraints, and implement optimisations to improve efficiency and resource use.</p> <p>Throughout this section, we\u2019ll continue using the variant calling example to deepen your understanding of the key decisions involved in tuning pipelines for the specific HPC infrastructure you work on.</p> <p>We will be using small data sets!</p> <p>In this section, we will continue using the small paired-end reads from Part 1 to demonstrate key concepts related to running and configuring workflows on HPC systems.</p> <p>Because the data is deliberately small, the processes will run quickly and with minimal resource requirements. This helps illustrate how the workflow behaves, but it also means that:</p> <ul> <li>Some resource-related errors (e.g. out-of-memory, walltime exceeded) will not occur, even if your configuration is suboptimal.</li> <li>Performance trade-offs (e.g. with different <code>cpus</code> or <code>memory</code> settings) may be less noticeable than with real-world data.</li> <li>HPC scheduling behaviour may differ slightly - smaller jobs are often scheduled more quickly and occupy less system space.</li> </ul> <p>When running real data sets on HPC systems, you may encounter different behaviours, longer runtimes, and additional errors. During Part 2 you will gain practice in identifying and debugging workflow errors, to build the skills you will need to run your own data analyses with Nextflow on HPC.  </p>"},{"location":"part2/20_intro/#203-why-do-you-need-custom-pipelines","title":"2.0.3 Why do you need custom pipelines?","text":"<p>There are several reasons why you might need to develop or adapt your own Nextflow pipeline:</p> <ul> <li>Tailored to your specific needs: custom pipelines give you full control over input/output formats, tool parameters, workflow logic, and configuration options.</li> <li>Gaps in available pipelienes: existing pipelines (e.g. nf-core) may not cover your use case, or a relevant pipeline may not exist at all.</li> <li>Resource optimisation: nf-core pipelines are generalised by design and may be over-provisioned or misconfigured for your HPC environment. Although easier to get running out-of-the box, this could lead to inefficient use of HPC resources or being charged excess service units (SUs)!</li> </ul>"},{"location":"part2/20_intro/#203-the-scenario-variant-calling-on-hpc","title":"2.0.3 The scenario: variant calling on HPC","text":"<p>Our use case for today is taking raw DNA sequence data from a number of human patients and using a series of data processing steps to obtain a final results file containing genetic variants for each patient. </p> <p>Remember, it does not matter if you are unfamiliar with the biology or the tools used; the focus is on learning how to efficiently run Nextflow pipelines on HPC.</p> <p>We start with an unoptimised and minimally configured pipeline (like something that was developed and run on a laptop), and run through the entire workflow on a single sample. We will then explore optimisation strategies, implement them, and finally scale up the workflow to handle multiple samples efficiently.</p> <p>The diagram below shows a high level overview of the workflow we will be creating, starting with the raw data for each patient, mapping it against a human reference genome file, and then identifying and summarising the genetic variants found in the input data.  </p> <p></p>"},{"location":"part2/20_intro/#204-the-pipeline-file-anatomy","title":"2.0.4 The pipeline file anatomy","text":"<p>This pipeline builds on the structure introduced in our introductory Nextflow for the life sciences workshop, where the workflow is separated into the data processing logic, and system-specific configuration. This layout helps keep things reproducible, easy to maintain, and simple to adapt across different environments - like moving from your laptop to an HPC!</p> <p>Recall the demo Nextflow workflow we explored in lesson 1.5.2. Our custom workflow will extend on this by introducing some new features that help us stay organised. This includes: </p> <ul> <li><code>config/</code> to house our custom configuration files</li> <li><code>modules/</code>to house our process files as <code>.nf</code> files</li> </ul> <p>At a glance:</p> <pre><code>tree -L 2\n</code></pre> <pre><code># (some folders are truncated)\n.\n\u251c\u2500\u2500 conf                    # System-specific configuration files\n\u2502   \u251c\u2500\u2500 pbspro.config\n\u2502   \u2514\u2500\u2500 slurm.config\n\u251c\u2500\u2500 main.nf                 # Main workflow structure\n\u251c\u2500\u2500 modules                 # Individual process modules\n\u2502   \u251c\u2500\u2500 align_chunk.nf\n\u2502   \u251c\u2500\u2500 align.nf\n\u2502   \u251c\u2500\u2500 fastqc.nf\n\u2502   \u251c\u2500\u2500 genotype.nf\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 nextflow.config         # Base parameters and profile defs\n\u251c\u2500\u2500 samplesheet_full.csv\n\u251c\u2500\u2500 samplesheet_single.csv\n\u2514\u2500\u2500 ...\n</code></pre> <p>Consider a basic Nextflow run command with this structure, where a user needs to specify some parameters and (optionally) a configuration file: </p> <ul> <li><code>main.nf</code> is the executable file that identifies the workflow structure, inputs, and processes that are pulled from <code>modules/</code></li> <li><code>--parameter</code> flag matches a parameter initialised in the <code>nextflow.config</code> and applies to the workflow execution </li> <li><code>-profile</code> flag is used to specify custom configuration details for our specific environments, but it can also apply to other customisations</li> </ul> <p></p> <p>In part 2, we will only edit the module files only if it involves optimising the workflow performance. The majority of exercises will focus on configuring how and where these steps run on HPCs, using separate config files. This separation makes it easy to test a pipeline locally and later scale it up on a system like Gadi or Setonix, without rewriting processes.</p> <p>Why modules?</p> <p>Nextflow pipelines are often set up using modules, which help keep things clean and organised. The key idea is that some files can stay the same no matter where you run them, while others are easily tweaked to match the system you're using. This is especially useful when you're testing a pipeline locally first, then moving to run it on an HPC - something many researchers do.</p> <p></p> <p>If everything were hardcoded, you'd end up changing lots of lines across multiple files, which can quickly get messy and error-prone. By using modules, you only need to configure how and where things run, without rewriting the pipeline a itself. It also makes it easier to reuse parts of a pipeline or swap out tools later on, without starting from scratch.</p> <p>For more information, see \"What's in <code>modules/</code>\".</p>"},{"location":"part2/20_intro/#2041-mainnf-and-modules","title":"2.0.4.1 <code>main.nf</code> and <code>modules/</code>","text":"<p>Let's take a look at our custom pipeline's <code>main.nf</code> to see how we have structured our variant calling workflow:</p> <pre><code>cat main.nf\n</code></pre> main.nf<pre><code>include { FASTQC } from './modules/fastqc'\ninclude { ALIGN } from './modules/align'\ninclude { GENOTYPE } from './modules/genotype'\ninclude { JOINT_GENOTYPE } from './modules/joint_genotype'\ninclude { STATS } from './modules/stats'\ninclude { MULTIQC } from './modules/multiqc'\n\n// Define the workflow\nworkflow {\n\n    // Define the fastqc input channel\n    reads = Channel.fromPath(params.samplesheet)\n        .splitCsv(header: true)\n        .map { row -&gt; {\n            // def strandedness = row.strandedness ? row.strandedness : 'auto'\n            [ row.sample, file(row.fastq_1), file(row.fastq_2) ]\n        }}\n\n    bwa_index = Channel.fromPath(params.bwa_index)\n        .map { idx -&gt; [ params.bwa_index_name, idx ] }\n        .first()\n    ref = Channel.of( [ file(params.ref_fasta), file(params.ref_fai), file(params.ref_dict) ] ).first()\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(reads, bwa_index)\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(ALIGN.out.aligned_bam, ref)\n\n    // Gather gvcfs and run joint genotyping\n    all_gvcfs = GENOTYPE.out.gvcf\n        .map { _sample_id, gvcf, gvcf_idx -&gt; [ params.cohort_name, gvcf, gvcf_idx ] }\n        .groupTuple()\n    JOINT_GENOTYPE(all_gvcfs, ref)\n\n    // Get VCF stats\n    STATS(JOINT_GENOTYPE.out.vcf)\n\n    // Collect summary data for MultiQC\n    multiqc_in = FASTQC.out.qc_out\n        .mix(STATS.out.stats_out)\n        .collect()\n\n    /*\n    * Generate the analysis report with the\n    * outputs from fastqc and bcftools stats\n    */\n    MULTIQC(multiqc_in)\n\n}\n</code></pre> <p>We have used:  </p> <ul> <li><code>include { process } from './modules/process-name'</code> to pull in processes from <code>modules/</code> </li> <li>Input channels (e.g. <code>reads</code>, <code>bwa_index</code>, <code>ref</code>) to define how data moves between processes   </li> <li>Channel operators (e.g. <code>.map()</code>, <code>.groupTuple()</code>, <code>.mix()</code>, <code>.collect()</code>) to transform and combine data streams dynamically  </li> <li>Parameterised inputs (e.g. <code>params.ref_fasta</code>, <code>params.samplesheet</code>) so the workflow can be reused on different datasets without having to edit the code</li> </ul> <p>This structure makes it easier to swap in alternative tools and processes, especially later when working with scatter-gather patterns without cluttering <code>main.nf</code> or compromising reproducibility.</p>"},{"location":"part2/20_intro/#2042-nextflowconfig-and-config","title":"2.0.4.2 <code>nextflow.config</code> and <code>config/</code>","text":"<p>Let's take a look at our custom pipeline's <code>nextflow.config</code> to see how we are configuring the workflow execution: </p> <pre><code>cat nextflow.config\n</code></pre> nextflow.config<pre><code>// Define params\nparams {\n    samplesheet = \"$projectDir/samplesheet_single.csv\"\n    ref_prefix = \"$projectDir/../data/ref/Hg38.subsetchr20-22\"\n    ref_fasta = \"${params.ref_prefix}.fasta\"\n    ref_fai = \"${params.ref_prefix}.fasta.fai\"\n    ref_dict = \"${params.ref_prefix}.dict\"\n    bwa_index = \"$projectDir/../data/ref\"\n    bwa_index_name = \"Hg38.subsetchr20-22.fasta\"\n    cohort_name = \"cohort\"\n    outdir = \"results\"\n}\n\n// Define HPC profiles to run with job scheduler\nprofiles {\n    // Use this profile to interact with the scheduler on setonix\n    slurm { includeConfig \"config/slurm.config\" }\n\n    // Use this profile to interact with the scheduler on gadi\n    pbspro { includeConfig \"config/pbspro.config\" }\n}\n</code></pre> <p>We have used: </p> <ul> <li><code>params {...}</code> blocks to centralise all user-controlled parameters</li> <li><code>profiles {...}</code> for profile definitions to enable us to run different configurations </li> <li><code>includeConfig \"conf/name.config\"</code> directives to pull in separate configuration files from <code>conf/</code></li> </ul> <p>Nextflow\u2019s configuration files define how and where each process runs, including what resources to request, which job scheduler to use, and how to execute software.</p> <p>In the context of HPCs, this means specifying:</p> <ul> <li>How many CPUs, memory, and time a process should use</li> <li>The appropriate executor (e.g. PBS Pro on Gadi or Slurm on Setonix)</li> <li>The default queue/partition and optional account/project codes</li> <li>Whether and how to use Singularity containers</li> </ul> <p>These settings are defined in the main <code>nextflow.config</code>, and extended using config profiles. This separation will allow us to run the same pipeline across different HPCs just by switching profiles, without modifying the core workflow. We will build our system-specific configuration files in <code>conf/</code> in the next lesson:</p> <p>Configuration imagination</p> <p>While we are using custom configs to allow us to make our pipeline \"portable\" so it can run on NCI Gadi and Pawsey Setonix HPCs, you can also use custom configurations to tailor your pipeline to many other scenarios. For example: </p> <ul> <li>Different datasets requiring more/less memory </li> <li>Testing vs production runs </li> </ul>"},{"location":"part2/20_intro/#205-summary","title":"2.0.5 Summary","text":"<p>This section introduced the basic structure of the custom variant calling Nextflow pipeline for the remainder of Part 2, emphasising the separation between workflow logic (<code>main.nf</code>, <code>modules/</code>) and system-specific configuration (<code>nextflow.config</code>, <code>conf/</code>). We reviewed how this separation supports portability, reproducibility, and ease of adaptation across environments, such as when transitioning from local testing to running on HPC systems like Gadi (PBS Pro) and Setonix (Slurm).</p>"},{"location":"part2/21_firstrun/","title":"Running a custom pipeline on HPC","text":"<p>Learning objectives</p> <ul> <li>Execute a custom Nextflow pipeline on HPC, applying system-specific configurations</li> <li>Diagnose and troubleshoot workflow failures by inspecting logs and files in work directories </li> <li>Construct a reproducible configuration file for an HPC environment.</li> </ul> <p>Testing and developing in the right environment</p> <p>Containers For the workshop, we have pre-pulled containers and use them from the cache. In real-world scenarios, pulling containers may take time. Consideration to whether your HPC system allows internet access from compute nodes is also important.</p> <p>Running the Nextflow head job As our test data is small and the workflow runs quickly, we will be running the Nextflow <code>run</code> command directly on the login nodes. On HPC, this is neither permitted nor feasible for real data analyses, as HPC login nodes typically kill long-running commands. </p> <p>We recommend using interactive jobs for testing and debugging workflows directly on compute nodes instead of the login nodes. This has the added benefit of testing in the same hardware environment as the final workflow will run. </p> <p>Some HPCs have dedicated workflow queues/partitions to which the Nextflow head job can be submitted. These nodes are set up to support long-running jobs with low resource requirements, as workflow processes that require higher resources are each submitted to different queues/partitions based on the resources specified within the custom infrastructure config. </p> <p>See the recommendations on running the head job for Gadi and Setonix.</p> <p>It is useful to develop your pipelines using a small, representative subset of your data. This allows you:</p> <ul> <li>Rapidly iterate your workflow design</li> <li>Validate environment and software set up</li> <li>Tune pipeline configuration without burning service units</li> <li>Reduce queue wait times</li> <li>Estimate basic performance characteristics of each process</li> </ul>"},{"location":"part2/21_firstrun/#211-run-without-configuration","title":"2.1.1 Run without configuration","text":"<p>We will start configuring our custom pipeline with using a subset of raw reads from a single individual (NA1287) in our sample cohort. This is a good proxy for the other two samples as they are all of the same input data type and about the same size. Once we\u2019re confident everything works as intended, we will scale up to run on the full set of samples.</p> <p>Let's start by running the pipeline out of the box to identify what we need to configure: </p> <p>Exercise: Run out of the box</p> <ul> <li>Load the Nextflow module, following the same method we learnt yesterday:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <pre><code>module load nextflow/24.04.5 \n</code></pre> <pre><code>module load nextflow/24.10.0\n</code></pre> <ul> <li>Run your Nextflow command out of the box:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <pre><code>nextflow run main.nf\n</code></pre> Output <pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [suspicious_almeida] DSL2 - revision: 5e5c4f57e0\n\nexecutor &gt;  local (2)\n[a3/a6da06] FASTQC (fastqc on NA12877) [100%] 1 of 1, failed: 1 \u2718\n[16/860647] ALIGN (1)                  [100%] 1 of 1, failed: 1 \u2718\n[-        ] GENOTYPE                   -\n[-        ] JOINT_GENOTYPE             -\n[-        ] STATS                      -\n[-        ] MULTIQC                    -\nERROR ~ Error executing process &gt; 'FASTQC (fastqc on NA12877)'\n\nCaused by:\nProcess `FASTQC (fastqc on NA12877)` terminated with an error exit status (127)\n\n\nCommand executed:\n\nmkdir -p \"fastqc_NA12877\"\nfastqc -t 1 --outdir \"fastqc_NA12877\" --format fastq NA12877_chr20-22.R1.fq.gz NA12877_chr20-22.R2.fq.gz\n\nCommand exit status:\n127\n\nCommand output:\n(empty)\n\nCommand error:\n.command.sh: line 3: fastqc: command not found\n\nWork dir:\n/scratch/project/username/nextflow-on-hpc-materials/part2/work/a3/a6da061dd65d454add3f000923235d\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n\n-- Check '.nextflow.log' file for details\n</code></pre> <pre><code>nextflow run main.nf\n</code></pre> Output <pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [trusting_brown] DSL2 - revision: 5e5c4f57e0\n\nexecutor &gt;  local (2)\nexecutor &gt;  local (2)\n[e6/21a49e] FASTQC (fastqc on NA12877) [100%] 1 of 1, failed: 1 \u2718\n[68/bda517] ALIGN (1)                  [100%] 1 of 1, failed: 1 \u2718\n[-        ] GENOTYPE                   -\n[-        ] JOINT_GENOTYPE             -\n[-        ] STATS                      -\n[-        ] MULTIQC                    -\nERROR ~ Error executing process &gt; 'ALIGN (1)'\n\nCaused by:\nProcess `ALIGN (1)` terminated with an error exit status (127)\n\n\nCommand executed:\n\nbwa mem -t 1 -R \"@RG\\tID:NA12877\\tPL:ILLUMINA\\tPU:NA12877\\tSM:NA12877\\tLB:NA12877\\tCN:SEQ_CENTRE\" ref/Hg38.subsetchr20-22.fasta NA12877_chr20-22.R1.fq.gz NA12877_chr20-22.R2.fq.gz | samtools sort -O bam -o NA12877.bam\nsamtools index NA12877.bam\n\nCommand exit status:\n127\n\nCommand output:\n(empty)\n\nCommand error:\n.command.sh: line 2: samtools: command not found\n.command.sh: line 2: bwa: command not found\n\nWork dir:\n/scratch/project/username/nextflow-on-hpc-materials/part2/work/68/bda517eeaa47f5ba24a9c401bdeb0e\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\n-- Check '.nextflow.log' file for details\n</code></pre> <p>Why did it fail?</p> <p>Use the output printed to your screen, <code>.nextflow.log</code> file in your current directory, as well as the <code>.command.log</code>, <code>.command.err</code>, and <code>.exitcode</code> files in the work directory of the failed task to identify what caused your workflow run to fail. </p> Answer <p>Our run stopped because one or more processes failed. Exit status <code>127</code> in Linux environments means your system was not able to find the command referenced in the process. This suggests the software is not available in our environment. </p>"},{"location":"part2/21_firstrun/#212-enabling-containers","title":"2.1.2 Enabling containers","text":"<p>All our process modules specify a container to run inside. This can only happen if Singularity is explicitly enabled in our configuration. Let's enable this in our system-specific configuration files and attempt to run again:  </p> <p>Exercise: Enable Singularity</p> <ul> <li>Load the Singularity module, following the same method we learnt yesterday:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <pre><code>module load singularity\n</code></pre> <pre><code>module load singularity/4.1.0-slurm\n</code></pre> <ul> <li>Add the following to your system-specific config file that you can find in <code>config/</code>. Remember, we have already enabled profiles in our <code>nextflow.config</code>, so no need to edit that file. </li> </ul> Gadi (PBS Pro)Setonix (Slurm) config/pbspro.sh<pre><code>process {\n    // Load the globally installed singularity module before running any process\n    module = 'singularity'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> config/slurm.config<pre><code>process {\n    // Load the globally installed singularity/4.1.0-slurm module before running any process\n    module = 'singularity/4.1.0-slurm'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <ul> <li>Run your updated Nextflow command:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <pre><code>nextflow run main.nf -profile pbspro\n</code></pre> Output Output<pre><code> N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [big_picasso] DSL2 - revision: e34a5e5f9d\n\nexecutor &gt;  local (6)\n[c3/dda9c5] FASTQC (fastqc on NA12877) | 1 of 1 \u2714\n[b4/210425] ALIGN (1)                  | 1 of 1 \u2714\n[89/5b15d9] GENOTYPE (1)               | 1 of 1 \u2714\n[7d/a18133] JOINT_GENOTYPE (1)         | 1 of 1 \u2714\n[3f/1db7ee] STATS (1)                  | 1 of 1 \u2714\n[be/fd841e] MULTIQC                    | 1 of 1 \u2714\n</code></pre> <pre><code>nextflow run main.nf -profile slurm\n</code></pre> Output <pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [distracted_angela] DSL2 - revision: e34a5e5f9d\n\nexecutor &gt;  local (6)\n[bb/ddf5ff] FASTQC (fastqc on NA12877) | 1 of 1 \u2714\n[1f/b0906a] ALIGN (1)                  | 1 of 1 \u2714\n[34/f0234b] GENOTYPE (1)               | 1 of 1 \u2714\n[07/08074e] JOINT_GENOTYPE (1)         | 1 of 1 \u2714\n[e0/b78049] STATS (1)                  | 1 of 1 \u2714\n[27/35c181] MULTIQC                    | 1 of 1 \u2714\n</code></pre> <p>Your workflow should have run successfully, however, there is one grave mistake when running on the HPC - all processes were run on the login node! This is Nextflow's default behaviour when no executor is specified. </p>"},{"location":"part2/21_firstrun/#213-scheduling-jobs","title":"2.1.3 Scheduling jobs","text":"<p>Recall from Part 1 that Nextflow's executor is the part of the workflow engine that talks to the computing environment (whether it's a laptop or HPC). When running on a shared HPC system, these settings are important to include so they are queued properly on a compute node.</p> <p>To have processes run on the compute nodes, <code>executor</code> needs to be set to the appropriate job scheduler in our system-specific config files to avoid the default executor (<code>local</code>) being used. 'Local' executions means to run in the same environment as the head job, whether that be your local laptop or the login node of an HPC. </p> <p>Although Gadi and Setonix both run Nextflow with Singularity, each has different environment variables, filesystem layouts, job schedulers, queue structures, module names/versions, and container cache behaviour. These differences affect how Nextflow executes each process. To have Nextflow submit our processes as separate compute jobs, we need to instruct which job scheduler and queue/partition should be used. </p> <p>The job scheduler is specified with the Nextflow configuration option <code>executor</code> while the queue/partition to submit jobs to is specified by the <code>queue</code> option. </p> <p>Note</p> <p>In Part 2 we will use the same queues and partitions:</p> <ul> <li>The <code>normalbw</code> queue on Gadi</li> <li>The <code>work</code> partition on Pawsey</li> </ul> <p>These are low-cost queues suitable for general compute jobs.</p> <p>Let's add both the <code>executor</code> and <code>queue</code> configuration options to our system-specific configs to tell Nextflow where to run the processes. </p> <p>Exercise: Configure executor and queue</p> Gadi (PBS Pro)Setonix (Slurm) config/pbspro.config<pre><code>process {\n    // Load the globally installed singularity module before running any process\n    module = 'singularity'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'pbspro'\n    queue = 'normalbw'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> config/slurm.config<pre><code>process {\n    // Load the globally installed singularity/4.1.0-slurm module before running any process\n    module = 'singularity/4.1.0-slurm'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'slurm'\n    queue = 'work'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <p>Before we re-run our pipeline, we want to add a few more settings to ensure we are using the HPC responsibly. These include options so you don't overwhelm the system by submitting too many jobs or job queries at once (<code>queueSize</code>, <code>pollInterval</code>, <code>queueStatInterval</code>), and options to avoid generating duplicate files (<code>cache</code>, <code>stageInMode</code>). </p> <p>Note that for Setonix, we have specified a <code>reservation</code>. This enables us to use reserved resources on Setonix for the duration of the workshop. If you are running Nextflow on Setonix outside of the workshop, this option should be omitted.</p> <p>Exercise: Further config options</p> <ul> <li>Add the additional config options to your system-specific config file: </li> </ul> Gadi (PBS Pro)Setonix (Slurm) config/pbspro.config<pre><code>executor {\n    // For high-throughput jobs, these values should be higher\n    queueSize = 30\n    pollInterval = '5 sec'\n    queueStatInterval = '5 sec'\n}\n\nprocess {\n    // Load the globally installed singularity module before running any process\n    module = 'singularity'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'pbspro'\n    queue = 'normalbw'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    cache = 'lenient'\n    stageInMode = 'symlink'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <ul> <li>Save the file and run the pipeline:</li> </ul> <pre><code>nextflow run main.nf -profile pbspro\n</code></pre> Output Output<pre><code>executor &gt;  pbspro (3)\n[51/e95140] FASTQC (fastqc on NA12877) | 1 of 1 \u2714\n[bb/c54519] ALIGN (1)                  | 1 of 1 \u2714\n[93/a8ecf4] GENOTYPE (1)               | 1 of 1, failed: 1 \u2718\n[-        ] JOINT_GENOTYPE             -\n[-        ] STATS                      -\n[-        ] MULTIQC                    | 0 of 1\nERROR ~ Error executing process &gt; 'GENOTYPE (1)'\n\nCaused by:\n  Process `GENOTYPE (1)` terminated with an error exit status (247)\n\n\nCommand executed:\n\n  gatk --java-options \"-Xmx4g\" HaplotypeCaller -R Hg38.subsetchr20-22.fasta -I NA12877.bam -O NA12877.g.vcf.gz -ERC GVCF\n\nCommand exit status:\n  247\n\nCommand output:\n  (empty)\n\nCommand error:\n    ...\n\nWork dir:\n /scratch/vp91/xyz777/nextflow-on-hpc-materials/part2/work/93/a8ecf466a33f67f23eb0ca12b0b753\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\n-- Check '.nextflow.log' file for details\n</code></pre> config/slurm.config<pre><code>executor {\n    // For high-throughput jobs, these values should be higher\n    queueSize = 30\n    pollInterval = '5 sec'\n    queueStatInterval = '5 sec'\n}\n\nprocess {\n    // Load the globally installed singularity/4.1.0-slurm module before running any process\n    module = 'singularity/4.1.0-slurm'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'slurm'\n    queue = 'work'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservation=NextflowHPC\"\n    cache = 'lenient'\n    stageInMode = 'symlink'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> <ul> <li>Save the  file and run the pipeline:</li> </ul> <pre><code>nextflow run main.nf -profile slurm\n</code></pre> Output <pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [wise_venter] DSL2 - revision: e34a5e5f9d\n\nexecutor &gt;  slurm (6)\n[ec/2e1a61] FASTQ | 1 of 1 \u2714\n[6a/e63199] ALIGN | 1 of 1 \u2714\n[07/5dea67] GENOT | 1 of 1 \u2714\n[76/bac7dd] JOINT | 1 of 1 \u2714\n[1e/4c035e] STATS | 1 of 1 \u2714\n[f3/c78e59] MULTI | 1 of 1 \u2714\nCompleted at: 17-Nov-2025 12:41:33\nDuration    : 2m 46s\nCPU hours   : (a few seconds)\nSucceeded   : 6\n</code></pre> <p>Zoom react!</p> <p>If your job has finished succesfully, react \"Yes\" on Zoom, and \"No\" if it returned an error</p> <p>Let's explore what resources were actually used and compare them to what was allocated, by inspecting the logs and system job information using the methods from Part 1.</p> <p>We will use the respective job scheduler introspection tools to observe the resources used on both both systems.</p> <p>Exercise: Inspect resource usage</p> <ul> <li>Find the Job ID of the failed or completed <code>GENOTYPE</code> process in your <code>.nextflow.log</code>: </li> </ul> <pre><code>grep -w GENOTYPE .nextflow.log | grep jobId\n</code></pre> <p>The output should look something like: <pre><code>Nov-11 12:26:17.249 [Task submitter] DEBUG nextflow.executor.GridTaskHandler - [PBSPRO] submitted process GENOTYPE (1) &gt; jobId: 154278383.gadi-pbs; workDir: /scratch/ad78/fj9712/nextflow-on-hpc-materials/part2/work/0b/465f7eacfa500698687ff12df65060\nNov-11 12:27:47.144 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed &gt; TaskHandler[jobI d: 154278383.gadi-pbs; id: 5; name: GENOTYPE (1); status: COMPLETED; exit: 0; error: -; workDir: /scratch/a d78/fj9712/nextflow-on-hpc-materials/part2/work/0b/465f7eacfa500698687ff12df65060 started: 1762828007140; exited: 2025-11-11T02:27:35Z; ]\n</code></pre></p> <p>The value we need in this example is <code>154278383</code> - this corresponds to the job id that was scheduled.</p> <ul> <li>Then, use this job ID and the specific job introspection command for your HPC system to view the resource usage of the <code>GENOTYPE</code> process:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <p>Use <code>qstat -xf &lt;job_id&gt;</code> for a comprehensive summary of job allocation, environment variables, and resource usage:</p> <p><pre><code>qstat -xf &lt;job_id&gt;\n</code></pre> <pre><code>Job Id: 154038474.gadi-pbs\nJob_Name = nf-GENOTYPE_1\nJob_Owner = user@gadi-login-05.gadi.nci.org.au\nresources_used.cpupercent = 94\nresources_used.cput = 00:02:50\nresources_used.jobfs = 0b\nresources_used.mem = 512124kb\nresources_used.ncpus = 1\nresources_used.vmem = 512124kb\nresources_used.walltime = 00:03:03\njob_state = F\nqueue = normal-exec\n...\n</code></pre></p> <p>Use <code>seff</code> for a simple summary of job resource usage:</p> <p><pre><code>seff &lt;job_id&gt;\n</code></pre> <pre><code>Job ID: 34903205\nCluster: setonix\nUser/Group: cou057/cou057\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:00:19\nCPU Efficiency: 36.54% of 00:00:52 core-walltime\nJob Wall-clock time: 00:00:26\nMemory Utilized: 682.74 MB\nMemory Efficiency: 37.11% of 1.80 GB (920.00 MB/core)\n</code></pre></p> <p>These reporting tools show that the jobs were assigned the following resources:</p> CPU Memory Gadi 1 512 MB Setonix 2 1.8 GB <p>We have observed that the default of 1.8 GB RAM allocated to the Setonix <code>GENOTYPE</code> job was sufficient, but the default of 512 MB on Gadi was not.</p> <p>This is why explicit resource configuration is important. Even though the pipeline technically ran (or failed), these defaults are unsuitable for real data.</p> <p>Many Nextflow workflows provide test data that can be used to help get the pipleline running on your HPC. Researchers can be left confused when a successful test run is followed by a failed run on their own data. The test data is often small and requires minimal resources, while real data can be orders of magnitude larger and more complex. This discrepancy can lead to unexpected failures if the workflow is not properly configured for the actual data being processed. </p> <p>We will now go on to look at how we can create a custom configuration file to specify appropriate resources for our data and HPC environment.</p>"},{"location":"part2/21_firstrun/#214-custom-workflow-configuration-files","title":"2.1.4 Custom workflow configuration files","text":"<p>So far we have applied the default <code>nextflow.config</code> file that contains details required to run the workflow on any platform. We have then gone on to create and apply an infrastructure-specific configuration file (<code>config/pbspro.config</code> or <code>config/slurm.config</code>) to tell Nextflow how and where to run on a particular HPC system. Next we will create a third configuration file (<code>config/custom.config</code>) to specify resource requirements that are appropriate for our data and HPC environment.</p> <p></p> <p>At this point, you may be wondering \"Why do we have so many configuration files? We use three different configuration files to keep our Nextflow workflows reproducible, modular, and portable across different systems. </p> <p>This setup:</p> <ul> <li>Ensures consistency when running the same pipeline in different environments</li> <li>Allows re-use of configuration components across multiple pipelines</li> <li>Simplifies maintenance by isolating system-specific settings from workflow logic</li> </ul> <p>Each configuration file serves a distinct purpose:</p> <ul> <li> <p><code>nextflow.config</code> is the main configuration file that defines the core behaviour of the workflow itself (e.g. <code>main.nf</code>). It includes parameters (params), and references to profiles. To maintain reproducibility, this file should not be modified during system-specific tuning. It should only change if the underlying workflow logic changes - that is, what gets run.</p> </li> <li> <p><code>config/pbspro.config</code> and <code>config/slurm.config</code> define how the pipeline should run on a particular type of HPC system. These files specify details such as which executor to use (e.g. PBS Pro or Slurm), whether to use Singularity or Docker, and other runtime behaviour. They do not control the internal logic of the pipeline. These files should be tailored to match the requirements and setup of the HPC infrastructure you are targeting. Working on a new HPC? You'll need to make a new config file for it! But the good news is you can still use the same <code>nextflow.config</code> file.</p> </li> <li> <p><code>config/custom.config</code> is an additional system-specific customisation layer that defines process settings such as CPU and memory requests. When developing or adapting a custom pipeline for an HPC environment, this is typically where most tuning happens to fit the specific node architecture, queue constraints, and resource requirements of the data being processed. While these settings could be included within the same config that defines the executor and other system-specific settings, separating them into a distinct file allows for easier management and modification of resource allocations without altering the core system configuration. This modularity is especially useful when experimenting with different resource configurations during pipeline development and testing or when adapting the pipeline for different datasets with varying resource needs.</p> </li> </ul> <p>While this structure is a useful starting point, it is not the only way to structure your configuration. The nf-core community have their own set of standards with some presets for some instutitions (including Gadi and Setonix!). However, it is important to double check that these configs are suitable and optimal for your purposes. For more information see nf-core/configs.</p>"},{"location":"part2/21_firstrun/#215-assigning-default-resources","title":"2.1.5 Assigning default resources","text":"<p>Now that we have an appreciation of the layers of configuration for running Nextflow workflows on HPC, we will continue to get the pipeline running by specifying what resources should be run, instead of relying on the system-specific default. We want to ensure that:</p> <ul> <li>Jobs are being scheduled correctly</li> <li>All process tasks and the pipeline complete successfully</li> </ul> <p>Note that the resources assigned differ across sytems - these values are based on the average memory available per core and will be revisited in the upcoming lesson on resourcing.</p> <p>Exercise: Create custom resource config</p> <ul> <li> <p>Create a new file <code>config/custom.config</code></p> <pre><code>touch config/custom.config\n</code></pre> </li> <li> <p>Open the new empty file, and add the following contents based on your HPC</p> </li> </ul> Gadi (PBS Pro)Setonix (Slurm) custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6 OR 9.1\n    memory = 4.GB\n}\n</code></pre> custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n}\n</code></pre> <p>Since we have been repeatedly running the same Nextflow run command, it makes sense to save this into a run script. This can reduce human error (for example missing a flag, typos) and is easier to re-run. This is especially useful in the testing and benchmarking stages and can later be adapted into a job submission script if your HPC has a queue where you can run Nextflow head jobs</p> <p>Exercise: Create and use a run script</p> <ul> <li> <p>Create a new file called <code>run.sh</code></p> <pre><code>touch run.sh\n</code></pre> </li> <li> <p>Open the new empty file, and copy-paste the following code based on your HPC:</p> </li> </ul> Gadi (PBS Pro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run main.nf -profile pbspro -c config/custom.config\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run main.nf -profile slurm -c config/custom.config\n</code></pre> <ul> <li> <p>Save the file (Windows: Ctrl+S, macOS: Cmd+S)</p> </li> <li> <p>Provide execute permission by running:</p> <pre><code>chmod +x run.sh\n</code></pre> </li> <li> <p>Run the workflow using the new run script: </p> </li> </ul> <pre><code>./run.sh\n</code></pre> Results <p>On both Gadi and Setonix, the workflow should now be successful and executed end-to-end on the respective scheduler.</p> Gadi (PBS Pro)Setonix (Slurm) <pre><code>Loading nextflow/24.04.5\nLoading requirement: java/jdk-17.0.2\n\n N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [determined_picasso] DSL2 - revision: 5e5c4f57e0\n\nexecutor &gt;  pbspro (6)\n[7b/16f7c2] FASTQC (fastqc on NA12877) | 1 of 1 \u2714\n[ec/5fd924] ALIGN (1)                  | 1 of 1 \u2714\n[17/803fe5] GENOTYPE (1)               | 1 of 1 \u2714\n[0f/e21d58] JOINT_GENOTYPE (1)         | 1 of 1 \u2714\n[40/0c6e28] STATS (1)                  | 1 of 1 \u2714\n[6a/94910b] MULTIQC                    | 1 of 1 \u2714\nCompleted at: 10-Nov-2025 12:19:38\nDuration    : 4m 1s\nCPU hours   : (a few seconds)\nSucceeded   : 6\n</code></pre> <pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [nostalgic_bell] DSL2 - revision: 5e5c4f57e0\n\nexecutor &gt;  slurm (6)\n[a4/1eae6a] FASTQC (fastqc on NA12877) [100%] 1 of 1 \u2714\n[76/9e6fca] ALIGN (1)                  [100%] 1 of 1 \u2714\n[96/f57a2e] GENOTYPE (1)               [100%] 1 of 1 \u2714\n[ec/547a9c] JOINT_GENOTYPE (1)         [100%] 1 of 1 \u2714\n[88/b49a40] STATS (1)                  [100%] 1 of 1 \u2714\n[8d/b5b351] MULTIQC                    [100%] 1 of 1 \u2714\nCompleted at: 10-Nov-2025 10:28:08\nDuration    : 3m 31s\nCPU hours   : (a few seconds)\nSucceeded   : 6\n</code></pre>"},{"location":"part2/21_firstrun/#216-summary","title":"2.1.6 Summary","text":"<p>You\u2019ve now built the scaffolding needed to begin fine-tuning your resource requests and exploring monitoring and optimisation techniques. In the next section, we'll start measuring actual resource usage and configuring processes more precisely for efficient implementation on the underlying HPC system.</p>"},{"location":"part2/21_firstrun/#217-code-checkpoint","title":"2.1.7 Code checkpoint","text":"Show complete code Gadi (PBS Pro)Setonix (Slurm) config/pbspro.config<pre><code>executor {\n    // For high-throughput jobs, these values should be higher\n    queueSize = 30\n    pollInterval = '5 sec'\n    queueStatInterval = '5 sec'\n}\n\nprocess {\n    // Load the globally installed singularity module before running any process\n    module = 'singularity'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'pbspro'\n    queue = 'normalbw'\n    clusterOptions = \"-P ${System.getenv('PROJECT')}\"\n    storage = \"scratch/${System.getenv('PROJECT')}\"\n    cache = 'lenient'\n    stageInMode = 'symlink'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run main.nf -profile pbspro -c config/custom.config\n</code></pre> slurm.config<pre><code>executor {\n    // For high-throughput jobs, these values should be higher\n    queueSize = 30\n    pollInterval = '5 sec'\n    queueStatInterval = '5 sec'\n}\n\nprocess {\n    // Load the globally installed singularity/4.1.0-slurm module before running any process\n    module = 'singularity/4.1.0-slurm'\n    // Run using the pbspro scheduler on the 'normalbw' queue\n    executor = 'slurm'\n    queue = 'work'\n    clusterOptions = \"--account=${System.getenv('PAWSEY_PROJECT')} --reservations=NextflowHPC\"\n    cache = 'lenient'\n    stageInMode = 'symlink'\n}\n\nsingularity {\n    // Explicitly turns on container execution\n    enabled = true\n    // Automatically bind-mount working directory on scratch and common system paths\n    autoMounts = true\n    // Define location of stored container images \n    cacheDir = \"/scratch/${System.getenv('PAWSEY_PROJECT')}/${System.getenv('USER')}/nextflow-on-hpc-materials/singularity\"\n}\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run main.nf -profile slurm -c config/custom.config\n</code></pre>"},{"location":"part2/22_reporting/","title":"Pipeline monitoring and reporting","text":"<p>Learning objectives</p> <ul> <li>Apply Nextflow's monitoring features to understand process-level resource usage on HPC</li> <li>Analyse key resource metrics to assess workflow efficiency and identify processes that require tuning </li> <li>Use nextflow log and custom trace fields to locate, track, and debug workflow execution</li> </ul> <p>In bioinformatics workflows, resource requirements are often not fixed for a given analysis type or tool. They can vary greatly for many reasons, for exampleinout data size, genomic complexity, tissue expression profile, and species. This means we can't assume CPU, memory, or time values will work the same for every dataset or even every sample within a dataset. On HPC systems - where resources are shared and allocations may be finite and/or charged - these differences matter. </p> <p>For these reasons, it is not efficient or reliable to just \"set and forget\" resource values for an entire workflow: you need to build in flexibility. This requires you to have visiblity over pipeline behaviour at the process level. Nextflow provides several monitoring and reporting tools that help you understand this behaviour including the <code>nextflow log</code> command, trace files, and reports. </p> <p>Now that our workflow is running without error on the HPC compute nodes, we will enable Nextflow's reports. This allows us to view the resource usage of each process for our representative sample.</p>"},{"location":"part2/22_reporting/#221-execution-reports-and-timelines","title":"2.2.1 Execution reports and timelines","text":"<p>Nextflow can produce an execution report at the end of your workflow run that summarises all process execution metrics. Similarly, it can create an execution timeline. These reports summarise how your workflow ran, which processes were executed, and how long they took. They are very helpful during development, troubleshooting, and performance optimisation. </p> <p>These reports can be created when running the pipeline using the <code>-with-report</code> and <code>-with-timeline</code> flags, or by adding <code>timeline{}</code> and <code>report{}</code> directives to our configuration file. Inclusion within the configuration file has many advantages, providing a means of ensuring the reports are always generated when the pipeline is run, regardless of the execution command used, and allowing for further customisation.</p> <p>Both <code>-with-report</code> and <code>-with-timeline</code> allow us to specify a custom file name, and choose whether or not their output files can be overwritten each run. Rather than overwriting the same report file every time we run the pipeline, we will add a timestamp parameter that automatically labels each report with the exact date and time the workflow was launched to easily ensure over-write never occurs. This makes it easier to track multiple runs, especially when you are iterating quickly and comparing resource usage. </p> <p>Exercise: Enable execution and timeline reports</p> <p>We will enable execution and timeline reports in our <code>custom.config</code> file. </p> <ul> <li>Copy the following code and paste at the end of your <code>custom.config</code>:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) config/custom.config<pre><code>// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n</code></pre> config/custom.config<pre><code>// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n</code></pre> <ul> <li>Save the confug, and use your run script to run the workflow: </li> </ul> <pre><code>./run.sh\n</code></pre> <p>Once the job completes, you should have a new folder called <code>runInfo</code>, where your timeline and report files are saved.</p> <p>Questions</p> <ol> <li>In VScode, right click on the <code>report.html</code> file and download to your local computer</li> <li>Open the file in your local browser</li> </ol> <p>Which process had required the highest memory usage?</p> Answer <p><code>GENOTYPE</code> used 0.8-1.6 GB memory</p>"},{"location":"part2/22_reporting/#222-resource-reporting-with-nextflow-log","title":"2.2.2 Resource reporting with <code>nextflow log</code>","text":"<p>Recall we generated a trace file in lesson 1.8.3. For the runs we have already executed, which lacked the trace config option or command line flag, we can generate a trace report using the <code>nextflow log</code> command. This command is not identical to the trace file, but provides similar information about resource usage for each process in a tabular format. Since we don't have a trace file yet, we will first perform resource tracing using the <code>nextflow log</code> utility.</p> <p>Exercise: Review resource usage with nextflow log</p> <ul> <li>Run the <code>nextflow log</code> command with no arguments to view a summary of previous runs:</li> </ul> <pre><code>nextflow log\n</code></pre> Output TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2025-11-17 12:59:51 2m 24s naughty_bartik OK e34a5e5f9d 0ad50a9d-4e39-401e-ae46-65a1ee4e7933 ... 2025-11-17 13:07:26 2m 13s jolly_stonebraker OK e34a5e5f9d fecd0d8c-0b9d-4cd6-9409-874ab4b2f976 ... <p>This information is extracted from file saved in the work directory called <code>.nextflow.log</code>, <code>.nextflow.log.1</code> etc. These files are renamed each run from the same directory, so that latest run is always <code>.nextflow.log</code> and the highest-numbered log is the oldest. In order to perform resource tracing using this method, it is vital to not delete these logs!</p> <ul> <li>Include the <code>-list-fields</code> flag to view all of the available fields for this utility:</li> </ul> <pre><code>nextflow log -list-fields\n</code></pre> <p>That's quite a few! Just a handful of fields are shown by default, but there are &gt;40 that can be optionally displayed.</p> <ul> <li>Extract some specific fields for a recent run. Choose any fields you like, but we suggest focusing on those that provide relevant run info. Ensure to substitute <code>&lt;run name&gt;</code> in the command below with the actual run name from one of your runs. </li> </ul> <pre><code>nextflow log &lt;run name&gt; -f name,status,exit,realtime,cpus,pcpu,memory,pmem,rss\n</code></pre> <p>Question</p> <p>Which process had the highest <code>realtime</code>?</p> Answer <p><code>GENOTYPE</code>: ~ 30 seconds. </p> <p>We have now used the <code>nextflow log</code> utility to easily extract a snapshot of the resource usage for all of our processes, including the CPU and memory requested (<code>cpus</code>, <code>memory</code>), how much of it was utilised (<code>%cpu</code>, <code>%mem</code>), and how long it ran (<code>duration</code>, <code>walltime</code>).</p> <p>Next we will make this automated and reproducible by adding the <code>trace{}</code> directive to our configuration file. This will save all this information to a text file <code>runInfo/</code> to keep our launch directory neat.</p>"},{"location":"part2/22_reporting/#223-resource-reporting-with-trace-files","title":"2.2.3 Resource reporting with trace files","text":"<p>Trace reports can be customised to provide detailed records of each process executed within a pipeline. As we learnt earlier, this file is generated by adding the <code>-with-trace</code> flag to your execution command or including the <code>trace{}</code> directive in your custom configuration file. While the HTML reports we looked at earlier focus on visual summaries of the whole run, trace text files give you raw process-level data that you can slice, filter, and analyse as needed: perfect for working on the HPC! </p> <p>The Nextflow <code>trace</code> tool has a lot of functionality. Like <code>nextflow log</code>, not all fields are displayed by default. For a full list and description of trace fields available, users should refer to the Nextflow trace documentation. Note that the <code>log</code> and <code>trace</code> fields are not identical, but do have many shared fields. </p> <p>Next we will extract the same fields with trace as we did with <code>nextflow log</code>, but note the small difference in field names for percentage CPU usage (<code>%cpu</code> vs <code>pcpu</code>) and percentage memory usage (<code>%mem</code> vs <code>pmem</code>). As always in bioinformatics - read the docs! </p> <p>Exercise: Enable trace reporting</p> <ul> <li>Add the following to your <code>custom.config</code> file: </li> </ul> <pre><code>trace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss'\n}\n</code></pre> <ul> <li>Save the config, then run your workflow again: </li> </ul> <pre><code>./run.sh\n</code></pre> <ul> <li>View your trace file:</li> </ul> <pre><code>cat runInfo/trace-*.txt\n</code></pre> <p>You should see something like this:</p> Gadi (PBS Pro)Setonix (Slurm) name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) COMPLETED 0 29.6s 1s 1 93.7% 4 GB 0.0% 95.8 MB FASTQC (fastqc on NA12877) COMPLETED 0 34.5s 5s 1 76.2% 4 GB 0.1% 286.6 MB GENOTYPE (1) COMPLETED 0 59.9s 45s 1 97.6% 4 GB 0.5% 950.3 MB JOINT_GENOTYPE (1) COMPLETED 0 34.8s 16s 1 93.3% 4 GB 0.3% 508.8 MB STATS (1) COMPLETED 0 19.9s 0ms 1 73.4% 4 GB 0.0% 3.1 MB MULTIQC COMPLETED 0 29.9s 4.7s 1 79.5% 4 GB 0.0% 97.2 MB name status exit duration realtime cpus %cpu memory %mem peak_rss FASTQC (fastqc on NA12877) COMPLETED 0 13.8s 4s 1 135.0% 2 GB 0.1% 240.6 MB ALIGN (1) COMPLETED 0 13.8s 2s 1 100.1% 2 GB 0.0% 98.2 MB GENOTYPE (1) COMPLETED 0 39.9s 28s 1 164.8% 2 GB 0.5% 1.4 GB JOINT_GENOTYPE (1) COMPLETED 0 19.2s 8s 1 204.2% 2 GB 0.2% 466 MB STATS (1) COMPLETED 0 14.9s 1s 1 45.2% 2 GB 0.0% 2 MB MULTIQC COMPLETED 0 19.9s 5.3s 1 62.4% 2 GB 0.0% 78.6 MB"},{"location":"part2/22_reporting/#224-trace-file-enhancement","title":"2.2.4 Trace file enhancement","text":"<p>When things go wrong with process execution, we often need to trawl through the work directory to view <code>.command.err</code> files or view the job with <code>qstat</code> or <code>sacct</code> to debug the error. With trace, we can add two very useful fields that help us quickly locate the work directory and the scheduler job ID for each process.</p> <p>Exercise: Add work directory and job ID to trace file</p> <ul> <li> <p>View the documentation on trace fields, and locate the two fields that provide the:</p> <ul> <li><code>work/</code> directory path where the task was executed</li> <li>job ID executed by a grid engine (i.e. the scheduler)</li> </ul> </li> <li> <p>Append these two field names to <code>trace.fields</code> in <code>custom.config</code></p> </li> </ul> Hint <pre><code>trace {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,duration,realtime,cpus,%cpu,memory,%mem,peak_rss,workdir,native_id'\n}\n</code></pre> <ul> <li>Add the Nextflow <code>-resume</code> flag to your run script to avoid re-running completed processes:</li> </ul> Hint <pre><code>nextflow run main.nf -profile slurm -c config/custom.config -resume\n</code></pre> <ul> <li>Save the run script config and run the workflow:</li> </ul> <pre><code>./run.sh\n</code></pre> <ul> <li>View the newly generated trace file inside the <code>runInfo/</code> folder</li> </ul> <pre><code>cat runInfo/trace-&lt;newest-timestamp&gt;.txt\n</code></pre> <ul> <li>Now view the previously generated trace file:</li> </ul> <pre><code>cat runInfo/trace-&lt;newest-timestamp&gt;.txt\n</code></pre> <p>Note that despite running with <code>-resume</code>, the trace file is still generated afresh for the entire run, including full resource usage details as if the process was executed again. This is very handy for benchmarking and tracking resource usage over multiple runs, as the full workflow details are always included in the latest trace regardless of whether a task was cached or re-executed.</p> <p>It is up to you how you want to configure your traces for your own pipelines and how much added information you require. The suggestions we have demonstrated in this lesson are a good starting point for most bioinformatics workflows.</p>"},{"location":"part2/22_reporting/#225-summary","title":"2.2.5 Summary","text":"<p>In this section, we have enabled Nextflow's execution report and timeline features to generate HTML summary reports, explored how to extract custom information from the <code>nextflow log</code> command, and configured customised trace file reporting. </p> <p>These powerful profiling features can help you benchmark and debug your runs, monitor resource usage, and plan for upcoming compute needs. Applying these tools when you set up your own pipelines will help you build efficient and reliable workflows that make the best use of your HPC resources.</p>"},{"location":"part2/22_reporting/#226-code-checkpoint","title":"2.2.6 Code checkpoint","text":"Show complete code Gadi (PBS Pro)Setonix (Slurm) config/custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6 OR 9.1\n    memory = 4.GB\n}\n\n// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n\ntrace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss,workdir,native_id'\n}\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run main.nf -profile pbspro -c config/custom.config -resume\n</code></pre> config/custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n}\n\n// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n\ntrace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss,workdir,native_id'\n}\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run main.nf -profile slurm -c config/custom.config -resume\n</code></pre>"},{"location":"part2/23_strategies/","title":"Optimising Nextflow for HPC","text":"<p>Learning objectives</p> <ul> <li>Identify the key factors that impact workflow performance on HPC systems</li> <li>Describe common workflow optimisation strategies in the context of Nextflow</li> </ul> <p>Our workflow is now functional: it runs successfully with scheduler-specific settings on HPC and outputs useful trace files showing resource usage. In Lesson 1.4, we learnt about the importance of appropriate resource requests on HPC and the value of running an efficient and optimised workflow. We will now begin to apply these principles to our custom workflow. </p>"},{"location":"part2/23_strategies/#231-why-optimise","title":"2.3.1 Why optimise?","text":"<p>Workflow optimisation involves fine-tuning your workflow to make it more efficient. It can be used to reduce runtime, facilitate higher throughput and larger more powerful studies, avoid idle hardware that could be freed up for other researchers, and decrease computing cost. Many HPCs charge use per core hour, yet even for those where the cost is not billed to the researcher, the cost of the resources must be covered by someone, and this may be the Faculty/School, institution, funding body, or federal budget. Demonstrating efficient use of compute funds can be important in grant success for projects requiring substantial compute. </p> <p>Another important reason optimisation is worthwhile is the impact on the environment. Bioinformatics data processing on HPCs has a significant carbon footprint due to the energy required to run our computations. Making our workflows complete in a faster time using less hardware contributes to sustaintable computing practices. </p> <p>This short editorial from Nature Computational Science highlights the challenges research computing faces in the context of the climate crisis.</p> <p>Today we will apply workflow optimisation from two perspectives:</p> <ul> <li>Resource efficiency: benchmarking and adjusting resources for efficient process execution on HPC</li> <li>Speed-up: introducing code changes to make the workflow run faster without costing more compute hours</li> </ul>"},{"location":"part2/23_strategies/#232-when-to-optimise","title":"2.3.2 When to optimise","text":"<p>A small workflow run a handful of times might not benefit dramatically from optimisation. Many Nextflow workflows that employ good practices (e.g. nf-core) will often run \"out of the box\" on HPC with default resources, but defaults might not always fit your data and therefore the behaviour of your processes, or the constraints of your cluster. Think back to Part 1 and the configuration customisations we implemented for our nf-core workflow. </p> <p>Optimising workflows on HPC becomes especially important when:</p> <ul> <li>You are analysing large datasets or many samples </li> <li>You will execute your pipeline repeatedly </li> <li>You are operating on a system that uses a service unit or time-limited allocation model</li> <li>Your processes have data-dependent resource usage </li> </ul> <p>Tip</p> <p>The ideal time to optimise a workflow is while it is being developed. This is often more simple to achieve than back-tracking and adding improvements to an existing codebase. Notably, this may add development time to producing a finished workflow, but the efforts are rewarded with a resilient and scalable workflow that will reduce the time interval between data acquisition and final results for the lifetime of its use.</p> <p>If you have an existing workflow that is reliable but inefficient, there is always value in taking the time to optimise your regularly used workflow. This endeavour also provides the opportunity to update tool versions and introduce other enhancements such as the use of Singularity containers.</p> <p>When optimising a workflow, resource efficiency should always be considered. Optimisation through parallelisation however is not always possible or recommended. It is vital to always consider what components of work can be parallelised or split up in a biologically valid way. If you are unsure, one means of testing is to run the analysis with and without parallelism on a subset of data to observe any impact on results. Due to heuristics of tool algorithms, in many cases a small amount of difference in the final result is valid and tolerated. In other cases, it may be expected that a tool produces identical results irrespective of multi-threading or parallelisation. It is important to check the tool documentation, consider the nature of the data and underlying biology, test the effects of parallelisation, and only apply parallelisation when it makes biological sense to do so. </p> <p>Consider the example of sequenced DNA fragments in whole genome sequencing. Each fragment is completely independent on every other fragment in the library, so it can be aligned to the reference sequence independently without affecting the mapping results. This is a perfect case of embarrassingly parallel processing, i.e. running the same analysis numerous times on slightly different input data. In reality, we would not align one read at a time as the overhead of submitting millions of tiny jobs would overload the scheduler, impact performance, and likely result in stern reproach from the HPC system administrators! So in this case, we need to balance parallelisation with sound HPC practices. </p> <p>When embarassingly parallel processing with scatter-gather through data chunking or interval chunking (e.g. operating over different intervals of the data as separate parallel tasks) is not biologically valid, we can still apply parallelisation by sample. Parallel by sample is logical for numerous common bioinformatics processing tasks, and is typically only invalid when all samples must be analysed together, for example when collating final results. Fortunately, Nextflow makes parallel by sample simple to apply, making workflows expertly scalable.  </p>"},{"location":"part2/23_strategies/#233-what-affects-performance","title":"2.3.3 What affects performance?","text":"<p>Efficiency of any workflow on HPC depends on the interaction of three factors: </p>"},{"location":"part2/23_strategies/#2331-your-hpc-system","title":"2.3.3.1 Your HPC system","text":"<p>We have already witnessed many differences between Gadi and Setonix in previous lessons. A workflow that performs well on one cluster may perform poorly on another simply because the underlying architecture and scheduler rules differ. </p> <p>Good optimisation respects the boundaries of the system you're working on. When planning an optimisation approach, consider:</p> HPC characteristic What it means Why it matters for optimisation Default scheduler behaviour Policies set by administrators: fair-share, job priorities, backfill rules, default limits Affects queue wait time, job placement efficiency, and how many tasks can run in parallel Queue limits Maximum walltime, cores, and memory allowed per queue or partition Determines which queues you can use, how large each job can be, and whether your workflow gets delayed Node architecture Hardware layout: cores per node, memory per node, CPU type (Intel/AMD), GPUs, local scratch Ensures you request resources that \u201cfit\u201d the node, avoid resource fragmentation, and maximise throughput Charging model How HPC usage is accounted (CPU proportion, memory proportion, or the maximum of both) Guides you to request only what you need: over requesting directly increases SU consumption without improving runtime"},{"location":"part2/23_strategies/#2332-the-characteristics-of-your-data","title":"2.3.3.2 The characteristics of your data","text":"<p>Data shapes the computational behaviour of bioinformatics workflows. Even two workflows with identical code can perform very differently depending on the file sizes, sample numbers, and data complexity. Understanding these factors can help you anticipate bottlenecks and assign resources more accurately. When planning an optimisation approach, consider: </p> Data characteristic What it means Why it matters for optimisation File size The total size of FASTQ, BAM/CRAM, reference genomes, annotation files, or intermediate outputs Larger files increase memory requirements, disk I/O, runtime, and queue time; they also influence whether single-threading or multi-threading is more efficient Sample number Total number of samples in the analysis, including replicates or cohorts More samples \u2192 more processes \u2192 heavier scheduler load; the workflow may require scatter\u2013gather to parallelise effectively and avoid bottlenecks Data heterogeneity Variability in file sizes, read depth, sample complexity, or quality across inputs Highly variable samples produce uneven resource usage; some processes may require per-sample resource overrides to prevent memory kills or slowdowns Data type Whether data are short reads, long reads, single-cell, imaging derivatives, matrices, VCFs, etc. Different data modalities have different computational profiles (I/O-heavy, CPU-heavy, memory-heavy); optimisation strategies should account for the modality\u2019s behaviour I/O intensity Frequency and volume of read/write operations (large temporary files, sort steps, indexing, BAM \u2194 FASTQ conversions) I/O-heavy processes benefit from local SSD or node-local scratch; misconfigured I/O can add hours to runtime on shared filesystems Parallelisability Whether samples or chunks of data can be processed independently Determines when scatter\u2013gather is useful, how many jobs can run concurrently, and how well the workflow scales on HPC Compression and indexing formats gzip vs bgzip, BAM vs CRAM, presence of .bai/.crai/.fai, CCS vs raw reads Impacts CPU time, memory, and I/O behaviour; inefficient formats slow down the entire workflow"},{"location":"part2/23_strategies/#2333-the-structure-of-your-workflow","title":"2.3.3.3 The structure of your workflow","text":"<p>Even with the same tools and data, two workflows can behave differently depending on their structure: </p> <ul> <li>Number of processes </li> <li>Order of dependencies </li> <li>Opportunities for parallelism</li> <li>Whether steps are CPU-bound, memory-bound, or I/O bound </li> <li>Incorporated tools ability to multi-thread </li> </ul>"},{"location":"part2/23_strategies/#234-what-will-we-optimise-today","title":"2.3.4 What will we optimise today?","text":"<p>For the remainder of Part 2 we will apply the strategies introduced from Part 1 to optimise then scale our custom workflow. </p> <p>In the next section, we will assign appropriate resources for each process by using trace files to fine-tune <code>cpus</code>, <code>memory</code>, and <code>time</code> and align these to the resources on the compute nodes of our HPCs. </p> <p>In Lesson 2.5, we will introduce parallel processing firstly by enabling multi-threading in a thread-aware tool (BWA), and then by coding scatter-gather parallelism into the workflow. </p> <p>In today's example workflow, we will be applying scatter-gather to run alignment with BWA. Note that the <code>GENOTYPE</code> process can also be parallelised in a biologically valid way using a parallelisation strategy known as \"interval chunking\", but for simplicity we will not be optimising that process today. </p> <p>Finally, in Lesson 2.6, we will scale to multiple samples. This will consolidate all of the resource optimisations and parallelisation strategies (multi-threading; scatter-gather; parallel by sample) that we have built up today into one efficient, end-to-end run optimised for our respective HPCs. </p>"},{"location":"part2/24_resourcing/","title":"Assigning process resources","text":"<p>Learning objectives</p> <ul> <li>Analyse process-level resource usage from Nextflow trace data to determine appropriate resource requests</li> <li>Apply resource-aware configuration strategies</li> <li>Configure processes to correctly utilise allocated resources by passing values into process script blocks.</li> </ul>"},{"location":"part2/24_resourcing/#241-resource-efficiency-on-hpc","title":"2.4.1 Resource efficiency on HPC","text":"<p>When running Nextflow workflows on HPC, the way you configure your process resources (cores, memory, walltime) can make a big difference to how efficiently your jobs run, how long they sit in the queue, and how many service units/resource hours the job costs.</p> <p>A common assumption is that requesting fewer resources is safer, more considerate, or will result in less compute cost. In practice, this is often not the case. Jobs that are under-resourced may run for much longer than they need to, or fail partway through. This can waste time and also lead to wasted service units as the job requires resubmission.</p> <p>In many cases, using more cores and memory for a shorter time can be more efficient and cost-effective. This principle applies broadly across HPC systems, but of course applies only to tools and analysis tasks which can make use of additional cores and memory. Once again, in bioinformatics, there is no substitute for reading the docs!</p> <p>The scheduler also plays a role here. HPC schedulers are tuned to fit jobs into available space to maximise throughput and minimise idle cores on a node. If your job is \"small and short\" (i.e. fewer cores/memory for shorter time), it might slide into a gap on a partially subscribed compute node, but if it is \"wide and tall\" (i.e. more cores/memory for longer time) it may need to wait longer for available resources to become free. Some \"wide and tall\" bioinformatics tasks, such as read alignment, can be made \"small and short\" to take advantage of this feature of HPC, and we will apply this in Lesson 2.5. </p> <p>Tip</p> <p>You can think of job scheduling like finding a table at a busy caf\u00e9. If you\u2019re alone or in a small group, it\u2019s easier to get seated quickly, but if you camp out all day at a large table and only order a coffee, you\u2019re not making good use of the space. The waiter would also preferentially give that table to the party of 8 queued behind you, requiring you to wait until peak time had passed if you insisted on that large table to yourself. The HPC scheduler works in a similar way: it tries to fill in available \u201cseats\u201d with jobs that fit neatly and will clear out efficiently.</p> <p>In general, jobs that request just enough resources to run efficiently and cleanly tend to move through the system faster and cause fewer problems for other users. In order to deduce what \"just enough\" looks like for your workflow requires benchmarking and resource usage monitoring, and the Nextflow trace files can help us achieve that. </p> <p>In this section, we\u2019ll explore how to make smart decisions about resource configuration such as:</p> <ul> <li>How many resources your process actually needs</li> <li>How to estimate the average amount of usable memory per core on a node</li> </ul> <p>These adjustments don\u2019t just benefit your own workflow, they make better use of shared infrastructure.</p>"},{"location":"part2/24_resourcing/#242-configuring-processes","title":"2.4.2 Configuring processes","text":"<p>To begin tuning our workflow, we first need to understand how many resources each process actually used. We will use the trace summaries generated in the previous lesson as a baseline for how much time, how many cores, and how much memory each process used. </p> Gadi (PBS Pro)Setonix (Slurm) name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) COMPLETED 0 29.6s 1s 1 93.7% 4 GB 0.0% 95.8 MB FASTQC (fastqc on NA12877) COMPLETED 0 34.5s 5s 1 76.2% 4 GB 0.1% 286.6 MB GENOTYPE (1) COMPLETED 0 59.9s 45s 1 97.6% 4 GB 0.5% 950.3 MB JOINT_GENOTYPE (1) COMPLETED 0 34.8s 16s 1 93.3% 4 GB 0.3% 508.8 MB STATS (1) COMPLETED 0 19.9s 0ms 1 73.4% 4 GB 0.0% 3.1 MB MULTIQC COMPLETED 0 29.9s 4.7s 1 79.5% 4 GB 0.0% 97.2 MB name status exit duration realtime cpus %cpu memory %mem peak_rss FASTQC (fastqc on NA12877) COMPLETED 0 13.8s 4s 1 90.6% 2 GB 0.1% 240.6 MB ALIGN (1) COMPLETED 0 13.8s 2s 1 100.1% 2 GB 0.0% 98.2 MB GENOTYPE (1) COMPLETED 0 39.9s 28s 1 164.8% 2 GB 0.5% 1.4 GB JOINT_GENOTYPE (1) COMPLETED 0 19.2s 8s 1 204.2% 2 GB 0.2% 466 MB STATS (1) COMPLETED 0 14.9s 1s 1 45.2% 2 GB 0.0% 2 MB MULTIQC COMPLETED 0 19.9s 5.3s 1 62.4% 2 GB 0.0% 78.6 MB <p>While we could configure each process to match these values, we\u2019re instead going to take a broader view. We'll explore how HPC systems allocate memory per CPU, and how to align our process requests to match this architecture more effectively.</p> <p>This approach lets us make the most of the available resources - sometimes even getting \"extra\" memory at no additional cost and still remain scheduled quickly.</p>"},{"location":"part2/24_resourcing/#243-effective-memory-per-core","title":"2.4.3 Effective memory per core","text":"<p>Recall that this is the configuration we used in Part 2.1 to get the pipeline running on the respective HPC systems:</p> Gadi (PBS Pro)Setonix (Slurm) custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6\n    memory = 4.GB\n}\n</code></pre> custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n}\n</code></pre> <p>Thee values were chosen according to the average amount of memory available to each core on the compute node. As illustrated by the simple schematic below, compute nodes are made up of a number of CPUs each with a number of cores and amount of attached RAM. </p> <p></p> <p>Most HPC systems allocate jobs to nodes based on both core and memory requests. This is critical to ensure that the node is not over-subscribed, and for fair distribution of RAM among the cores on the node. Fair distribution is simply an average, so the total amount of RAM on the node divided by the number of cores, giving the average memory per core that a user can request without incurring additional job costs. </p> <p>Avoiding additional job costs motivates HPC users to consider this average when resourcing their jobs, which in turn means the scheduler can place more jobs per node, facilitating higher workload throughout and enhanced overall system utilisation. </p> <p>Both Gadi and Setonix apply the average amount of memory per core to the calculation of job cost. If your job requests more memory per core than the average memory per core, it will be charged based on the memory usage, and not the number of cores requested. </p>"},{"location":"part2/24_resourcing/#244-exploring-resource-options-for-fastqc","title":"2.4.4 Exploring resource options for FASTQC","text":"<p>Recall this bar chart from Lesson 1.4, where we observed that BWA executes faster when assigned more cores but FastQC walltime remains constant. </p> <p></p> <p>Given that our samples have two input files each (paired-end reads, R1 and R2), and we know that the FastQC <code>-threads N</code> parameter can process N files at once, we should request 2 cores for the <code>FASTQC</code> process. According to the trace file, it does not require much memory, so the limiting resource here is cores (the initial runs requested 1 core). </p> <p>Let's find the effective usable memory per core for the <code>normalbw</code> queue on Gadi, and the <code>work</code> partition on Setonix.</p> <p>Note: Some queues and partitions may have different nodes with different hardware specifications, such as nodes with more or less memory. Gadi's <code>normalbw</code> queue is one such case, where the queue has some nodes with 128 GB RAM and some with 256 GB. Using the effective memory per core to fit the smaller node has the benefit of running on either sized node, whereas requesting to fit only the 256 GB node may queue for longer. Typically however, a queue/partition will be made up of a number of identical compute nodes, and the consideration will be which queue matches your resource needs, and not which value to use when calculating the average memory per core. </p> <p>Exercise: Match task memory request with node hardware</p> <p>How much memory should you allocate to a process that requests <code>cpus = 2</code>?</p> <p>Steps:</p> <ul> <li>Refer to the HPC documentation and find the relevant information on the queue/partition resources  </li> <li>Divide the usable RAM on the queue/partition by the number of cores</li> <li>Multiply that value with the number of cores requested by the process</li> <li>Round up/down if required</li> </ul> Gadi (PBS Pro)Setonix (Slurm) <p>Review Queue Limits for <code>normalbw</code> queue.</p> Answer <ul> <li>128 GB/28 cores ~ 4.6 GB per CPU</li> <li>4.6 GB x 2 cores requested = 9.2 GB</li> <li>9 GB memory</li> </ul> <p>OR</p> <ul> <li>18 GB to fit 256 GB nodes</li> </ul> <p>Review partition resources for <code>work</code> partition</p> Answer <ul> <li>230 GB/ 128 cores ~ 1.8 GB per core</li> <li>1.8 GB x 2 cores requested = 3.6 GB</li> <li>4 GB memory</li> </ul> <p>Now that we\u2019ve calculated the effective memory per core on each system and determined what our memory request should be based on maximising resources available to the job without incurring additional job costs, the next consideration is - how much should we actually request for a small, fast process like <code>FASTQC</code> on our small test data?</p> <p>Let\u2019s look at the trade-offs.</p> <p>If we request:</p> Gadi (PBS Pro)Setonix (Slurm) <ul> <li>2 CPUs and 8 GB memory (on the Gadi <code>normalbw</code> queue), this takes advantage of all the memory you\u2019re entitled to, but <code>FASTQC</code> won't actually use that memory for input data of this size. So you're not getting any extra performance and may lengthen the time in queue.</li> <li>2 CPUs and 1 GB memory, on the other hand, still gives <code>FASTQC</code> enough to run, and because you're requesting less RAM, your job may be scheduled faster alongside other jobs that request more than the available memory per core on that node. This is more memory efficient too.</li> </ul> <ul> <li>2 CPUs and 4 GB memory (on the Setonix <code>work</code> partition), this takes advantage of all the memory you\u2019re entitled to, but <code>FASTQC</code> won't actually use that memory. So you're not getting any extra performance and may lengthen the time in queue.</li> <li>2 CPUs and 1 GB memory, on the other hand, still gives <code>FASTQC</code> enough to run, and because you're requesting less RAM, your job may be scheduled faster alongside other jobs that request more than the available memory per core on that node. This is more memory efficient too.</li> </ul> <p>We will proceed by requesting 2 cores and 1 GB memory for <code>FASTQC</code> as the job won't benefit from the extra memory.</p>"},{"location":"part2/24_resourcing/#245-configuring-with-process-names-withname","title":"2.4.5 Configuring with process names (<code>withName</code>)","text":"<p>We have learnt that adding custom workflow configurations into config files rather than into module code aids portability and ease of maintenance by keeping  workflow logic and system-specific configuration separate.</p> <p>We have also learnt that multiple configuration files can be applied at once in order to tailor a run, with workflow-specific configurations required for minimal execution of the workflow, system-specific configurations to get it running on a specific HPC, and further custom configurations tailored to our requirements.   </p> <p>Question</p> <p>Into which of our 3 configuration files do you think we should add the cores and memory requests for the <code>FASTQC</code> process? </p> Show answer <p>If you answered \"any\", you are correct in your understanding that adding the resource requests to any of our 3 configs would lead to a successful run. But since we have chosen our memory value specifically for Gadi|Setonix, this is a system-specific configuration. While specific to the HPC, it is also specific to our unique run of the data - these resource values would not be suitable to a colleague who wanted to run the workflow over samples with more than one pair of fastq files each of much larger size. Since the resources are both system specific and use-case specific they should ideally be specified within the <code>config/custom.config</code> file. </p> <p>The next part of the puzzle is understanding how we can assign these resources for the <code>FASTQC</code> process. Our custom config currently assigns the same resources to all processes in the workflow, under the <code>process</code> scope. To specify which process or processes to apply a set of resources to, we can use the Nextflow <code>withName</code> directive. </p> <p><code>withName</code> is a flexible tool that can be used to:</p> <ul> <li>Specifically target individual modules by their process name</li> <li>Specify multiple module names using wildcards (<code>*</code> or <code>|</code>)</li> <li>Avoid editing the module.nf file to add a process label (remember: separation of workflow logic and system-specific tuning)</li> <li>Has a higher priority than the counterpart tool <code>withLabel</code></li> </ul> <p>Configuring <code>withLabel</code> and configuration priorities</p> <p>Processes that require similar resources can be configured using the <code>withLabel</code> process selector. Processes can be tagged with multiple labels to flexibly categorise different runtime needs (e.g. high memory). However, <code>withLabel</code> has a lower priority than <code>withName</code>, making <code>withName</code> more specific. </p> <p>For more information, see custom resource configuration using process labels.</p> <p>Looking at a summary of observations from our trace file, we can see that some of our processes have very similar resource usage metrics and could be grouped together:  </p> Process Resources Rationale FASTQC 2 cores, 1 GB 2 cores to process R1 and R2, memory sufficient ALIGN, GENOTYPE, JOINT_GENOTYPE 2 cores, 1 GB High CPU utilisation &gt;90% GENOTYPE 2 cores, 2 GB High CPU utilisation &gt; 90%, requires 1.5GB memory STATS, MULTIQC 1 core, 2 GB Defaults are suitable <p>Let's record these in our config, grouping some of the processes under the same set of resources. </p> <p>Exercise: Use <code>withName</code> to configure process resources</p> <ul> <li>Update the <code>process{}</code> scopes of your <code>./config/custom.config</code>:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) config/custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6\n    memory = 4.GB\n\n    withName: /FASTQC|ALIGN|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n}\n</code></pre> config/custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n\n    withName: /FASTQC|ALIGN|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n}\n</code></pre> <ul> <li>Save the config file, and run:</li> </ul> <pre><code>./run.sh\n</code></pre> <p>Review the new trace file. Did the resource usage of the <code>FASTQC</code> process change?  </p> Example trace files Gadi (PBS Pro)Setonix (Slurm) name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) COMPLETED 0 24.5s 1s 2 108.5% 1 GB 0.1% 102.3 MB FASTQC (fastqc on NA12877) COMPLETED 0 29.6s 5s 2 93.8% 1 GB 0.1% 194.4 MB GENOTYPE (1) COMPLETED 0 1m 15s 35s 2 131.1% 2 GB 0.8% 1.4 GB JOINT_GENOTYPE (1) COMPLETED 0 34.9s 7s 2 169.1% 1 GB 0.3% 506 MB STATS (1) COMPLETED 0 44.9s 0ms 1 72.2% 2 GB 0.0% 3 MB MULTIQC COMPLETED 0 44.9s 4.5s 1 66.7% 2 GB 0.0% 88.6 MB name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) COMPLETED 0 14.1s 1s 2 139.3% 1 GB 0.0% 2 MB FASTQC (fastqc on NA12877) COMPLETED 0 19.6s 4s 2 90.6% 1 GB 0.1% 226.4 MB GENOTYPE (1) COMPLETED 0 44.8s 28s 2 164.0% 2 GB 0.5% 1.3 GB JOINT_GENOTYPE (1) COMPLETED 0 19.4s 9s 2 211.3% 1 GB 0.1% 343.3 MB STATS (1) COMPLETED 0 14.5s 0ms 1 132.7% 2 GB 0.0% 2 MB MULTIQC COMPLETED 0 19.9s 4.6s 1 76.1% 2 GB 0.0% 98.3 MB <p>The <code>cpus</code> field indicates that 2 cores were provided for our <code>FASTQC</code> task to use, however the <code>%cpu</code> field shows only ~90% utilisation. Note that in this context, we expect to see 100% per requested core for perfect utilisation, i.e. 100% = 1 core fully utilised, 200% = 2 cores fully utilised, and so on.</p> <p>If you observe a lower <code>%cpu</code> value than you expect, there could be two main culprits: either the tool cannot make use of all the cores that were allocated to it, or your workflow code is missing some required additional configuration. </p> <p>Which do you think is the likely cause in this example? We will explore this in the next section. </p>"},{"location":"part2/24_resourcing/#246-passing-allocated-resources-into-process-scripts","title":"2.4.6 Passing allocated resources into process scripts","text":"<p>We know how to instruct Nextflow to request specific resources for each process, but how does the tool inside the process know how to use those allocated resources? This does not happen automatically - our module code must obtain these values, and do so in a portable and flexible way. </p> <p>If our module code does not specifically instruct the tool to use the allocated resources, many tools will use defaults. If the defaults are less than what we have requested from the scheduler, we have wasted resources and run an inefficient workflow. If the defaults are higher than we have requested, our job may be killed by the scheduler due to exceeding available resources, causing the whole workflow execution to prematurely terminate. Some tools intelligently detect and use only the resources visible to the job, but this is not the norm in bioinformatics and should not be relied upon. </p> <p>Let's inspect our <code>FASTQC</code> module code:</p> <p>Exercise: Review thread use in <code>./modules/fastqc.nf</code></p> <pre><code>cat modules/fastqc.nf\n</code></pre> Show code modules/fastqc.nf<pre><code>process FASTQC {\n\n    tag \"fastqc on ${sample_id}\"\n    container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads_1), path(reads_2)\n\n    output:\n    path \"fastqc_${sample_id}\", emit: qc_out\n\n    script:\n    \"\"\"\n    mkdir -p \"fastqc_${sample_id}\"\n    fastqc -t 1 --outdir \"fastqc_${sample_id}\" --format fastq $reads_1 $reads_2\n    \"\"\"\n}\n</code></pre> <p>Note in the highlighted line the syntax <code>fastqc -t 1</code>. Here we are asking <code>fastqc</code> to use only a single thread, which means the 2 input files are processed in series (i.e. one at a time, each using a single core) instead of our intention of both files being processed at once to speed up run time. No matter how many cores we allocate to this process in our custom config, without adjusting this module code, the <code>FASTQC</code> process will always use a single core and process one file at once. </p> <p>Hardcoding is generally bad practice, and should be actively avoided in Nextflow workflows. Even for tools that mandate a specific amount of memory or cores, these values should still not be hard-coded, as these values may change in the future.</p> <p>We will next instruct our <code>FASTQC</code> process to take in the number of cores we provide it dynamically. This means whatever <code>cpus</code> we provide will be parsed to the process and applied as a value to the threads parameter (<code>-t</code>|<code>threads</code>). </p> <p>Exercise: Dynamically assigning cores to <code>modules/fastqc.nf</code></p> <ul> <li>Open your <code>modules/fastqc.nf</code> file. Replace <code>-t 1</code> with <code>-t ${task.cpus}</code>.</li> </ul> Show code modules/fastqc.nf<pre><code>process FASTQC {\n\n    tag \"fastqc on ${sample_id}\"\n    container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads_1), path(reads_2)\n\n    output:\n    path \"fastqc_${sample_id}\", emit: qc_out\n\n    script:\n    \"\"\"\n    mkdir -p \"fastqc_${sample_id}\"\n    fastqc -t ${task.cpus} --outdir \"fastqc_${sample_id}\" --format fastq $reads_1 $reads_2\n    \"\"\"\n\n}\n</code></pre> <ul> <li> <p>Save the script and run your workflow:</p> <pre><code>./run.sh\n</code></pre> </li> <li> <p>View the trace and note the <code>cpus</code> and <code>%cpu</code> values for <code>FASTQC</code>: </p> </li> </ul> Show trace Gadi (PBS Pro)Setonix (Slurm) name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) CACHED 0 24.5s 1s 2 108.5% 1 GB 0.1% 102.3 MB GENOTYPE (1) CACHED 0 1m 15s 35s 2 131.1% 2 GB 0.8% 1.4 GB JOINT_GENOTYPE (1) CACHED 0 34.9s 7s 2 169.1% 1 GB 0.3% 506 MB STATS (1) CACHED 0 44.9s 0ms 1 72.2% 2 GB 0.0% 3 MB FASTQC (fastqc on NA12877) COMPLETED 0 44.5s 4s 2 120.4% 1 GB 0.1% 163.1 MB MULTIQC COMPLETED 0 44.9s 3.7s 1 88.9% 2 GB 0.0% 93.6 MB name status exit duration realtime cpus %cpu memory %mem peak_rss ALIGN (1) CACHED 0 14.1s 1s 2 139.3% 1 GB 0.0% 2 MB GENOTYPE (1) CACHED 0 44.8s 28s 2 164.0% 2 GB 0.5% 1.3 GB JOINT_GENOTYPE (1) CACHED 0 19.4s 9s 2 211.3% 1 GB 0.1% 343.3 MB STATS (1) CACHED 0 14.5s 0ms 1 132.7% 2 GB 0.0% 2 MB FASTQC (fastqc on NA12877) COMPLETED 0 14.9s 3s 2 196.9% 1 GB 0.1% 236.7 MB MULTIQC COMPLETED 0 14s 4.4s 1 80.1% 2 GB 0.0% 95.6 MB <p>We can now directly observe that the 2 cores we allocated to the <code>FASTQC</code> process have resulted in the expected CPU utilisation of close to 200%. Our onfiguration has worked! The walltime has also increased, but not quite the halving of execution time we might expect. The change is minimal due to the very small size of the test data, and would be more meaningful on \"real\" data.</p> <p>Nextflow <code>-resume</code></p> <p>In the previous few runs we have used <code>-resume</code> to run only the processes that were modified. Using <code>-resume</code> can considerably shorten development and testing time, and is also very handy when a real-world workflow fails part way through. </p> <p>However, the <code>-resume</code> function relies on cached data, and may not always resume when you expect it to! This aged but handy blog helps to demystify Nextflow resume and may be a good read - in addition to the Nextflow <code>-resume</code> guide - if you find yourself wondering why a task that you expected to be cached has re-run (or vice versa!) </p> <p>Writing efficient module scripts</p> <p>Beyond resource flags, it's also important to write efficient code inside your processes. If you're writing custom scripts (e.g. in Python or R):</p> <ul> <li>Prefer vectorised operations over loops</li> <li>Use optimised libraries like <code>numpy</code> for Python scripts</li> <li>Consider parallelisation strategies (e.g. OpenMP)</li> <li>Avoid holding large objects in memory unnecessarily</li> </ul> <p>While these are outside the scope of this workshop, they\u2019re good to consider if you want to scale up workflows on HPC.</p>"},{"location":"part2/24_resourcing/#247-summary","title":"2.4.7 Summary","text":"<p>In this lesson, we:</p> <ul> <li>Used our custom trace files alongside the details of the compute node hardware to come up with tailored resource requests for efficient execution</li> <li>Customised process resources within our custom configuration file, using Nextflow <code>withName</code> to specify which process received which resources</li> <li>Learnt that the process script must also be coded to use the allocated resources, and applied this dynamically using a Nextflow parameter</li> </ul> <p>The skills covered in this lesson equip you to build efficient workflows and to work responsibly on HPC. In the next lesson, we will apply strategies to increase the speed of our workflows, without harming the efficiency. </p>"},{"location":"part2/24_resourcing/#248-code-checkpoint","title":"2.4.8 Code checkpoint","text":"Show complete code modules/fastqc.nf<pre><code>process FASTQC {\n\n    tag \"fastqc on ${sample_id}\"\n    container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads_1), path(reads_2)\n\n    output:\n    path \"fastqc_${sample_id}\", emit: qc_out\n\n    script:\n    \"\"\"\n    mkdir -p \"fastqc_${sample_id}\"\n    fastqc -t ${task.cpus} --outdir \"fastqc_${sample_id}\" --format fastq $reads_1 $reads_2\n    \"\"\"\n\n}\n</code></pre> Gadi (PBS Pro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run main.nf -profile pbspro -c config/custom.config -resume\n</code></pre> custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6\n    memory = 4.GB\n\n    withName: /FASTQC|ALIGN|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n}\n    // Name the reports according to when they were run\n    params.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n    // Generate timeline-timestamp.html timeline report \n    timeline {\n        enabled = true\n        overwrite = false\n        file = \"./runInfo/timeline-${params.timestamp}.html\"\n    }\n\n    // Generate report-timestamp.html execution report \n    report {\n        enabled = true\n        overwrite = false\n        file = \"./runInfo/report-${params.timestamp}.html\"\n    }\n\n    trace {\n        enabled = true \n        overwrite = false \n        file = \"./runInfo/trace-${params.timestamp}.txt\"\n        fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss'\n    }\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run main.nf -profile slurm -c config/custom.config -resume\n</code></pre> custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n\n    withName: /FASTQC|ALIGN|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n}\n\n// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n\ntrace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss'\n}\n</code></pre>"},{"location":"part2/25_parallelisation/","title":"Parallelisation","text":"<p>Learning objectives</p> <ul> <li>Explain the limitations of parallelisation and cases where splitting data is not biologically correct </li> <li>Differentiate between multi-threading and scatter-gather parallelisation methods</li> <li>Implement parallelisation approaches in Nextflow and evaluate their impact on resource usage </li> </ul> <p>In Lesson 1.4, strategies to speed up your workflow by utilising more resources were introduced. As your data size or sample throughput grows, so too does the importance of efficient processing. In this lesson we will explore how Nextflow supports different forms of parallelisation to help you scale your workflows by adding multi-threading and scatter-gather parallelism to our custom workflow.</p> <p>Recall that assigning more cores than your tool can efficiently utilise or splitting your data up into too many chunks can lead to diminishing returns, where CPU efficiency is decreased and service unit cost is increased for little to no walltime gain. Multi-threading and parallelisation requires benchmarking to find the right balance between walltime, CPU efficiency, and service unit consumption. </p> <p>The figure below shows benchmarking results on real world data for alignment of human whole genome data using a scatter-gather method. As the number of nodes per sample increases (x-axis) the walltime decreases, until plateau by 6 nodes. This highlights the importance of benchmarking to avoid over-parallelisation.</p> <p></p>"},{"location":"part2/25_parallelisation/#251-multi-threading-bwa-mem","title":"2.5.1 Multi-threading <code>bwa mem</code>","text":"<p>In this section we will look at implementing another multi-threading example with <code>bwa mem</code>, used in the <code>ALIGN</code> process. These are the example benchmarking results from Part 1, with the CPU efficiency calculated for you with the formula  <code>cpu time / walltime / cpus</code>:</p> Cores Walltime (s) CPU time (s) CPU efficiency 2 0.744 1.381 93% 4 0.456 1.537 84% 6 0.355 1.618 76% 8 0.291 1.628 70% <p>CPU efficiency</p> <p>Recall that CPU efficiency is a measure of how many cpus were actually used, in comparison to how many cpus were requested. A high CPU efficiency (100%) means that all of the CPUs were utilised, while a low efficiency suggests that too many were requested.</p> <p>It is typical to see speed gains when additional resources are allocated to a tool that can make use of them, with some tools achieving a near-perfect efficiency of near 100% for many increases to core count. However, in reality very few multi-threading tools can sustain 1:1 gains at high core counts, so efficiency decreases, walltime plateaus, and service units creep up. As tempting as it may be to run BWA with 128 threads on Setonix, we promise you this is not a good idea! </p> <p>Note that the above benchmarking results are for demonstration purposes only, and for real-world benchmarking, larger test data should be used, aiming for a walltime of at least 5 minutes to observe reliable performance changes at different threads. </p> <p>From the example benchmarking results above, we need to consider the trade-offs between each run and what we would like to optimise for:</p> <ul> <li>Providing 2 cores has the slowest walltime but utilises the 2 CPUs most efficiently (93%)</li> <li>On the other hand, providing 8 cores provides ~40% speedup in walltime, but at the cost of reduced CPU efficiency</li> </ul> <p>The most suitable choice for your workflow depends on your research priorities at the time: turnaround time or minimise compute cost? In general however, aiming for &gt;80% CPU efficiency ensures we are not reserving resources in excess.</p> <p>Poll</p> <ol> <li>How many cores would you choose to provide <code>ALIGN</code> to ensure that it still uses the CPUs efficiently, but with a speedup in walltime? </li> <li>Which <code>.config</code> file would you want to use? (<code>nextflow.config</code> - workflow-specific, system-agnostic; <code>custom.config</code> - workflow-specific, system-specific)</li> <li>How much extra memory can you utilise if required? (Consider the effective RAM/CPUs proportion of the queue or partition)</li> </ol> Gadi (PBS Pro)Setonix (Slurm) Answers <ul> <li>4 CPUs has &gt; 80% CPU efficiency  </li> <li><code>custom.config</code> to ensure it is tuned for the <code>normalbw</code> queue</li> <li>4 CPUs with 16-18 GB memory</li> </ul> Answers <ul> <li>4 CPUs has &gt; 80% CPU efficiency  </li> <li><code>custom.config</code> to ensure it fits the <code>work</code> partition</li> <li>4 CPUs with 7-8 GB memory</li> </ul> <p>Exercise: Reassigning resources for <code>ALIGN</code></p> <ul> <li>In <code>config/custom.config</code>, update the <code>process</code> scope:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) conf/custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6\n    memory = 4.GB\n\n    withName: /FASTQC|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: ALIGN {\n        cpus = 4\n        memory = 16.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n}\n</code></pre> conf/custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n\n    withName: /FASTQC|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: ALIGN {\n        cpus = 4\n        memory = 8.GB\n        time = 2.minutes\n    }\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n}\n</code></pre> <p>Note: our <code>ALIGN</code> process (<code>modules/align.nf</code>) has <code>-t $task.cpus</code> already defined, so you do not need to amend it.</p> <ul> <li>Save your file and run with:</li> </ul> <pre><code>./run.sh\n</code></pre> <p>This should not have re-run the <code>ALIGN</code>, or any other processes. This is because we still have our <code>-resume</code> flag in our run scripts. Nextflow did not detect any changes to our workflow so used the cached files from the previous run:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [mighty_carson] DSL2 - revision: e34a5e5f9d\n\n[26/95a75a] FASTQC (fastqc on NA12877) | 1 of 1, cached: 1 \u2714\n[7d/99698f] ALIGN (1)                  | 1 of 1, cached: 1 \u2714\n[96/cdfe17] GENOTYPE (1)               | 1 of 1, cached: 1 \u2714\n[8a/2b3191] JOINT_GENOTYPE (1)         | 1 of 1, cached: 1 \u2714\n[88/dab55c] STATS (1)                  | 1 of 1, cached: 1 \u2714\n[34/65d4e2] MULTIQC                    | 1 of 1, cached: 1 \u2714\n</code></pre> <p>As configuration generally does not trigger the re-run of processes, we need to run the workflow from the beginning in order to apply the updated resource configuration. </p> <p>Exercise: Run from scratch!</p> <ul> <li>Remove the <code>-resume</code> flag from your <code>run.sh</code> script, save, and re-submit:</li> </ul> <pre><code>./run.sh\n</code></pre> <ul> <li>Inspect your trace file and confirm that <code>ALIGN</code> has been allocated the cores and memory that you added in <code>custom.config</code>:</li> </ul> Output Gadi (PBS Pro)Setonix (Slurm) name status exit realtime cpus %cpu memory %mem rss FASTQC (fastqc on NA12877) COMPLETED 0 4s 2 100.2% 1 GB 0.2% 241.5 MB ALIGN (1) COMPLETED 0 1s 4 197.8% 16 GB 0.0% 6.3 MB GENOTYPE (1) COMPLETED 0 41s 2 137.3% 2 GB 1.1% 1.3 GB JOINT_GENOTYPE (1) COMPLETED 0 8s 2 160.9% 1 GB 0.3% 438.5 MB STATS (1) COMPLETED 0 0ms 1 61.9% 2 GB 0.0% 3.1 MB MULTIQC COMPLETED 0 3.7s 1 88.9% 2 GB 0.1% 92.6 MB name status exit realtime cpus %cpu memory %mem rss ALIGN (1) COMPLETED 0 0ms 4 257.0% 8 GB 0.0% 2 MB FASTQC (fastqc on NA12877) COMPLETED 0 9s 2 61.4% 1 GB 0.1% 247.6 MB GENOTYPE (1) COMPLETED 0 39s 2 127.2% 2 GB 0.6% 1.6 GB JOINT_GENOTYPE (1) COMPLETED 0 9s 2 176.0% 1 GB 0.2% 447.7 MB STATS (1) COMPLETED 0 1s 1 70.1% 2 GB 0.0% 2 MB MULTIQC COMPLETED 0 9.3s 1 38.7% 2 GB 0.0% 85.7 MB <p>Remember to read the tool documentation!</p> <p>All software and bioinformatics tools are built differently. Some support multi-threading, some can only run things with a single thread. Overlooking these details may not be crucial when running on systems where you have autonomy and access to all resources (personal compute, cloud instances), however, these are important parts of configuring your workflow on HPC shared systems to set reasonable limits and requests.</p>"},{"location":"part2/25_parallelisation/#2511-code-checkpoint","title":"2.5.1.1 Code checkpoint","text":"Show complete code Gadi (PBS Pro)Setonix (Slurm) config/custom.config<pre><code>process {\n    cpu = 1 // 'normalbw' queue = 128 GB / 28 CPU ~ 4.6\n    memory = 4.GB\n\n    withName: /FASTQC|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: ALIGN {\n        cpus = 4\n        memory = 16.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n}\n\n// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n\ntrace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss'\n}\n</code></pre> config/custom.config<pre><code>process {\n    cpu = 1 // 'work' partition = 230 GB / 128 CPU ~ 1.8\n    memory = 2.GB\n\n    withName: /FASTQC|JOINT_GENOTYPE/ {\n        cpus = 2\n        memory = 1.GB\n        time = 2.minutes\n    }\n\n    withName: ALIGN {\n        cpus = 4\n        memory = 8.GB\n        time = 2.minutes\n    }\n\n    withName: GENOTYPE {\n        cpus = 2\n        memory = 2.GB\n        time = 2.minutes\n    }\n\n    withName: /STATS|MULTIQC/ {\n        cpus = 1\n        memory = 2.GB\n        time = 2.minutes\n    }\n}\n\n// Name the reports according to when they were run\nparams.timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate timeline-timestamp.html timeline report \ntimeline {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/timeline-${params.timestamp}.html\"\n}\n\n// Generate report-timestamp.html execution report \nreport {\n    enabled = true\n    overwrite = false\n    file = \"./runInfo/report-${params.timestamp}.html\"\n}\n\ntrace {\n    enabled = true \n    overwrite = false \n    file = \"./runInfo/trace-${params.timestamp}.txt\"\n    fields = 'name,status,exit,realtime,cpus,%cpu,memory,%mem,rss'\n}\n</code></pre>"},{"location":"part2/25_parallelisation/#252-faster-alignment-with-scatter-gather","title":"2.5.2 Faster alignment with scatter-gather","text":"<p>One of the core benefits of running bioinformatics workflows on HPC is access to increased processing power and hardware. For jobs that can be conducted independently of each other, if configured correctly, we can run many jobs simultaneously and reduce the overall walltime required to run the workflow. One strategy to implement this is by:</p> <ol> <li>Splitting the data into smaller sub-parts </li> <li>\"Scattering\" analysis of the parts across the compute cluster to be analysed separately </li> <li>\"Gathering\" the processed outputs back into a single combined file</li> </ol> <p>Not everything can or should be split</p> <p>Recall from Part 1 that we can't split everything - it should only be done if the particular processing step can be run on subsections of the data independently of each other. Scattering tasks does not make sense when results depend on comparing all data together, such as detecting structural variants across multiple chromosomes.</p> <p>With scatter-gather parallelism, we can take \"tall\" jobs (long walltime) amenable to valid chunking - like sequence alignment - and mould it into many \"small and short\" jobs to take advantage of the \"gap filling\" tendency of the job scheduler. </p>"},{"location":"part2/25_parallelisation/#2521-scatter-splitting-our-reads","title":"2.5.2.1 Scatter: splitting our reads","text":"<p>In whole genome sequence analysis, alignment is typically the largest bottleneck. Given the independent nature of the sequence fragments, scatter-gather of this step is a widely used approach for speeding up processing time. </p> <p>To do so, we will leverage Nextflow's built-in <code>splitFastq</code> operator.</p> <p>Exercise: Adding <code>.splitFastq</code> to the workflow</p> <ul> <li>Copy the following lines, and paste in <code>main.nf</code> after <code>FASTQC(reads)</code> and before <code>ALIGN(reads, bwa_index)</code>:</li> </ul> main.nf<pre><code>    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .view()\n</code></pre> Show change <p>Before:</p> main.nf<pre><code>    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(reads, bwa_index)\n</code></pre> <p>After:</p> main.nf<pre><code>    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(reads, bwa_index)\n</code></pre> <ul> <li>The <code>reads</code> channel is taken as input. It contains the <code>[ sample_name, fastq_r1, fastq_r2 ]</code></li> <li><code>.splitFastq</code> splits each paired <code>.fastq</code> file (<code>pe: true</code>) into three files (<code>limit: 3</code>)</li> <li><code>file: true</code> stores each split <code>.fastq</code> file in the work directory and avoids out-of-memory issues</li> <li>We include <code>.view()</code> to inspect the contents of the <code>split_fqs</code> channel we just created</li> </ul> <p>Next, we need to update the inputs to <code>ALIGN</code>, so it takes the split <code>.fastq</code> files.</p> <p>Exercise: Updating the <code>ALIGN</code> input</p> <ul> <li>In <code>main.nf</code>, in the <code>workflow</code> scope, replace the input argument to <code>ALIGN</code> from <code>ALIGN(reads, bwa_index)</code> to <code>ALIGN(split_fqs, bwa_index)</code>.</li> </ul> main.nf<pre><code>    ALIGN(split_fqs, bwa_index)\n</code></pre> Show change <p>Before:</p> main.nf<pre><code>    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(reads, bwa_index)\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(ALIGN.out.aligned_bam, ref)\n</code></pre> <p>After:</p> main.nf<pre><code>    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(split_fqs, bwa_index)\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(ALIGN.out.aligned_bam, ref)\n</code></pre> <ul> <li>Save the file, and run:</li> </ul> <pre><code>./run.sh\n</code></pre> Show output Output<pre><code> N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [suspicious_wiles] DSL2 - revision: de5d65b946\n\n[47/e84f4d] FASTQC (fastqc on NA12877) [100%] 1 of 1 \u2714\n[c5/6d69ea] ALIGN (3)                  [100%] 3 of 3 \u2714\n[a7/7a6424] GENOTYPE (2)               [100%] 3 of 3 \u2714\n[47/e84f4d] FASTQC (fastqc on NA12877) [100%] 1 of 1 \u2714\n[c5/6d69ea] ALIGN (3)                  [100%] 3 of 3 \u2714\n[a7/7a6424] GENOTYPE (2)               [100%] 3 of 3 \u2714\n[-        ] JOINT_GENOTYPE             -\n[-        ] STATS                      -\n[-        ] MULTIQC                    -\n[NA12877, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/a0/569c8d068367a6f922be0841dce142/NA12877_chr20-22.R1.1.fq, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/9e/c4cab021b4ecfa15e1f9a059ffd8e7/NA12877_chr20-22.R2.1.fq]\n[NA12877, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/a0/569c8d068367a6f922be0841dce142/NA12877_chr20-22.R1.2.fq, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/9e/c4cab021b4ecfa15e1f9a059ffd8e7/NA12877_chr20-22.R2.2.fq]\n[NA12877, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/a0/569c8d068367a6f922be0841dce142/NA12877_chr20-22.R1.3.fq, /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/9e/c4cab021b4ecfa15e1f9a059ffd8e7/NA12877_chr20-22.R2.3.fq]\nERROR ~ Error executing process &gt; 'JOINT_GENOTYPE (1)'\n\nCaused by:\n  Process `JOINT_GENOTYPE` input file name collision -- There are multiple input files for each of the following file names: NA12877.g.vcf.gz, NA12877.g.vcf.gz.tbi\n\nContainer:\n  /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/singularity/quay.io-biocontainers-gatk4-4.6.2.0--py310hdfd78af_1.img\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\n</code></pre> <p>Let's take a look at the <code>stdout</code> printed.</p> <p>The output of <code>reads.splitFastq()</code> include three separate arrays that contain:</p> <ul> <li>The name of the sample</li> <li>The path to the R1 <code>.fastq</code> file</li> <li>The path to the R2 <code>.fastq</code> file</li> </ul> <p>Note that each <code>.fastq</code> file is now identified with a chunk number (e.g. <code>.../NA12877_chr20-22.R2.1.fq</code>) - we have successfully split the reads into three.</p> <p>The following line indicates that the <code>ALIGN</code> and <code>GENOTYPE</code> processes now run three times, successfully:</p> <pre><code>        [c5/6d69ea] ALIGN (3)                  [100%] 3 of 3 \u2714\n        [a7/7a6424] GENOTYPE (2)               [100%] 3 of 3 \u2714\n</code></pre> <p>Note</p> <p>Three separate tasks were automatically generated and scheduled by Nextflow without any extra instructions from us. Because we have already configured <code>ALIGN</code> and <code>GENOTYPE</code> in our config files, all scattered tasks used those resource settings automatically.</p> <p>Setting up your channels and using groovy operators can be a bit tricky at first, however, once these are set up correctly, Nextflow will take care of the scatter\u2013gather orchestration for you. This makes it straightforward to parallelise work at scale with minimal additional code.</p> <p>However, you should have received an error before <code>JOINT_GENOTYPE</code> was run:</p> Output<pre><code> Caused by:\n          Process `JOINT_GENOTYPE` input file name collision -- There are multiple input files for each of the following file names: NA12877.g.vcf.gz, NA12877.g.vcf.gz.tbi\n</code></pre> <p>Let's troubleshoot by inspecting the output of the <code>GENOTYPE</code> process</p> <p>Advanced exercise: Troubleshoot <code>GENOTYPE</code> error</p> <ul> <li>Inspect the process outputs using <code>.view()</code>. Copy and paste the following line after <code>GENOTYPE(ALIGN.out.aligned_bam, ref)</code>.</li> </ul> main.nf<pre><code>GENOTYPE.out.view()\n</code></pre> <ul> <li>Save the <code>main.nf</code> file</li> <li>Update your run script so it runs with <code>-resume</code>, and re-run:</li> </ul> <pre><code>./run.sh \n</code></pre> Output Output<pre><code># workdirs have been truncated with '...' for readability\n[NA12877, .../NA12877_chr20-22.R1.1.fq, .../NA12877_chr20-22.R2.1.fq]\n[NA12877, .../NA12877_chr20-22.R1.2.fq, .../NA12877_chr20-22.R2.2.fq]\n[NA12877, .../NA12877_chr20-22.R1.3.fq, .../NA12877_chr20-22.R2.3.fq]\n[NA12877, .../NA12877.g.vcf.gz, .../NA12877.g.vcf.gz.tbi]\n[NA12877, .../NA12877.g.vcf.gz, .../NA12877.g.vcf.gz.tbi]\n[NA12877, .../NA12877.g.vcf.gz, .../NA12877.g.vcf.gz.tbi]\nERROR ~ Error executing process &gt; 'JOINT_GENOTYPE (1)'\n\nCaused by:\n  Process `JOINT_GENOTYPE` input file name collision -- There are multiple input files for each of the following file names: NA12877.g.vcf.gz, NA12877.g.vcf.gz.tbi\n\n\nContainer:\n  /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/singularity/quay.io-biocontainers-gatk4-4.6.2.0--py310hdfd78af_1.img\n\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n\n -- Check '.nextflow.log' file for details\n</code></pre> <ul> <li>The first three lines are the outputs of our <code>.splitFastq()</code> operation, this has not changed since the last time the workflow was run</li> <li>The last three lines are the outputs emitted from the <code>GENOTYPE</code> process. One output reflects the run for one of the chunks processed. However, all the output names are the same. This is the cause of the error.</li> </ul> <p>We will resolve this by updating our channels to include the chunk id, and rename how the bam files are output, ensuring they are uniquely identified.</p> <p>Exercise: Adding the <code>chunk_id</code> to the tuple</p> <ul> <li>Update the workflow script to include the <code>chunk_id</code>. This will be used to identify the reads to avoid the file name collision error previously. Add <code>.view()</code> to see how the <code>split_fqs</code> channel and the output for <code>ALIGN</code> has changed.</li> </ul> main.nf<pre><code>    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .map { sample, r1, r2 -&gt;\n            def chunk_id = r1.toString().tokenize('.')[2]\n            return [ sample, r1, r2, chunk_id ]\n        }\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(split_fqs, bwa_index)\n    ALIGN.out.view()\n</code></pre> <p>Since our input channel has changed, we need to update the input block of our <code>ALIGN</code> module to recognise the <code>chunk_id</code>. We also want to rename our output bam files to include the chunk id.</p> <ul> <li>Copy and paste the following, replacing your entire <code>module/align.nf</code></li> </ul> module/align.nf<pre><code>process ALIGN {\n\n    container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0\"\n    publishDir \"${params.outdir}/alignment\"\n\n    input:\n    tuple val(sample_id), path(reads_1), path(reads_2), val(chunk_id)\n    tuple val(ref_name), path(bwa_index)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}.${chunk_id}.bam\"), path(\"${sample_id}.${chunk_id}.bam.bai\"), emit: aligned_bam\n\n    script:\n    \"\"\"\n    bwa mem -t $task.cpus -R \"@RG\\\\tID:${sample_id}\\\\tPL:ILLUMINA\\\\tPU:${sample_id}\\\\tSM:${sample_id}\\\\tLB:${sample_id}\\\\tCN:SEQ_CENTRE\" ${bwa_index}/${ref_name} $reads_1 $reads_2 | samtools sort -O bam -o ${sample_id}.${chunk_id}.bam\n    samtools index ${sample_id}.${chunk_id}.bam\n    \"\"\"\n\n}\n</code></pre> <ul> <li>Save your <code>main.nf</code> and <code>module/align.nf</code> and re-run:</li> </ul> <pre><code>./run.sh\n</code></pre> Output Output<pre><code>[3c/2aa92e] FASTQC (fastqc on NA12877) [100%] 1 of 1, cached: 1 \u2714\n[25/85c48b] ALIGN (1)                  [100%] 3 of 3, cached: 3 \u2714\n[3d/f1dc44] GENOTYPE (1)               [100%] 1 of 1, failed: 1\n[-        ] JOINT_GENOTYPE             -\n[-        ] STATS                      -\n[-        ] MULTIQC                    -\n[NA12877, .../NA12877_chr20-22.R1.1.fq, .../NA12877_chr20-22.R2.1.fq, 1]\n[NA12877, .../NA12877_chr20-22.R1.2.fq, .../NA12877_chr20-22.R2.2.fq, 2]\n[NA12877, .../NA12877_chr20-22.R1.3.fq, .../NA12877_chr20-22.R2.3.fq, 3]\n[NA12877, .../NA12877.3.bam, .../NA12877.3.bam.bai]\n[NA12877, .../NA12877.1.bam, .../NA12877.1.bam.bai]\n[NA12877, .../NA12877.2.bam, .../NA12877.2.bam.bai]\n</code></pre> <p>The pipeline will fail, however <code>ALIGN</code> now includes the chunk id in the bam and bam index names.</p> <p>Question</p> <p>Can you think of why the code will fail? </p>"},{"location":"part2/25_parallelisation/#2522-code-checkpoint","title":"2.5.2.2 Code checkpoint","text":"Show complete code main.nf<pre><code>include { FASTQC } from './modules/fastqc'\ninclude { ALIGN } from './modules/align'\ninclude { GENOTYPE } from './modules/genotype'\ninclude { JOINT_GENOTYPE } from './modules/joint_genotype'\ninclude { STATS } from './modules/stats'\ninclude { MULTIQC } from './modules/multiqc'\n\n// Define the workflow\nworkflow {\n\n    // Define the fastqc input channel\n    reads = Channel.fromPath(params.samplesheet)\n        .splitCsv(header: true)\n        .map { row -&gt; {\n            // def strandedness = row.strandedness ? row.strandedness : 'auto'\n            [ row.sample, file(row.fastq_1), file(row.fastq_2) ] \n        }}\n\n    bwa_index = Channel.fromPath(params.bwa_index)\n        .map { idx -&gt; [ params.bwa_index_name, idx ] }\n        .first()\n    ref = Channel.of( [ file(params.ref_fasta), file(params.ref_fai), file(params.ref_dict) ] ).first()\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .map { sample, r1, r2 -&gt;\n            def chunk_id = r1.toString().tokenize('.')[2]\n            return [ sample, r1, r2, chunk_id ]\n        }\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(split_fqs, bwa_index)\n    ALIGN.out.view()\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(ALIGN.out.aligned_bam, ref)\n\n    // Gather gvcfs and run joint genotyping\n    all_gvcfs = GENOTYPE.out.gvcf\n        .map { _sample_id, gvcf, gvcf_idx -&gt; [ params.cohort_name, gvcf, gvcf_idx ] }\n        .groupTuple()\n    JOINT_GENOTYPE(all_gvcfs, ref)\n\n    // Get VCF stats\n    STATS(JOINT_GENOTYPE.out.vcf)\n\n    // Collect summary data for MultiQC\n    multiqc_in = FASTQC.out.qc_out\n        .mix(STATS.out.stats_out)\n        .collect()\n\n    /*\n    * Generate the analysis report with the \n    * outputs from fastqc and bcftools stats\n    */ \n    MULTIQC(multiqc_in)\n\n}\n</code></pre> module/align.nf<pre><code>    process ALIGN {\n\n    container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0\"\n    publishDir \"${params.outdir}/alignment\"\n\n    input:\n    tuple val(sample_id), path(reads_1), path(reads_2), val(chunk_id)\n    tuple val(ref_name), path(bwa_index)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}.${chunk_id}.bam\"), path(\"${sample_id}.${chunk_id}.bam.bai\"), emit: aligned_bam\n\n    script:\n    \"\"\"\n    bwa mem -t $task.cpus -R \"@RG\\\\tID:${sample_id}\\\\tPL:ILLUMINA\\\\tPU:${sample_id}\\\\tSM:${sample_id}\\\\tLB:${sample_id}\\\\tCN:SEQ_CENTRE\" ${bwa_index}/${ref_name} $reads_1 $reads_2 | samtools sort -O bam -o ${sample_id}.${chunk_id}.bam\n    samtools index ${sample_id}.${chunk_id}.bam\n    \"\"\"\n\n}\n</code></pre>"},{"location":"part2/25_parallelisation/#2523-gather-combining-our-scattered-alignments","title":"2.5.2.3 Gather: combining our scattered alignments","text":"<p>Now that we have sucessfully split our reads and uniquely identified the output bam files, we will implement a gather pattern to bring our alignments into a single file again. This is the source of the above expected error: the workflow logic expected one mapping file output per sample, but we have 3. Like we added a process for the splitting task, we also need to add a process for the gathering task. </p> <p>Different patterns for different needs</p> <p>There is no one-size-fits-all approach for scattering and gathering. How this is implemented in Nextflow will be highly dependent on your workflow structure, and input and output files. Nextflow patterns provides examples of commonly used patterns that support a range of different needs, such as splitting text and CSV files, to re-grouping and organising results for downstream processing.</p> <p>Whole\u2011genome alignment is an excellent use case for scatter\u2013gather as the alignment of each chunk can run independently, dramatically reducing walltime for this heavy step. Once alignment is complete, the BAM files are merged and the workflow proceeds as normal.</p> <p>Where you choose to re\u2011gather your data will depend on where your bottlenecks are and at which points you need to process the dataset as a whole again.</p> <p>Exercise: Adding <code>MERGE_BAMS</code></p> <ul> <li>Import the <code>MERGE_BAMS</code> module in your <code>main.nf</code> file.</li> </ul> main.nf<pre><code>include { FASTQC } from './modules/fastqc'\ninclude { SPLIT_FASTQ } from './modules/split_fastq'\ninclude { ALIGN } from './modules/align'\ninclude { MERGE_BAMS } from './modules/merge_bams'\ninclude { GENOTYPE } from './modules/genotype'\ninclude { JOINT_GENOTYPE } from './modules/joint_genotype'\ninclude { STATS } from './modules/stats'\ninclude { MULTIQC } from './modules/multiqc'\n</code></pre> <ul> <li>Insert the following lines in <code>main.nf</code>, after <code>ALIGN.out.view()</code>. Update the <code>GENOTYPE</code> process so it takes in our merged bams. </li> </ul> main.nf<pre><code>    gathered_bams = ALIGN.out.aligned_bam\n        .groupTuple()\n\n    MERGE_BAMS(gathered_bams)\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(MERGE_BAMS.out.aligned_bam, ref)\n</code></pre> Show changes <p>Before:</p> main.nf<pre><code>// Run the align step with the reads_in channel and the genome reference\nALIGN(split_fqs, bwa_index)\nALIGN.out.view()\n\n// Run genotyping with aligned bam and genome reference\nGENOTYPE(ALIGN.out.aligned_bam, ref)\n</code></pre> <p>After:</p> main.nf<pre><code>// Run the align step with the reads_in channel and the genome reference\nALIGN(split_fqs, bwa_index)\nALIGN.out.view()\n\ngathered_bams = ALIGN.out.aligned_bam\n    .groupTuple()\n\nMERGE_BAMS(gathered_bams)\n\n// Run genotyping with aligned bam and genome reference\nGENOTYPE(MERGE_BAMS.out.aligned_bam, ref)\n</code></pre> <ul> <li>Re-run the pipeline:</li> </ul> <pre><code>./run.sh\n</code></pre> <p>Now, let's re-inspect that the merge worked as intended.</p> <p>Exercise: Inspecting the <code>MERGE_BAM</code> task directory</p> <ul> <li>Locate the work directory for the <code>MERGE_BAM</code> process using the trace file, if you have the <code>workdir</code> field, or use the nextflow log.</li> </ul> <p><pre><code>tree -a &lt;workdir&gt;\n</code></pre> Output<pre><code>.\n\u251c\u2500\u2500 .command.begin\n\u251c\u2500\u2500 .command.err\n\u251c\u2500\u2500 .command.log\n\u251c\u2500\u2500 .command.out\n\u251c\u2500\u2500 .command.run\n\u251c\u2500\u2500 .command.sh\n\u251c\u2500\u2500 .command.trace\n\u251c\u2500\u2500 .exitcode\n\u251c\u2500\u2500 NA12877.1.bam -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/fc/bebb980d3d81cba7dacb6d052faf08/NA12877.1.bam\n\u251c\u2500\u2500 NA12877.1.bam.bai -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/fc/bebb980d3d81cba7dacb6d052faf08/NA12877.1.bam.bai\n\u251c\u2500\u2500 NA12877.2.bam -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/ec/b16bfd4ccc0c627553eb0e55337b21/NA12877.2.bam\n\u251c\u2500\u2500 NA12877.2.bam.bai -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/ec/b16bfd4ccc0c627553eb0e55337b21/NA12877.2.bam.bai\n\u251c\u2500\u2500 NA12877.3.bam -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/c7/b93c40ec2e8c9eebe4cce19ac96ef7/NA12877.3.bam\n\u251c\u2500\u2500 NA12877.3.bam.bai -&gt; /scratch/pawsey1227/fjaya/nextflow-on-hpc-materials/part2/work/c7/b93c40ec2e8c9eebe4cce19ac96ef7/NA12877.3.bam.bai\n\u251c\u2500\u2500 NA12877.bam\n\u2514\u2500\u2500 NA12877.bam.bai\n</code></pre></p> <ul> <li>View the <code>.command.sh</code> file. Have the expected files been merged?</li> </ul> <pre><code>cat .command.sh\n</code></pre> Output<pre><code>#!/bin/bash -ue\nsamtools cat NA12877.3.bam NA12877.2.bam NA12877.1.bam | samtools sort -O bam -o NA12877.bam\nsamtools index NA12877.bam\n</code></pre> <p>We can now see that each of the 3 BAMs resulting from the scattered alignment was merged into a single BAM file for the sample, and the rest of the workflow progressed as normal.</p> <p>To sum up this step, you successfully:</p> <ol> <li>Split the paired FASTQ reads into 3 chunks using <code>.splitFastq</code></li> <li>Aligned each chunk in parallel</li> <li>Merged the aligned chunks into a single BAM (<code>MERGE_BAMS()</code>)</li> <li>Ran the remainder of the workflow as usual</li> </ol> <p>This change optimises performance for large datasets by leveraging parallel processing.</p> <p>What about multiple samples?</p> <p>You have now used scatter-gather physical data chunking to align a single sample \"embarassingly parallel\". As processes are run independently of each other this does not always need to apply to single sample that is split. Running multiple samples is also a form of multi-processing and comes shipped with Nextflow's dataflow model. Once your pipeline is configured to run well with a single sample, queue channels make adding additional samples relatively easy.</p> <p>We will revisit this in the next section.</p>"},{"location":"part2/25_parallelisation/#2524-code-checkpoint","title":"2.5.2.4 Code checkpoint","text":"Show code main.nf<pre><code>include { FASTQC } from './modules/fastqc'\ninclude { ALIGN } from './modules/align'\ninclude { MERGE_BAMS } from './modules/merge_bams'\ninclude { GENOTYPE } from './modules/genotype'\ninclude { JOINT_GENOTYPE } from './modules/joint_genotype'\ninclude { STATS } from './modules/stats'\ninclude { MULTIQC } from './modules/multiqc'\n\n// Define the workflow\nworkflow {\n\n    // Define the fastqc input channel\n    reads = Channel.fromPath(params.samplesheet)\n        .splitCsv(header: true)\n        .map { row -&gt; {\n            // def strandedness = row.strandedness ? row.strandedness : 'auto'\n            [ row.sample, file(row.fastq_1), file(row.fastq_2) ] \n        }}\n\n    bwa_index = Channel.fromPath(params.bwa_index)\n        .map { idx -&gt; [ params.bwa_index_name, idx ] }\n        .first()\n    ref = Channel.of( [ file(params.ref_fasta), file(params.ref_fai), file(params.ref_dict) ] ).first()\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads)\n\n    // Split FASTQs for each sample\n    split_fqs = reads\n        .splitFastq(limit: 3, pe: true, file: true)\n        .map { sample, r1, r2 -&gt;\n            def chunk_id = r1.toString().tokenize('.')[2]\n            return [ sample, r1, r2, chunk_id ]\n        }\n        .view()\n\n    // Run the align step with the reads_in channel and the genome reference\n    ALIGN(split_fqs, bwa_index)\n    ALIGN.out.view()\n\n    gathered_bams = ALIGN.out.aligned_bam\n        .groupTuple()\n\n    MERGE_BAMS(gathered_bams)\n\n    // Run genotyping with aligned bam and genome reference\n    GENOTYPE(MERGE_BAMS.out.aligned_bam, ref)\n\n    // Gather gvcfs and run joint genotyping\n    all_gvcfs = GENOTYPE.out.gvcf\n        .map { _sample_id, gvcf, gvcf_idx -&gt; [ params.cohort_name, gvcf, gvcf_idx ] }\n        .groupTuple()\n    JOINT_GENOTYPE(all_gvcfs, ref)\n\n    // Get VCF stats\n    STATS(JOINT_GENOTYPE.out.vcf)\n\n    // Collect summary data for MultiQC\n    multiqc_in = FASTQC.out.qc_out\n        .mix(STATS.out.stats_out)\n        .collect()\n\n    /*\n    * Generate the analysis report with the \n    * outputs from fastqc and bcftools stats\n    */ \n    MULTIQC(multiqc_in)\n\n}\n</code></pre>"},{"location":"part2/25_parallelisation/#253-a-note-on-dynamic-resourcing","title":"2.5.3 A note on dynamic resourcing","text":"<p>Since our data is small and similar-sized, we can apply the same resource configurations within the same process and it will still run successfully. However, it is common that we need to run the same process with input data of widely varying sizes. For example, if we were analysing tumour-normal matched genome sequences and the tumour samples were sequenced to double the depth of the normal samples, we may need to apply double the walltime and increased memory for some tasks. </p> <p>One option may be to configure the resource usage to suit the largest workflow input (e.g. highest coverage sample). This will ensure all processes run sucessfully at the cost of underutilising the resources you have requested for the smaller inputs:</p> <ul> <li>If extra walltime than needed is requested, this may cause jobs to queue for longer, particularly if they also request larger quantities of cores and memory. </li> <li>If additional cores or memory are involved, the smaller inputs that do not need this resource boost will waste resources that could be allocated to other jobs, and cause the workflow to be charged more service units than necessary. </li> </ul> <p>The alternative would be to take a dynamic approach when you need to process a number of inputs that require different resources. </p> <p>If a job fails, you can tell Nextflow to automatically re-run it with additional resources. For example:</p> Directive Closure Example Attempt 1 (Initial Run) Attempt 2 (First Retry) <code>memory</code> <code>{ 2.GB * task.attempt }</code> 2 GB 4 GB <code>time</code> <code>{ 1.hour * task.attempt }</code> 1 hour 2 hours <p>Another approach is to dynamically assign a resource based on properties of the input data. For example, by the size of the file:</p> <pre><code>process {\n  withName: 'ALIGN' {\n    memory = { reads.size() * 2 }\n  }\n}\n</code></pre> <p>For more information, see Nextflow's training on:</p> <ul> <li>Retry strategies</li> <li>Dynamic directives</li> </ul> <p>The same resource optimisation and configuration concepts that we have covered in this workshop continue to apply here: perform benchmarking with your data on the target HPC, monitor resource usage, and configure optimal resources to best suit the underlying hardware of the most suited queue/partition. As always, note that the development time for this optimsiation will be increased, however the time spent is particularly worthwhile if developing and running high-throughput workflows, for example cancer population studies where very high sequencing depth, 2 or more samples per patient, and large patient cohorts, are often involved. </p>"},{"location":"part2/25_parallelisation/#254-summary","title":"2.5.4 Summary","text":"<p>Recall that it is not always biologically valid to parallelise everything, and embarassingly parallel tasks need to be benchmarked to find the sweet spot where speedup has not introduced a large increase in sevice unit cost due to declining CPU efficiency. </p> <p>Once you have your workflow running on HPC, reviewing the custom trace and resource monitoring files can help identify long-running or inefficent processes that can benefit from optimisation. Explore whether these tasks can benefit from multi-threading or scatter-gather parallelism. Always use parallel by sample in your Nextflow workflows (never loops!) and only analyse all samples in one job when the nature of the task is to combine/co-analyse all data at once. </p>"},{"location":"part2/27_scale/","title":"Scale to multiple samples","text":"<p>Learning objectives</p> <ul> <li>Apply an optimised pipeline across multiple samples</li> <li>Understand sample-level and within-sample parallelism</li> <li>Recall best practices for running multi-sample data with samplesheets</li> </ul> <p>Now that we have a pipeline that runs successfully and is configured for our HPC according to a representative sample, we will proceed to run it on all samples in our simplified dataset. </p> <p>Nextflow's \"dataflow\" model and channels makes this easy to execute. Unlike the exercises where we added multi-threading and scatter-gather parallelisation, we do not need to edit the workflow code to scale our analysis to include more samples. This is one of the many great advantages to Nextflow.  </p> <p>Every additional sample can run through all per-sample processing tasks independently and in parallel, using the same code and resources we've configured. For more information, refer to the Nextflow for the Life Science's explainers on queue channels and input samplesheets.</p>"},{"location":"part2/27_scale/#271-applying-at-scale","title":"2.7.1 Applying at scale","text":"<p>In order to run all samples, we need to supply the workflow with a samplesheet that contains metadata for all samples in the workflow. The samplesheet we have used for our testing and development includes one sample only. </p> <p>We will now switch to the full samplesheet. Note that careful and error-free construction of a samplesheet is just as important as the workflow codebase. Errors within the samplesheet can be a source of early or late workflow failure, so care at this stage can avoid frustration downstream. </p> <p>We will now replace the samplesheet we have provided as a parameter to the workflow, and include the <code>-resume</code> flag so that the sample already processed during workflow development is not wastefully re-run. </p> <p>Exercise: Running on all samples</p> <ul> <li>Update your <code>run.sh</code> script to provide the <code>samplesheets_full.csv</code> file as argument to the <code>--samplesheet</code> parameter:</li> </ul> Gadi (PBS Pro)Setonix (Slurm) run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.04.5\nmodule load singularity\n\nnextflow run main.nf -profile pbspro -c config/custom.config --samplesheet \"samplesheet_full.csv\" -resume\n</code></pre> run.sh<pre><code>#!/bin/bash\n\nmodule load nextflow/24.10.0\nmodule load singularity/4.1.0-slurm\n\nnextflow run main.nf -profile slurm -c config/custom.config --samplesheet \"samplesheet_full.csv\" -resume\n</code></pre> <ul> <li>Save your script and re-run!</li> </ul> <pre><code>./run.sh\n</code></pre> <p>Your output should now look something like this:</p> Gadi (PBS Pro)Setonix (Slurm) Output<pre><code>N E X T F L O W   ~  version 24.04.5\n\nLaunching `main.nf` [irreverent_hilbert] DSL2 - revision: 029efd6fbc\n\nexecutor &gt;  pbspro (22)\n[8c/d3131f] FASTQC (fastqc on NA12889)             [100%] 3 of 3 \u2714\n[82/38ccb6] SPLIT_FASTQ (split fastqs for NA12877) [100%] 3 of 3 \u2714\n[e2/7a1203] ALIGN_CHUNK (2)                        [100%] 9 of 9 \u2714\n[48/38ee62] MERGE_BAMS (3)                         [100%] 3 of 3 \u2714\n[28/88ea07] GENOTYPE (1)                           [100%] 3 of 3 \u2714\n[31/bff8ce] JOINT_GENOTYPE (1)                     [100%] 1 of 1 \u2714\n[e0/2769b4] STATS (1)                              [100%] 1 of 1 \u2714\n[de/544e91] MULTIQC                                [100%] 1 of 1 \u2714\nCompleted at: 12-Nov-2025 09:34:02\nDuration    : 6m 2s\nCPU hours   : 0.1\nSucceeded   : 22\n</code></pre> Output<pre><code>N E X T F L O W   ~  version 24.10.0\n\nLaunching `main.nf` [fabulous_panini] DSL2 - revision: 029efd6fbc\n\nexecutor &gt;  slurm (22)\n[e6/da7692] FASTQC (fastqc on NA12877)             [100%] 3 of 3 \u2714\n[f0/fadc47] SPLIT_FASTQ (split fastqs for NA12878) [100%] 3 of 3 \u2714\n[74/bced2a] ALIGN_CHUNK (6)                        [100%] 9 of 9 \u2714\n[84/1bd560] MERGE_BAMS (1)                         [100%] 3 of 3 \u2714\n[2f/2d96be] GENOTYPE (1)                           [100%] 3 of 3 \u2714\n[b8/f88205] JOINT_GENOTYPE (1)                     [100%] 1 of 1 \u2714\n[6e/165917] STATS (1)                              [100%] 1 of 1 \u2714\n[65/432772] MULTIQC                                [100%] 1 of 1 \u2714\nCompleted at: 12-Nov-2025 07:32:34\nDuration    : 4m 32s\nCPU hours   : (a few seconds)\nSucceeded   : 22\n</code></pre>"},{"location":"part2/27_scale/#272-sanity-checking-workflow-execution","title":"2.7.2 Sanity checking workflow execution","text":"<p>We have spent all our time so far optimising the workflow to execute efficiently on one sample. We should also make some sanity checks after scaling up a run. As we have learnt, bioinformatics workflows typically contain some parts that can be parallelised within a single sample, some parts that can only be parallel by sample, and some parts that must collectively analyse all samples together (i.e. no parallelisation). </p> <p>It may be difficult to observe any logic failures when only a single sample is used for development and testing. If you have a large sample cohort, the final stage of testing prior to submitting the full workflow at maximum scale should be to test on a handful of samples. This is what we are doing in today's final exercise. </p> <p>Note that in real life, we would need to complete another round of benchmarking and resource optimisation to scale to the full genome (we have used only 3 of the smaller chromosomes) and full sequencing depth (we have used a small subset of sequence data per sample). However, thanks to the hard work we have put in place creating modular, dynamic, well-configured code, this would be a swift task. </p> <p>Exercise: Inspecting the timeline</p> <ul> <li> <p>Download the timeline file to your local computer and view it in your local browser (on VS Code, right click the file within the <code>runInfo</code> folder and select <code>Download</code>).</p> </li> <li> <p>Which processes were run in parallel?</p> </li> </ul> Show timeline <p></p> <p>The timeline clearly shows that our workflow logic was correct and processes are being run in the expected number, in the expected order and at the expected stage of execution:</p> <ul> <li>Parallel by sample execution of <code>FASTQC</code>, <code>MERGE_BAMS</code> and <code>GENOTYPE</code></li> <li>Scattered paralellisation of <code>ALIGN</code>, with 3 parallel alignment tasks for each of the 3 samples</li> <li>The non-parallel downstream data combining steps of <code>JOINT_GENOTYPE</code>, <code>STATS</code> and <code>MULTIQC</code> run once only and in order for the workflow</li> </ul> <p>Tip</p> <p>The placement of tasks on this timeline can show potential errors, for example if the scattered <code>ALIGN</code> tasks were waiting for the previous align task to finish before commencing. This could suggest a resource availability issue: are there enough free resources on your compute platform to run &gt;1 <code>ALIGN</code> task at once? Or it could suggest a channel issue, where incorrect syntax was causing a process to wait for input data it did not need. </p> <p>Another important check is to verify your workflow outputs are correct and what you are expecting to produce. This is out of scope for this workshop, but of critical importance to perform thoroughly (ideally on a few full-sized sample where possible) prior to expending a large amount of HPC service units on a full-scale run. </p>"},{"location":"part2/27_scale/#273-matching-hpc-allocation-to-the-scale-required","title":"2.7.3 Matching HPC allocation to the scale required","text":"<p>Of final note is the concept of service units and other compute resources, and their importance to running your Nextflow workflow on HPC. We have mentioned service units throughout this workshop as one of the reasons why workflow optimisation is important. Here we will expand on this slightly and also list other recommendations to check before submitting your workflow at full scale. </p> <p>Both Gadi and Setonix allocate a finite amount of service units to research projects to spend on these HPCs. Service units are finite due to the fact that a machine has a finite amount of resource hours within a time period (typically a quarter on HPC). The system administrators divvy up the amount of available hours (total system cores X hours within the quarter) among projects to ensure equitable access. </p> <p>Before submitting a full-scale workflow on HPC, it is important to first check that your project has sufficient service units to run the workflow. You can use your benchmarking trace reports to help estimate the amount required. If your project runs out of service units part way through workflow execution, jobs may be held, rejected by the scheduler resulting in premature workflow termination, or downgraded to low priority. Planning your compute work and applying for service units ahead of time can help avoid frustrating delays due to insufficient resources. Visit the NCI job costs page and the Pawsey accounting page for more information on service units and their calculation on these systems. </p> <p>Finally, it is important to also consider the scratch disk resources your workflow outputs will require. Physical disk usage as well as the total number of files and folders (referred to as \"inodes\") are monitored on HPC to ensure high performance of the I/O filesystem is maintained. Just as running out of service units can halt your workflow execution, so too can surpassing these per-project disk limits. Extrapolating disk requirements from your benchmark samples and multiplying those requirements by ~ 1.2 - 1.5 (to allow for failed runs, complex samples, and additional ad-hoc post-processing) is a reasonable approach to estimating the scratch disk requirement for your full-scale workflow.   </p>"},{"location":"part2/27_scale/#274-workflow-adaptability","title":"2.7.4 Workflow adaptability","text":"<p>When developing and running workflows on \"real\" data, input data is rarely uniform. While the sample data used here was intentionally consistent, actual data sets often contain variation in file size, formats, sequencing depth, metadata etc. These differences can impact performance and potentially require updating the workflow structure (<code>main.nf</code>, modules) and resourcing (custom configuration files) to suit.</p> <p>Configuration is an iterative process - get it right for a representative sample, scale up first to a few and then to all samples, iterate and optimise as you go. Using dynamic resourcing strategies and customised trace reports can simplify the tweaks that will be inadvertently required over time and use cases. </p>"},{"location":"part2/28_outro/","title":"Summary and takeaways","text":"<p>Optimising Nextflow pipelines on HPC involves more than just \"getting it to run\". It\u2019s about making it run efficiently and being a responsible user on a shared system. Below are a few key takeaways that can help guide your approach when developing your own Nextflow workflows for HPC. </p>"},{"location":"part2/28_outro/#get-familiar-with-your-hpc-environment","title":"Get familiar with your HPC environment","text":"<p>Each HPC has unique hardware, limits, charging models, and ways to work. Before you run anything:</p> <ul> <li>Familiarise yourself with the user documentation</li> <li>Know where to run jobs (not on the login node!)</li> <li>Check for network access, especially if pulling containers or pipelines on-the-go</li> <li>Attend your HPCs next training event</li> <li>Reach out to your HPC support team - they are your best source of optimisation advice</li> <li>Understand the accounting model of your HPC and ensure your project has sufficient SU and disk resources to run your full-scale workflow</li> </ul>"},{"location":"part2/28_outro/#start-small-then-scale","title":"Start small, then scale","text":"<p>Before launching full-scale analyses:</p> <ul> <li>Run the workflow on a small, representative sample</li> <li>Ensure the pipeline works end-to-end with appropriately configured resources</li> <li>Use this opportunity to benchmark walltime, memory, and cpu usage</li> <li>Scale small to test worfklow logic before running at vast scales</li> </ul>"},{"location":"part2/28_outro/#use-nextflows-built-in-profiling-features","title":"Use Nextflow's built-in profiling features","text":"<p>Use the trace and timeline outputs to guide configuration decisions. Review these before and after tuning.</p> <ul> <li>Identify long-running or inefficient processes</li> <li>Spot over-provisioned tasks (e.g. jobs using a small % of allocated memory or cores)</li> <li>Adjust as needed, remembering to configure to fit your HPC queues/partitions</li> </ul>"},{"location":"part2/28_outro/#choose-the-right-optimisation-strategy","title":"Choose the right optimisation strategy","text":"<p>Consider the different strategies we explored in Part 2 when tuning your pipeline:</p> <ul> <li>Review your tools, does it support multi-threading? \u2192 Increase CPUs, add ${task.cpus} to the <code>process</code> (remember to benchmark!)</li> <li>Can data be split into physical or interval chunks without breaking biology? \u2192 Use scatter-gather patterns</li> <li>Would more resources help speed up processing time? \u2192 Match requests to node sizes (e.g. 2 GB/core)</li> <li>When scaling, do you expect your input data to be heterogeneous? \u2192 Consider dynamic resource configuration </li> <li>Acknowledge what your research goals are - what do you need to optimise for? \u2192 Time, compute cost, throughput?</li> </ul>"},{"location":"part2/28_outro/#iterate-and-adapt","title":"Iterate and adapt","text":"<p>Efficiency is context-specific. What works for one sample or one HPC may not work for another.</p> <ul> <li>Run benchmarks iteratively, especially if scaling to 10s - 1000s of samples</li> <li>Keep your pipeline reproducible and flexible across systems - separate what the workflow runs to how and where it should run with separate configs for each</li> <li>Document and comment any tuning decisions to support reproducibility</li> </ul>"},{"location":"part2/28_outro/#resources","title":"Resources","text":""},{"location":"part2/28_outro/#developed-by-us","title":"Developed by us","text":"<ul> <li>SIH Nextflow template</li> <li>SIH Nextflow template guide</li> <li>SIH Nextflow for the life sciences 2025</li> <li>SIH Customising nf-core workshop</li> <li>SIH Introduction to NCI Gadi - Optimisation</li> <li>Australian BioCommons Seqera Platform Service</li> <li>NCI Gadi nf-core institutional config</li> <li>Pawsey Setonix nf-core institutional config</li> </ul>"},{"location":"part2/28_outro/#developed-by-others","title":"Developed by others","text":"<ul> <li>Nextflow training</li> <li>Nextflow patterns</li> <li>Nextflow blog</li> <li>Nextflow coding best practice recommendations</li> <li>Seqera community forums</li> </ul>"}]}