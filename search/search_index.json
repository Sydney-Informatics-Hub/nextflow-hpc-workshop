{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nextflow and HPC","text":"<p>This workshop is delivered over 2 3-hour sessions that cover how HPC works, to running and optimising workflows on it using Nextflow and nf-core.</p> <p>We work through a realistic experimental scenario: adapting a whole genome variant calling workflow to run on high performance computing (HPC) infrastructure. Each lesson connects foundational HPC concepts to workflow design and configuration in Nextflow. </p> <p>This workshop is delivered concurrently on Australia's Tier-1 HPCs; NCI's Gadi HPC and Pawsey's Setonix HPC. </p>"},{"location":"#developers","title":"Developers","text":"<ul> <li>Michael Geaghan, Sydney Informatics Hub, University of Sydney</li> <li>Fred Jaya, Sydney Informatics Hub, University of Sydney</li> <li>Mitchell O'Brien, Sydney Informatics Hub, University of Sydney </li> <li>Georgie Samaha, Sydney Informatics Hub, University of Sydney </li> </ul>"},{"location":"#facilitators","title":"Facilitators","text":"<ul> <li>Giorgia Mori, Australian BioCommons </li> <li>Sarah Beecroft, Pawsey Supercomputing Research Centre </li> <li>Kisaru Liyanage, National Computational Infrastructure </li> <li>Cali Willet, Sydney Informatics Hub, University of Sydney</li> <li>Kristina Gagalova, Curtin University </li> <li>Gayatri Aniruddha, University of Western Australia</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is an intermediate-advanced workshop for people developing reproducible bioinformatics workflows to run on HPC. It assumes experience with the following: </p> <ul> <li>Running and developing Nextflow workflows </li> <li>Working in a Linux environment using basic scripting (e.g. Bash)</li> <li>Working on a HPC cluster.  </li> </ul> <p>Attendees would benefit from having previously completed our Nextflow for the Life Sciences workshop. </p>"},{"location":"#set-up-requirements","title":"Set up requirements","text":"<p>Please see our set up instructions to set up your laptop for this workshop.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>In order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences * Gracefully accept constructive criticism</li> <li>Focus on what is best for the community</li> <li>Show courtesy and respect towards other community members.</li> </ul>"},{"location":"#workshop-schedule","title":"Workshop schedule","text":"<p>TODO add schedules. </p>"},{"location":"#course-survey","title":"Course survey","text":""},{"location":"#credits-and-acknowledgements","title":"Credits and acknowledgements","text":"<p>We gratefully acknowledge the contributions of Abdullah Shaikh, Ludovic Capelli, Ziad Al-Bkhetan, Melissa Burke, Magda Antczak, Wenjing Xue, and Matthew Downton. </p> <p>This workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in collaboration. The workshop was enabled by Australian BioCommons' BioCLI Platforms Project (NCRIS via Bioplatforms Australia).</p> <p></p>"},{"location":"00_setup/","title":"Set up your computer","text":"<p>In this workshop, we will be using Pawsey's Setonix HPC and NCI's Gadi HPC. </p> <p>The requirements for this workshop are a personal computer with:</p> <ul> <li>Visual Studio Code (VSCode)</li> <li>A web browser</li> </ul> <p>Below, you will find instructions on how to set up VSCode and connect to the HPC system to which you've been assigned</p> <p>. Each participant will be provided with their training account and password prior to the workshop. Before the workshop, you must have the following:</p> <ol> <li>VSCode installed</li> <li>The necessary VSCode extensions installed</li> <li>Be able to connect to your assigned HPC.</li> </ol> <p>Info</p> <p>If you require assistance with the setup, please write in the discussion board on the Google document.</p>"},{"location":"00_setup/#installing-visual-studio-code","title":"Installing Visual Studio Code","text":"<p>Visual Studio Code (VSCode) is a versatile code editor that we will use for the workshop. We will use VSCode to connect to the VM, navigate the directories, edit, view and download files.</p> <ol> <li>Download VSCode by following the installation instructions for your local Operating System.</li> <li>Open VSCode to confirm it was installed correctly.</li> </ol> <p></p>"},{"location":"00_setup/#installing-the-vscode-extensions","title":"Installing the VSCode extensions","text":"<p>Specific VSCode extensions are required to connect to the VM and make working with Nextflow files easier (i.e. syntax highlighting).</p> <ol> <li>In the VSCode sidebar on the left, click on the extensions button (four blocks)</li> <li>In the Extensions Marketplace search bar, search for <code>remote ssh</code>. Select \"Remote - SSH\"</li> </ol> <p> 3. Click on the blue <code>Install</code> button</p> <p> 4. Search for <code>nextflow</code> and install the \"Nextflow\" extension</p> <p> 5. Close the Extensions tab and sidebar</p>"},{"location":"00_setup/#connecting-to-the-hpcs","title":"Connecting to the HPCs","text":"<p>Ensure you have your training details of your assigned system. </p>"},{"location":"00_setup/#connect-to-gadi-todo","title":"Connect to Gadi TODO","text":""},{"location":"00_setup/#connect-to-setonix-todo","title":"Connect to Setonix TODO","text":""},{"location":"00_setup/#configuring-vscode-for-the-workshop","title":"Configuring VSCode for the workshop","text":"<p>Success</p> <p>You have now configured VSCode for the workshop!</p>"},{"location":"setup/","title":"Set up your computer","text":"<p>In this workshop, we will be using Pawsey's Setonix HPC and NCI's Gadi HPC. </p> <p>The requirements for this workshop are a personal computer with:</p> <ul> <li>Visual Studio Code (VSCode)</li> <li>A web browser</li> </ul> <p>Below, you will find instructions on how to set up VSCode and connect to the HPC system to which you've been assigned</p> <p>. Each participant will be provided with their training account and password prior to the workshop. Before the workshop, you must have the following:</p> <ol> <li>VSCode installed</li> <li>The necessary VSCode extensions installed</li> <li>Be able to connect to your assigned HPC.</li> </ol> <p>Info</p> <p>If you require assistance with the setup, please write in the discussion board on the Google document.</p>"},{"location":"setup/#installing-visual-studio-code","title":"Installing Visual Studio Code","text":"<p>Visual Studio Code (VSCode) is a versatile code editor that we will use for the workshop. We will use VSCode to connect to the VM, navigate the directories, edit, view and download files.</p> <ol> <li>Download VSCode by following the installation instructions for your local Operating System.</li> <li>Open VSCode to confirm it was installed correctly.</li> </ol> <p></p>"},{"location":"setup/#installing-the-vscode-extensions","title":"Installing the VSCode extensions","text":"<p>Specific VSCode extensions are required to connect to the VM and make working with Nextflow files easier (i.e. syntax highlighting).</p> <ol> <li>In the VSCode sidebar on the left, click on the extensions button (four blocks)</li> <li>In the Extensions Marketplace search bar, search for <code>remote ssh</code>. Select \"Remote - SSH\"</li> </ol> <p> 3. Click on the blue <code>Install</code> button</p> <p> 4. Search for <code>nextflow</code> and install the \"Nextflow\" extension</p> <p> 5. Close the Extensions tab and sidebar</p>"},{"location":"setup/#connecting-to-the-hpcs","title":"Connecting to the HPCs","text":"<p>Ensure you have your training details of your assigned system. </p>"},{"location":"setup/#connect-to-gadi-todo","title":"Connect to Gadi TODO","text":""},{"location":"setup/#connect-to-setonix-todo","title":"Connect to Setonix TODO","text":""},{"location":"setup/#configuring-vscode-for-the-workshop","title":"Configuring VSCode for the workshop","text":"<p>Success</p> <p>You have now configured VSCode for the workshop!</p>"},{"location":"part1/01_0_intro/","title":"Part 1 intro","text":""},{"location":"part1/01_1_hpc_for_workflows/","title":"1.1 HPC for bioinformatics workflows","text":"<p>Learning objectives</p> <ul> <li>Describe the HPC system components that workflows interact with.</li> <li>Identify why containerisation and resource-aware design are essential for scalable workflows.</li> <li>Describe how HPC scheduling and resource limitations shape pipeline configuration.</li> <li>Connect HPC principles to the Nextflow workflow management systems.</li> </ul> <p>High Performance Computing (HPC) systems are built to run large numbers of computational jobs efficiently. Bioinformatics analysis often involves many steps, many tools, and many samples, making it a perfect match for HPC. However, HPCs expect work to be submitted in a particular way, following specific rules. This means our workflows often need to be designed for HPC, not just moved to HPC.</p>"},{"location":"part1/01_1_hpc_for_workflows/#111-when-does-a-workflow-need-hpc","title":"1.1.1 When does a workflow need HPC?","text":"<p>In bioinformatics, a workflow is simply a defined series of steps that take data as input and transform that data into processed data and/or analytical results. This is true whether you are doing whole genome variant calling, proteomics quantification, single-cell transcriptomics, or metagenomics assembly. Each step in the pipeline performs one job, and each job depends on some form of computation and storage.</p> <p></p> <p>But workflows don\u2019t always need HPC. Many can run perfectly well on a laptop or a small workstation during development or for small datasets. </p>"},{"location":"part1/01_1_hpc_for_workflows/#signs-your-workflow-is-ready-for-hpc","title":"Signs your workflow is ready for HPC","text":"<p>TODO this is not very good, can come up with some better examples here, can be communicated better. </p> <p>Not every workflow needs a supercomputer. Many analyses start on a laptop and stay there\u2014especially during method development, testing small datasets, or when turnaround is more important than throughput. HPC becomes necessary when your workflow starts to hit practical limits of time, memory, storage, reliability, or governance.</p> <p>A workflow is usually ready for HPC when scale becomes a problem. This might be scale in data size (more gigabytes than your laptop can hold), compute time (weeks of serial runs), memory usage (jobs crash due to insufficient RAM), or workflow complexity (tens of jobs become too painful to run manually).</p> Challenge Scenario Runtime is too long A single sample takes &gt;12 hours to process Data size is too big Multiple large FASTQs to be processed Memory limits hit R or Python crashes loading matrices Scaling samples manually is painful Running multiple scripts across multiple samples Storage is a bottleneck Local disk constantly full You need parallel execution Multi-sample analysis must run faster Workflow reliability matters Need checkpointing and recovery Data must stay on institutional systems Governance, ethics, security"},{"location":"part1/01_1_hpc_for_workflows/#112-from-your-laptop-to-hpc","title":"1.1.2 From your laptop to HPC","text":"<p>Before running a workflow, it is important to understand the system we are running it on. Running workloads on HPC is very different from running them on your laptop or a local workstation. HPCs are not just bigger, they are also: </p> <ul> <li>Shared</li> <li>Scheduled</li> <li>Resource constrained. </li> </ul> <p>This introduces an important trade-off. HPCs give you access to massive computational power but at the cost of flexibility. On your laptop or a local workstation you can run whatever you like, whenever you like so long as it fits within the resource limitations of the system. On HPC, you gain scale and speed but you must work within system policies and limits. </p> <p></p>"},{"location":"part1/01_1_hpc_for_workflows/#113-hpc-architecture-for-workflows","title":"1.1.3 HPC architecture for workflows","text":"<p>While HPCs can look intimidating, their architecture follows a simple structure that supports large-scale computation through shared resources. From a workflow perspective, this architecture means there are a few important realities to accept: work is not run interactively, resources must be requested rather than assumed and everything is governed by shared access. </p> <p></p>"},{"location":"part1/01_1_hpc_for_workflows/#login-nodes","title":"Login nodes","text":"<p>When a user connects to an HPC, they first land on a login node. This is a shared access point used to prepare work, not perform computations. From here, users submit jobs to the scheduler, monitor their progress and organise their project directories. The login node exists only to coordinate access to the system, and because it is shared by many people at once, it must not be overloaded with computational tasks.</p>"},{"location":"part1/01_1_hpc_for_workflows/#compute-nodes","title":"Compute nodes","text":"<p>The real work happens on the compute nodes. These are powerful machines with many CPU cores, large amounts of memory and fast access to storage. Workflows do not run directly on them; instead, the scheduler assigns workflow tasks to available compute nodes based on the resources requested. This separation between the login node and compute nodes allows users to interact with the system while computation is queued and executed elsewhere.</p> <p>TODO some clarification re: compute nodes allocated to different queues</p> <p>Exercise</p> <p>TODO An exercise for understanding the login node vs compute node </p>"},{"location":"part1/01_1_hpc_for_workflows/#shared-storage","title":"Shared storage","text":"<p>All nodes are connected to a shared parallel filesystem. This is a large, high-speed storage system where input data, reference files and workflow outputs are kept. Because it is shared across all users, it enables collaborative research and scalable workflows. However, it also introduces constraints around file organisation and performance, which is why workflows must be careful about how they read and write data here.</p> <p>Exercise</p> <p>TODO An exercise for understanding shared storage </p>"},{"location":"part1/01_1_hpc_for_workflows/#job-scheduler","title":"Job scheduler","text":"<p>At the centre of everything is the job scheduler. Rather than allowing users to run programs directly, HPCs rely on a scheduling system (e.g. Slurm or PBS Pro) to manage fair access to shared compute resources. When a job is submitted, it enters a queue where the scheduler decides when and where it will run. Jobs are matched to compute nodes based on requested resources like CPU, memory and runtime. Understanding how the scheduler behaves is essential for designing workflows that run efficiently.</p> <p>TODO some clarification re: queues </p> <p>Exercise</p> <p>TODO An exercise for understanding the scheduler</p>"},{"location":"part1/01_1_hpc_for_workflows/#114-software-installation-is-different-on-hpc","title":"1.1.4 Software installation is different on HPC","text":"<p>No sudo for you!</p> <p>Unlike your laptop, you do not have administrative (<code>sudo</code>) privileges on HPC systems. On a laptop, you can install software however you like. On HPC, thousands of users share the same system, so unrestricted installs would break environments, cause version conflicts, and introduce security risks. That\u2019s why HPC systems block <code>sudo</code>.</p> <p>Bioinformatics workflows need software, and often many versions of it. Consider a typical bioinformatics pipeline:</p> <ul> <li>TODO insert tools and versions we are using for our demo pipeline in day 2</li> </ul> <p>These tools don\u2019t always play nicely together. Installing them globally is impossible. </p> <p>You can install and manage software for your own workflows, you just need to use HPC-approved methods. There are three main approaches:</p> Approach Description Pros Cons Environment modules Pre-installed software provided by HPC admins, loaded with <code>module load</code> Fast, easy to use, no setup required Limited versions; may conflict with workflow needs Conda/Mamba environments User-managed Python/R environments Flexible, easy for development Slow installs; dependency conflicts common; not fully reproducible Containers (Apptainer/Singularity) Portable, isolated software environments Reproducible, portable, avoids dependency issues Requires container knowledge <p>Containers bundle all the software a workflow needs, including tools, dependencies, libraries, even specific OS layers\u2014into a single portable image. On HPC, that means:</p>"},{"location":"part1/01_1_hpc_for_workflows/#conclusion","title":"Conclusion","text":"<p>This module sets the foundation for workflow configuration principles:</p>"},{"location":"part1/01_2_smarter/","title":"1.2 Work smarter, not harder","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <ul> <li>Parallelism, threads, memory usage per process</li> <li>Right-size your resources (avoid over-allocation)</li> <li>Scatter/gather vs multithreading</li> <li>Key takeaway: Efficiency determines queue time AND cost.</li> </ul> <p>HPC systems give us access to large amounts of compute, but that doesn\u2019t mean we should use resources carelessly. Misusing compute leads to long queue times, wasted allocation, unstable workflows and unhappy HPC administrators. Designing resource-aware workflows is essential for performance and fair use.</p> <p>At its core, HPC efficiency is about matching the structure of your workflow to the available compute. There are two main ways to increase performance on HPC:</p>"},{"location":"part1/01_2_smarter/#parallelisation-multithreading","title":"Parallelisation: multithreading","text":""},{"location":"part1/01_2_smarter/#parallelisation-scatter-gather","title":"Parallelisation: scatter-gather","text":""},{"location":"part1/01_3_experiment/","title":"Experimental context","text":""},{"location":"part1/01_3_experiment/#116-our-workflow-example","title":"1.1.6 Our workflow example","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <p>TODO add explainer of the high level genomics workflow we are using in the workshop. </p>"},{"location":"part1/01_4_nf_hpc/","title":"Running Nextflow on HPC","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part1/01_4_nf_hpc/#what-happens-when-a-workflow-runs-on-hpc","title":"What happens when a workflow runs on HPC?","text":"<p>Run https://github.com/Sydney-Informatics-Hub/config-demo-nf </p>"},{"location":"part1/02_1_nfcore_intro/","title":"2.1 - nf-core","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part1/02_1_nfcore_intro/#211-what-is-nf-core","title":"2.1.1 What is nf-core","text":""},{"location":"part1/02_1_nfcore_intro/#212-where-to-find-nf-core-pipelines","title":"2.1.2 Where to find nf-core pipelines","text":""},{"location":"part1/02_1_nfcore_intro/#213-nf-core-pipeline-structure","title":"2.1.3 nf-core pipeline structure","text":""},{"location":"part1/02_1_nfcore_intro/#214-nf-core-tools","title":"2.1.4 nf-core tools","text":""},{"location":"part1/02_1_nfcore_intro/#215-introducing-todays-pipeline-nf-coresarek","title":"2.1.5 Introducing today's pipeline: <code>nf-core/sarek</code>","text":""},{"location":"part1/02_2_nfcore_run/","title":"2.2 Running nf-core pipelines","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part1/02_2_nfcore_run/#221-prepraring-the-environment","title":"2.2.1 Prepraring the environment","text":""},{"location":"part1/02_2_nfcore_run/#222-write-a-simple-run-script","title":"2.2.2 Write a simple run script","text":""},{"location":"part1/02_2_nfcore_run/#223-running-the-pipeline","title":"2.2.3 Running the pipeline","text":""},{"location":"part1/02_3_nfcore_config/","title":"2.3 Configuring nf-core","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part1/02_3_nfcore_config/#231-executors","title":"2.3.1 Executors","text":""},{"location":"part1/02_3_nfcore_config/#232-containers-in-nf-core","title":"2.3.2 Containers in nf-core","text":""},{"location":"part1/02_3_nfcore_config/#233-configuring-hpc-resources","title":"2.3.3 Configuring HPC resources","text":""},{"location":"part1/02_4_nfcore_layer/","title":"2.4 Layering Nextflow configurations","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part1/02_4_nfcore_layer/#241-configuration-priorities","title":"2.4.1 Configuration priorities","text":""},{"location":"part1/02_4_nfcore_layer/#242-optimising-nf-coresarek-for-our-data","title":"2.4.2 Optimising <code>nf-core/sarek</code> for our data","text":""},{"location":"part1/03_0_outro/","title":"Part 1 wrap up","text":""},{"location":"part2/01_0_intro/","title":"Intro to part 2","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <p>Introduce our workflow use case and lesson structure </p>"},{"location":"part2/01_1_custom/","title":"Intro to part 2","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"},{"location":"part2/02_1_reporting/","title":"Add some reporting","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <p>Once we get the workflow running without error on the scheduler, where can we optimise. </p> <p>Look at the report.html. Create a more informative trace file: https://www.nextflow.io/docs/latest/reports.html#trace-file</p>"},{"location":"part2/02_2_scatter_gather/","title":"Optimise a process with scatter-gather","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <p>Alignment is a bottleneck, implelement SPLIT_FASTQ </p>"},{"location":"part2/03_1_scale/","title":"Scale to multiple samples","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul> <p>Now it has run successfully and efficiently on one, run on multiple samples in parallel. Explain sample-level paralellism. Refer back to smarter not harder lesson in HPC foundations. </p>"},{"location":"part2/03_2_dynamic/","title":"Dynamic resourcing","text":"<p>Learning objectives</p> <ul> <li>blah </li> <li>blah </li> </ul>"}]}