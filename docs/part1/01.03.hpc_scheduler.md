# 1.3 HPC fundamentals - Scheduler

## 1.3.1 Basic HPC resources

## 1.3.2 Submitting jobs


!!! example "Exercise 1.3.2: Submitting a job to the HPC scheduler"

    In this exercise, we will learn how to submit a job to the HPC scheduler. There are different schedulers out there with different commands and options, and in fact the two HPCs we are using in this workshop - Gadi and Setonix - use different schedulers - PBSPro and SLURM, respectively. However, know that while they use different commands, the general concepts are the same.

    Before we submit anything, we will need to slightly modify our `fastqc.sh` script. Previously, we interactively loaded the Singularity module, then ran the whole script within a Singularity container. However, to simplify things for job submission, we will load the Singularity module as a command within the script, and just use Singularity to run the `fastqc` command. Open the `fastqc.sh` script in VSCode and make the following changes:

    === "Gadi"

        ```bash title="fastqc.sh" hl_lines="3 10"
        #!/bin/bash

        module load singularity

        SAMPLE_ID="tiny"
        READS_1="data/${SAMPLE_ID}.R1.fq"
        READS_2="data/${SAMPLE_ID}.R2.fq"

        mkdir -p "results/fastqc_${SAMPLE_ID}_logs"
        singularity exec singularity/fastqc.sif \
        fastqc \
            --outdir "results/fastqc_${SAMPLE_ID}_logs" \
            --format fastq ${READS_1} ${READS_2}
        ```

    === "Setonix"

        ```bash title="fastqc.sh" hl_lines="3 10"
        #!/bin/bash

        module load singularity/4.1.0-slurm

        SAMPLE_ID="tiny"
        READS_1="data/${SAMPLE_ID}.R1.fq"
        READS_2="data/${SAMPLE_ID}.R2.fq"

        mkdir -p "results/fastqc_${SAMPLE_ID}_logs"
        singularity exec singularity/fastqc.sif \
        fastqc \
            --outdir "results/fastqc_${SAMPLE_ID}_logs" \
            --format fastq ${READS_1} ${READS_2}
        ```

    We have added two lines. The first new line (line 3) simply loads the singularity module within the script. The second new line (line 10) prefixes our `fastqc` command with `singularity exec singularity/fastqc.sif`; that is, we are now running the `fastqc` command through the `fastqc.sif` container. Note the trailing backslash `\` on line 10, indicating that the command continues on the next line.

    Now that we have our updated script, we can go ahead and submit it to the HPC. In doing so, we will provide the following details to the scheduler:

    - Project name: This is the HPC project that we want to run our job under, used to determine which filesystems we have access to and what project to bill to.
    - Job name: A name to give our job, to help distinguish it from others we or other people may be running; we will call our job `fastqc`
    - Queue: Which compute queue to submit our job to. Different queues have different resource limits. We will use the standard node for our HPCs.
    - Number of nodes and/or CPUs: A job can request differing numbers of nodes and CPUs it requires to run; we will just request 1 CPU on 1 node.
    - Amount of memory: The amount of RAM that our job requires to run; we will request 1 gigabyte.
    - Walltime: The time our job needs to complete; we will request 1 minute.

    Go ahead and run the appropriate command for your HPC system. These are detailed below, with some additional explanation of each of the parameters and how they relate to the details mentioned above:

    === "Gadi"

        ```bash
        qsub \
            -P $PROJECT \
            -N fastqc \
            -q normalbw \
            -l ncpus=1 \
            -l mem=1GB \
            -l walltime=00:01:00 \
            -l storage=scratch/$PROJECT \
            -l wd \
            fastqc.sh
        ```

        Let's deconstruct this command:

        - `-P $PROJECT`: This specifies the project using the environment variable `$PROJECT`, which is pre-defined with your default NCI project ID.
        - `-N fastqc`: This specifies the job name as `fastqc`.
        - `-q normalbw`: This specifies the queue as `normalbw`; this is a general queue on NCI for regular jobs that don't need any special resources.
        - `-l key=value`: The `-l` parameter lets us specify resources that we require in a `key=value` format:
            - `-l ncpus=1`: This specifies that we want 1 CPU to run our job.
            - `-l mem=1GB`: Here we request 1 GB of memory.
            - `-l walltime=00:01:00`: This requests 1 minute of walltime; walltime is specified in the format `HH:MM:SS`
            - `-l storage=scratch/$PROJECT`: This tells Gadi to mount the scratch space for our project; this is not done by default, so we always need to specify it.
            - `-l wd`: This tells Gadi to run the job in the current **w**orking **d**irectory.

    === "Setonix"

        ```bash
        sbatch \
            --account=$PAWSEY_PROJECT \
            --job-name=fastqc \
            --partition=work \
            --nodes=1 \
            --ntasks=1 \
            --cpus-per-task=1 \
            --mem=1GB \
            --time=00:01:00 \
            fastqc.sh
        ```

        Let's deconstruct this command:

        - `--account=$PAWSEY_PROJECT`: This specifies the project using the environment variable `$PAWSEY_PROJECT`, which is pre-defined with your default Pawsey project ID.
        - `--job-name=fastqc`: This specifies the job name as `fastqc`.
        - `--partition=work`: This specifies the queue (also called "partitions" on Setonix) as `work`; this is a general queue on Pawsey for regular jobs that don't need any special resources.
        - `--nodes=1 --ntasks=1 --cpus-per-task=1`: This specifies that we want 1 CPU on 1 node to run our job.
        - `--mem=1GB`: Here we request 1 GB of memory.
        - `--time=00:01:00`: This requests 1 minute of walltime; walltime is specified in the format `HH:MM:SS`

    Once submitted, you can monitor the progress of your job with the following command:

    === "Gadi"

        ```bash
        qstat -u ${USER}
        ```

    === "Setonix"

        ```bash
        squeue -u ${USER}
        ```

    This will output a list of all running jobs and their status:

    === "Gadi"

        ```console title="Output"
        gadi-pbs: 
                                                                        Req'd  Req'd   Elap
        Job ID               Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
        -------------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
        123456789.gadi-pbs   usr123   normal   fastqc        --    1   1  1024m 00:10 Q   -- 
        ```

        The `S` column near the end shows the status of the job, with typical codes being `Q` for queued, `R` for running, and `E` for finished or ending jobs.

    === "Setonix"

        ```console title="Output"
        JOBID        USER ACCOUNT                   NAME   EXEC_HOST ST     REASON START_TIME       END_TIME  TIME_LEFT NODES   PRIORITY       QOS
        12345678 username pawsey1234                fastqc       n/a PD       None N/A                   N/A      10:00     1      75423    normal
        ```

        The `ST` column near the middle shows the status of the job, with typical codes being `PD` for pending or queued, `R` for running, and `CG` for finished jobs.

## 1.3.3 Cleaning up job submission

!!! example "Exercise 1.3.3: A cleaner job submission"

    Submitting jobs with a long line of parameters can be fiddly, annoying, and error-prone. Instead, a good practice is to put as many of these parameters into the header of the script itself. Both Gadi and Setonix support this feature, by utilising special comments at the top of the script file. Note, however, that only static - i.e. unchanging - parameters can use this feature; if you need to dynamically assign resources to jobs, you will still need to use the commandline parameters.

    Update your `fastqc.sh` script with the following header comments:

    === "Gadi"

        ```bash title="fastqc.sh" hl_lines="2-9"
        #!/bin/bash
        #PBS -P ab01
        #PBS -N fastqc
        #PBS -q normalbw
        #PBS -l ncpus=1
        #PBS -l mem=1GB
        #PBS -l walltime=00:01:00
        #PBS -l storage=scratch/ab01
        #PBS -l wd

        module load singularity

        SAMPLE_ID="tiny"
        READS_1="data/${SAMPLE_ID}.R1.fq"
        READS_2="data/${SAMPLE_ID}.R2.fq"

        mkdir -p "results/fastqc_${SAMPLE_ID}_logs"
        singularity exec singularity/fastqc.sif \
        fastqc \
            --outdir "results/fastqc_${SAMPLE_ID}_logs" \
            --format fastq ${READS_1} ${READS_2}
        ```

        Note how we need to explicitly state the project name for both the `-P` and `-l storage` parameters.

    === "Setonix"

        ```bash title="fastqc.sh" hl_lines="2-9"
        #!/bin/bash
        #SBATCH --account=pawsey1234
        #SBATCH --job-name=fastqc
        #SBATCH --partition=work
        #SBATCH --nodes=1
        #SBATCH --ntasks=1
        #SBATCH --cpus-per-task=1
        #SBATCH --mem=1GB
        #SBATCH --time=00:01:00

        module load singularity/4.1.0-slurm

        SAMPLE_ID="tiny"
        READS_1="data/${SAMPLE_ID}.R1.fq"
        READS_2="data/${SAMPLE_ID}.R2.fq"

        mkdir -p "results/fastqc_${SAMPLE_ID}_logs"
        singularity exec singularity/fastqc.sif \
        fastqc \
            --outdir "results/fastqc_${SAMPLE_ID}_logs" \
            --format fastq ${READS_1} ${READS_2}
        ```

        Note how we need to explicity state the project name for the `--account` parameter.

    With the script updated, you can simply run your HPC submission command without any of the previously supplied parameters:

    === "Gadi"

        ```bash
        qsub fastqc.sh
        ```

    === "Setonix"

        ```bash
        sbatch fastqc.sh
        ```